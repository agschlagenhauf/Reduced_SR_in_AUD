{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to store csv files\n",
    "import pathlib\n",
    "pathlib.Path('./Punctate').mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc1(gamma, alpha, explore_chance, end_states, start_states, rewards, transitions, v_state, state_list, action_list, RPE_list):\n",
    "    time_step = 1\n",
    "    current_state = np.random.choice(start_states) - 1\n",
    "    timestep_list = []\n",
    "    not_end = True\n",
    "    end_states_adjusted = [i-1 for i in end_states]\n",
    "    \n",
    "    while not_end:\n",
    "        if current_state in end_states_adjusted:\n",
    "            not_end = False\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            ## Determine the next state, either a random subsequent state or the highest-value one based on the exploration parameter\n",
    "            if np.random.uniform() < explore_chance:\n",
    "                next_move = np.random.randint(len(transitions[current_state]))\n",
    "            else:\n",
    "                next_values = v_state[current_state]\n",
    "                next_move = np.argmax(next_values)\n",
    "\n",
    "            next_state = transitions[current_state][next_move] - 1\n",
    "\n",
    "\n",
    "            next_move = np.random.randint(len(transitions[current_state]))\n",
    "            next_state = transitions[current_state][next_move] - 1\n",
    "\n",
    "            # Get reward\n",
    "            reward = rewards[current_state][next_move]\n",
    "            \n",
    "            # calculate RPE and update weights and state values\n",
    "            if next_state in end_states_adjusted: # reached the goal state\n",
    "                delta = reward + 0 - v_state[current_state][next_move]\n",
    "            else:\n",
    "                delta = reward + gamma*np.max(v_state[next_state]) - v_state[current_state][next_move]\n",
    "            \n",
    "            # update state value\n",
    "            v_state[current_state][next_move] += alpha * delta\n",
    "            \n",
    "            state_list.append(current_state + 1)\n",
    "            action_list.append(next_state + 1)\n",
    "            RPE_list.append(delta)\n",
    "            timestep_list.append(time_step)\n",
    "\n",
    "            # Move to the next state\n",
    "            current_state = next_state\n",
    "            \n",
    "            time_step += 1\n",
    "\n",
    "    return v_state, state_list, action_list, RPE_list, timestep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for multi episodes\n",
    "def punc2(epi_num, gamma, alpha, explore_chance, end_states, start_states, rewards, transitions, v_state, state_list, action_list, RPE_list, epi_num_list):\n",
    "    epi_length = []\n",
    "    for k in range(epi_num):\n",
    "        c_v_state, c_state_list, c_action_list, c_RPE_list, timestep_list = \\\n",
    "        punc1(gamma, alpha, explore_chance, end_states, start_states, rewards, transitions, v_state, state_list, action_list, RPE_list)\n",
    "        \n",
    "        for j in range(len(timestep_list)):\n",
    "            epi_num_list.append(k+1)\n",
    "                \n",
    "        for j in range(len(timestep_list)):\n",
    "            epi_length.append(k+1)\n",
    "        \n",
    "        v_state = c_v_state\n",
    "        state_list = c_state_list\n",
    "        action_list = c_action_list\n",
    "        RPE_list = c_RPE_list\n",
    "        \n",
    "    return c_v_state, c_state_list, c_action_list, c_RPE_list, epi_num_list, epi_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for multi simulations\n",
    "def punc3(sim_num, epi_num, gamma, alpha, explore_chance, end_states, start_states, rewards, transitions, state_list, action_list, RPE_list, epi_num_list):\n",
    "    sim_num_list = []\n",
    "    \n",
    "    for t in range(sim_num):\n",
    "        v_state = []\n",
    "        \n",
    "        '''\n",
    "        for i in range(len(rewards)):\n",
    "            v_state.append(rewards[i].copy())\n",
    "        '''\n",
    "\n",
    "        for i in range(len(rewards)):\n",
    "            row = []\n",
    "            for j in range(len(rewards[i])):\n",
    "                row.append(0)\n",
    "            v_state.append(row)\n",
    "        \n",
    "        c_v_state, c_state_list, c_action_list, c_RPE_list, c_epi_num_list, epi_length = \\\n",
    "        punc2(epi_num, gamma, alpha, explore_chance, end_states, start_states, rewards, transitions, v_state, state_list, action_list, RPE_list, epi_num_list)\n",
    "        \n",
    "        for u in range(len(epi_length)):\n",
    "            sim_num_list.append(t+1)\n",
    "        \n",
    "        state_list = c_state_list\n",
    "        action_list = c_action_list\n",
    "        RPE_list = c_RPE_list\n",
    "        epi_num_list = c_epi_num_list\n",
    "    \n",
    "    return c_v_state, c_state_list, c_action_list, c_RPE_list, c_epi_num_list, sim_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Phase\n",
    "\n",
    "sim_num = 1\n",
    "epi_num = 200\n",
    "alpha = 0.50\n",
    "gamma = 0.95\n",
    "explore_chance = 0.5\n",
    "state_list = []\n",
    "action_list = []\n",
    "RPE_list = []\n",
    "epi_num_list = []\n",
    "\n",
    "end_states_base = [10, 11, 12]\n",
    "start_states_base = [1]\n",
    "rewards_base = [[0, 0], [0, 0], [0, 0], [15], [0], [30], [0], [0], [0], [0], [0], [0]]\n",
    "transitions_base = [[2, 3], [4, 5], [5, 6], [7], [8], [9], [10], [11], [12], [], [], []]\n",
    "\n",
    "v_state = []\n",
    "for i in range(len(rewards_base)):\n",
    "    row = []\n",
    "    for j in range(len(rewards_base[i])):\n",
    "        row.append(0)\n",
    "    v_state.append(row)\n",
    "\n",
    "rl_base = punc2(epi_num, gamma, alpha, explore_chance, end_states_base, start_states_base, rewards_base, transitions_base, v_state, state_list, action_list, RPE_list, epi_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[13.537499999998747, 27.074999999999733], [14.249999999998735, 0.0], [0.0, 28.49999999999983], [14.999999999999973], [0.0], [29.999999999999996], [0.0], [0.0], [0.0], [0], [0], [0]], [1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 5, 8, 1, 3, 5, 8, 1, 2, 4, 7, 1, 2, 4, 7, 1, 2, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 2, 4, 7, 1, 3, 6, 9, 1, 2, 5, 8, 1, 2, 4, 7, 1, 3, 5, 8, 1, 3, 5, 8, 1, 2, 4, 7, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 6, 9, 1, 2, 5, 8, 1, 3, 6, 9, 1, 2, 5, 8, 1, 2, 4, 7, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 6, 9, 1, 3, 6, 9, 1, 2, 4, 7, 1, 2, 5, 8, 1, 3, 5, 8, 1, 3, 5, 8, 1, 2, 5, 8, 1, 3, 6, 9, 1, 2, 4, 7, 1, 3, 5, 8, 1, 3, 5, 8, 1, 2, 5, 8, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 6, 9, 1, 2, 4, 7, 1, 3, 5, 8, 1, 3, 5, 8, 1, 3, 5, 8, 1, 2, 5, 8, 1, 2, 4, 7, 1, 2, 4, 7, 1, 2, 5, 8, 1, 2, 4, 7, 1, 3, 6, 9, 1, 3, 5, 8, 1, 2, 4, 7, 1, 2, 4, 7, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 5, 8, 1, 3, 5, 8, 1, 2, 4, 7, 1, 2, 4, 7, 1, 2, 4, 7, 1, 2, 4, 7, 1, 2, 4, 7, 1, 2, 4, 7, 1, 3, 6, 9, 1, 2, 5, 8, 1, 2, 5, 8, 1, 3, 5, 8, 1, 3, 5, 8, 1, 2, 5, 8, 1, 2, 5, 8, 1, 2, 4, 7, 1, 2, 5, 8, 1, 2, 5, 8, 1, 2, 4, 7, 1, 2, 5, 8, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 6, 9, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 6, 9, 1, 2, 5, 8, 1, 3, 5, 8, 1, 2, 5, 8, 1, 2, 5, 8, 1, 2, 4, 7, 1, 3, 6, 9, 1, 3, 5, 8, 1, 2, 4, 7, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 6, 9, 1, 2, 4, 7, 1, 2, 4, 7, 1, 2, 5, 8, 1, 3, 6, 9, 1, 3, 6, 9, 1, 3, 6, 9, 1, 2, 5, 8, 1, 2, 5, 8, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 6, 9, 1, 3, 6, 9, 1, 2, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 6, 9, 1, 2, 4, 7, 1, 2, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 2, 4, 7, 1, 2, 4, 7, 1, 3, 6, 9, 1, 2, 4, 7, 1, 2, 5, 8, 1, 2, 5, 8, 1, 3, 6, 9, 1, 2, 5, 8, 1, 3, 5, 8, 1, 3, 5, 8, 1, 2, 4, 7, 1, 2, 5, 8, 1, 2, 5, 8, 1, 2, 4, 7, 1, 3, 5, 8, 1, 2, 4, 7, 1, 3, 5, 8, 1, 2, 5, 8, 1, 2, 5, 8, 1, 3, 5, 8, 1, 2, 4, 7, 1, 3, 5, 8, 1, 3, 6, 9, 1, 2, 4, 7, 1, 3, 5, 8, 1, 2, 4, 7, 1, 2, 5, 8, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 2, 5, 8, 1, 2, 4, 7, 1, 3, 6, 9, 1, 2, 4, 7, 1, 3, 5, 8, 1, 2, 5, 8, 1, 3, 5, 8, 1, 2, 4, 7, 1, 3, 6, 9, 1, 2, 5, 8, 1, 3, 6, 9, 1, 2, 4, 7, 1, 3, 5, 8, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 6, 9, 1, 3, 6, 9, 1, 3, 6, 9, 1, 2, 4, 7, 1, 2, 4, 7, 1, 3, 6, 9, 1, 2, 5, 8, 1, 2, 4, 7, 1, 2, 5, 8, 1, 2, 4, 7, 1, 2, 4, 7, 1, 2, 4, 7, 1, 3, 6, 9, 1, 2, 4, 7, 1, 2, 5, 8, 1, 3, 6, 9, 1, 2, 4, 7, 1, 2, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 3, 6, 9, 1, 2, 4, 7, 1, 3, 6, 9, 1, 3, 5, 8, 1, 2, 5, 8, 1, 3, 5, 8, 1, 3, 5, 8, 1, 3, 5, 8, 1, 2, 5, 8, 1, 2, 5, 8, 1, 3, 6, 9, 1, 3, 5, 8, 1, 2, 5, 8, 1, 2, 5, 8, 1, 2, 5, 8], [3, 6, 9, 12, 3, 5, 8, 11, 3, 5, 8, 11, 3, 5, 8, 11, 2, 4, 7, 10, 2, 4, 7, 10, 2, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 2, 4, 7, 10, 3, 6, 9, 12, 2, 5, 8, 11, 2, 4, 7, 10, 3, 5, 8, 11, 3, 5, 8, 11, 2, 4, 7, 10, 3, 5, 8, 11, 3, 6, 9, 12, 3, 6, 9, 12, 2, 5, 8, 11, 3, 6, 9, 12, 2, 5, 8, 11, 2, 4, 7, 10, 3, 6, 9, 12, 3, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 3, 6, 9, 12, 3, 6, 9, 12, 3, 6, 9, 12, 2, 4, 7, 10, 2, 5, 8, 11, 3, 5, 8, 11, 3, 5, 8, 11, 2, 5, 8, 11, 3, 6, 9, 12, 2, 4, 7, 10, 3, 5, 8, 11, 3, 5, 8, 11, 2, 5, 8, 11, 3, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 3, 6, 9, 12, 2, 4, 7, 10, 3, 5, 8, 11, 3, 5, 8, 11, 3, 5, 8, 11, 2, 5, 8, 11, 2, 4, 7, 10, 2, 4, 7, 10, 2, 5, 8, 11, 2, 4, 7, 10, 3, 6, 9, 12, 3, 5, 8, 11, 2, 4, 7, 10, 2, 4, 7, 10, 3, 6, 9, 12, 3, 5, 8, 11, 3, 5, 8, 11, 3, 5, 8, 11, 2, 4, 7, 10, 2, 4, 7, 10, 2, 4, 7, 10, 2, 4, 7, 10, 2, 4, 7, 10, 2, 4, 7, 10, 3, 6, 9, 12, 2, 5, 8, 11, 2, 5, 8, 11, 3, 5, 8, 11, 3, 5, 8, 11, 2, 5, 8, 11, 2, 5, 8, 11, 2, 4, 7, 10, 2, 5, 8, 11, 2, 5, 8, 11, 2, 4, 7, 10, 2, 5, 8, 11, 3, 5, 8, 11, 3, 6, 9, 12, 3, 6, 9, 12, 3, 6, 9, 12, 3, 5, 8, 11, 3, 6, 9, 12, 2, 5, 8, 11, 3, 5, 8, 11, 2, 5, 8, 11, 2, 5, 8, 11, 2, 4, 7, 10, 3, 6, 9, 12, 3, 5, 8, 11, 2, 4, 7, 10, 3, 6, 9, 12, 3, 5, 8, 11, 3, 6, 9, 12, 2, 4, 7, 10, 2, 4, 7, 10, 2, 5, 8, 11, 3, 6, 9, 12, 3, 6, 9, 12, 3, 6, 9, 12, 2, 5, 8, 11, 2, 5, 8, 11, 3, 5, 8, 11, 3, 6, 9, 12, 3, 6, 9, 12, 3, 6, 9, 12, 2, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 3, 6, 9, 12, 3, 6, 9, 12, 2, 4, 7, 10, 2, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 2, 4, 7, 10, 2, 4, 7, 10, 3, 6, 9, 12, 2, 4, 7, 10, 2, 5, 8, 11, 2, 5, 8, 11, 3, 6, 9, 12, 2, 5, 8, 11, 3, 5, 8, 11, 3, 5, 8, 11, 2, 4, 7, 10, 2, 5, 8, 11, 2, 5, 8, 11, 2, 4, 7, 10, 3, 5, 8, 11, 2, 4, 7, 10, 3, 5, 8, 11, 2, 5, 8, 11, 2, 5, 8, 11, 3, 5, 8, 11, 2, 4, 7, 10, 3, 5, 8, 11, 3, 6, 9, 12, 2, 4, 7, 10, 3, 5, 8, 11, 2, 4, 7, 10, 2, 5, 8, 11, 3, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 2, 5, 8, 11, 2, 4, 7, 10, 3, 6, 9, 12, 2, 4, 7, 10, 3, 5, 8, 11, 2, 5, 8, 11, 3, 5, 8, 11, 2, 4, 7, 10, 3, 6, 9, 12, 2, 5, 8, 11, 3, 6, 9, 12, 2, 4, 7, 10, 3, 5, 8, 11, 3, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 3, 6, 9, 12, 3, 6, 9, 12, 3, 6, 9, 12, 2, 4, 7, 10, 2, 4, 7, 10, 3, 6, 9, 12, 2, 5, 8, 11, 2, 4, 7, 10, 2, 5, 8, 11, 2, 4, 7, 10, 2, 4, 7, 10, 2, 4, 7, 10, 3, 6, 9, 12, 2, 4, 7, 10, 2, 5, 8, 11, 3, 6, 9, 12, 2, 4, 7, 10, 2, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 3, 6, 9, 12, 2, 4, 7, 10, 3, 6, 9, 12, 3, 5, 8, 11, 2, 5, 8, 11, 3, 5, 8, 11, 3, 5, 8, 11, 3, 5, 8, 11, 2, 5, 8, 11, 2, 5, 8, 11, 3, 6, 9, 12, 3, 5, 8, 11, 2, 5, 8, 11, 2, 5, 8, 11, 2, 5, 8, 11], [0.0, 0.0, 30.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 15.0, 0, 0.0, 7.125, 7.5, 0.0, 3.384375, 0.0, 0.0, 0.0, 0.0, 14.25, 15.0, 0.0, 6.76875, 0.0, 0.0, 0.0, 1.6921875, 7.125, 3.75, 0.0, 3.384375, 14.25, 7.5, 0.0, 4.23046875, 0.0, 0.0, 0.0, 2.115234375, 5.34375, 1.875, 0.0, 8.4609375, 0.0, 0.0, 0.0, 4.23046875, 0.0, 0.0, 0.0, 3.5958984375, 3.5625, 0.9375, 0.0, 2.115234375, 0.0, 0.0, 0.0, 1.0576171875, 10.6875, 3.75, 0.0, 5.60537109375, 7.125, 1.875, 0.0, 3.4901367187499996, 0.0, 0.0, 0.0, 6.187060546874999, 4.453125, 0.9375, 0.0, 1.745068359374999, 0.0, 0.0, 0.0, 0.8725341796875004, 2.2265625, 0.46875, 0.0, 5.208764648437498, 2.671875, 0.46875, 0.0, 3.873522949218753, 0.0, 0.0, 0.0, 1.9367614746093764, 1.55859375, 0.234375, 0.0, 1.7087127685546868, 0.0, 0.0, 0.0, 0.8543563842773452, 0.890625, 0.1171875, 0.0, 0.8502250671386733, 0.5009765625, 0.05859375, 0.0, 0.6630764007568359, 0.2783203125, 0.029296875, 0.0, 1.493884277343751, 1.3359375, 0.234375, 0.0, 1.3815124511718757, 0.0, 0.0, 0.0, 0.46374034881591797, 0.0, 0.0, 0.0, 0.23187017440795898, 0.0, 0.0, 0.0, 0.6907562255859379, 0.0, 0.0, 0.0, 0.11593508720397949, 0.153076171875, 0.0146484375, 0.0, 0.34537811279296804, 0.779296875, 0.1171875, 0.0, 0.13067872524261404, 0.0, 0.0, 0.0, 0.06533936262130524, 0.0, 0.0, 0.0, 0.5428550720214833, 0.0, 0.0, 0.0, 0.0326696813106544, 0.0, 0.0, 0.0, 0.016334840655328975, 0.08349609375, 0.00732421875, 0.0, 0.047828064858912, 0.0, 0.0, 0.0, 0.023914032429456, 0.04522705078125, 0.003662109375, 0.0, 0.27142753601074077, 0.4453125, 0.05859375, 0.0, 0.03343986533582566, 0.0, 0.0, 0.0, 0.016719932667914605, 0.0, 0.0, 0.0, 0.008359966333959079, 0.0, 0.0, 0.0, 0.34723720550537074, 0.0, 0.0, 0.0, 0.17361860275268448, 0.25048828125, 0.029296875, 0.0, 0.20579123497009277, 0.13916015625, 0.0146484375, 0.0, 0.1689966917037964, 0.0, 0.0, 0.0, 0.0844983458518982, 0.0765380859375, 0.00732421875, 0.0, 0.004179983166981316, 0.02435302734375, 0.0018310546875, 0.0, 0.013657679571771553, 0.0, 0.0, 0.0, 0.07860476374626124, 0.041748046875, 0.003662109375, 0.0, 0.05913270413875438, 0.022613525390625, 0.0018310546875, 0.0, 0.006828839785885776, 0.0130462646484375, 0.00091552734375, 0.0, 0.009611395600948924, 0.0, 0.0, 0.0, 0.004805697800474462, 0.0, 0.0, 0.0, 0.0024028489002390074, 0.0, 0.0, 0.0, 0.04030777662992513, 0.012176513671875, 0.00091552734375, 0.0, 0.025937732309103012, 0.00652313232421875, 0.000457763671875, 0.0, 0.016067354008555412, 0.00347900390625, 0.0002288818359375, 0.0, 0.009686203859747167, 0.0018482208251953125, 0.00011444091796875, 0.0, 0.005721006821842067, 0.0009784698486328125, 5.7220458984375e-05, 0.0, 0.0033252765890221525, 0.0005164146423339844, 2.86102294921875e-05, 0.0, 0.0012014244501195037, 0.0069580078125, 0.000457763671875, 0.0, 0.001907935249619186, 0.0, 0.0, 0.0, 0.000953967624809593, 0.0, 0.0, 0.0, 0.0039057659359968966, 0.0, 0.0, 0.0, 0.0019528829679984483, 0.0, 0.0, 0.0, 0.0004769838124047965, 0.0, 0.0, 0.0, 0.00023849190620239824, 0.0, 0.0, 0.0, 0.00011924595310119912, 0.00027179718017578125, 1.430511474609375e-05, 0.0, 0.00018872663713409565, 0.0, 0.0, 0.0, 9.436331856704783e-05, 0.0, 0.0, 0.0, 4.7181659283523913e-05, 0.00014269351959228516, 7.152557373046875e-06, 0.0, 9.137025144667632e-05, 0.0, 0.0, 0.0, 0.0009764414839992241, 0.0, 0.0, 0.0, 0.0004882207419996121, 0.003696441650390625, 0.0002288818359375, 0.0, 0.001999920154936774, 0.001956939697265625, 0.00011444091796875, 0.0, 0.0019295064336688483, 0.0010328292846679688, 5.7220458984375e-05, 0.0, 0.0014553471270524199, 0.0, 0.0, 0.0, 0.0007276735635244336, 0.0005435943603515625, 2.86102294921875e-05, 0.0, 4.568512572333816e-05, 0.0, 0.0, 0.0, 0.000622044102929209, 0.0, 0.0, 0.0, 2.284256286166908e-05, 0.0, 0.0, 0.0, 1.1421281431722718e-05, 0.0, 0.0, 0.0, 5.710640715861359e-06, 7.474422454833984e-05, 3.5762786865234375e-06, 0.0, 0.00031102205146282813, 0.0002853870391845703, 1.430511474609375e-05, 0.0, 0.0002910698693412428, 0.0, 0.0, 0.0, 3.83588270178592e-05, 3.9070844650268555e-05, 1.7881393432617188e-06, 0.0, 0.0001455349346706214, 0.0001494884490966797, 7.152557373046875e-06, 0.0, 0.00014377448065872045, 0.0, 0.0, 0.0, 7.188724032758387e-05, 7.814168930053711e-05, 3.5762786865234375e-06, 0.0, 3.773806471762953e-05, 2.0384788513183594e-05, 8.940696716308594e-07, 0.0, 2.855180690097825e-05, 1.0617077350616455e-05, 4.470348358154297e-07, 0.0, 1.931901519292012e-05, 0.0, 0.0, 0.0, 7.30609225847445e-05, 4.076957702636719e-05, 1.7881393432617188e-06, 0.0, 5.589601037669922e-05, 2.123415470123291e-05, 8.940696716308594e-07, 0.0, 3.803422866965889e-05, 1.1041760444641113e-05, 4.470348358154297e-07, 0.0, 9.659507595571881e-06, 0.0, 0.0, 0.0, 4.829753796897762e-06, 0.0, 0.0, 0.0, 2.4261950549941957e-05, 0.0, 0.0, 0.0, 1.2130975274970979e-05, 5.733221769332886e-06, 2.2351741790771484e-07, 0.0, 8.788767978273881e-06, 2.9727816581726074e-06, 1.1175870895385742e-07, 0.0, 5.806455277479472e-06, 1.539476215839386e-06, 5.587935447692871e-08, 0.0, 2.4148768975607027e-06, 0.0, 0.0, 0.0, 3.6344788405529016e-06, 7.962808012962341e-07, 2.7939677238464355e-08, 0.0, 2.195472802668519e-06, 0.0, 0.0, 0.0, 1.0977364013342594e-06, 4.1141174733638763e-07, 1.3969838619232178e-08, 0.0, 7.442887799413711e-07, 2.123415470123291e-07, 6.984919309616089e-09, 0.0, 1.2074384496685298e-06, 5.520880222320557e-06, 2.2351741790771484e-07, 0.0, 3.226137330614165e-06, 0.0, 0.0, 0.0, 4.730066223146423e-07, 1.0948861017823219e-07, 3.4924596548080444e-09, 0.0, 2.8851040312360965e-07, 0.0, 0.0, 0.0, 1.6130686653070825e-06, 2.866610884666443e-06, 1.1175870895385742e-07, 0.0, 2.168174502159559e-06, 1.4863908290863037e-06, 5.587935447692871e-08, 0.0, 1.4425519978544799e-07, 5.640322342514992e-08, 1.7462298274040222e-09, 0.0, 1.7901228961392235e-06, 7.69738107919693e-07, 2.7939677238464355e-08, 0.0, 1.260687049864373e-06, 0.0, 0.0, 0.0, 6.303435249321865e-07, 0.0, 0.0, 0.0, 9.891913066439884e-08, 2.903107088059187e-08, 8.731149137020111e-10, 0.0, 3.1517176246609324e-07, 0.0, 0.0, 0.0, 6.324932400048056e-08, 0.0, 0.0, 0.0, 3.162466200024028e-08, 0.0, 0.0, 0.0, 1.575858803448682e-07, 3.9814040064811707e-07, 1.3969838619232178e-08, 0.0, 2.679096304802897e-07, 0.0, 0.0, 0.0, 1.3395481524014485e-07, 0.0, 0.0, 0.0, 6.697740673189401e-08, 2.0570587366819382e-07, 6.984919309616089e-09, 0.0, 1.5812332776476978e-08, 0.0, 0.0, 0.0, 1.311989930030677e-07, 1.0617077350616455e-07, 3.4924596548080444e-09, 0.0, 7.906166388238489e-09, 0.0, 0.0, 0.0, 1.1603061267351222e-07, 0.0, 0.0, 0.0, 5.801530633675611e-08, 0.0, 0.0, 0.0, 3.9530831941192446e-09, 0.0, 0.0, 0.0, 2.9007653168378056e-08, 5.4744305089116096e-08, 1.7462298274040222e-09, 0.0, 1.9765415970596223e-09, 0.0, 0.0, 0.0, 9.882725748866505e-10, 1.493026502430439e-08, 4.3655745685100555e-10, 0.0, 4.050737167915486e-08, 2.820161171257496e-08, 8.731149137020111e-10, 0.0, 7.586013595073382e-09, 0.0, 0.0, 0.0, 3.364944944905801e-08, 1.4515535440295935e-08, 4.3655745685100555e-10, 0.0, 2.3719604058669574e-08, 0.0, 0.0, 0.0, 3.793005021179852e-09, 0.0, 0.0, 0.0, 1.896502510589926e-09, 7.672497304156423e-09, 2.1827872842550278e-10, 0.0, 4.592688895854735e-09, 0.0, 0.0, 0.0, 1.1859802029334787e-08, 0.0, 0.0, 0.0, 5.929901902845813e-09, 7.465132512152195e-09, 2.1827872842550278e-10, 0.0, 2.296346224284207e-09, 3.939931048080325e-09, 1.0913936421275139e-10, 0.0, 6.510889605237935e-09, 3.836248652078211e-09, 1.0913936421275139e-10, 0.0, 3.019639649437522e-09, 0.0, 0.0, 0.0, 5.077662734720434e-09, 0.0, 0.0, 0.0, 1.5098216010756005e-09, 0.0, 0.0, 0.0, 2.538831367360217e-09, 1.9699655240401626e-09, 5.4569682106375694e-11, 0.0, 7.549125768946396e-10, 2.0218067220412195e-09, 5.4569682106375694e-11, 0.0, 2.2051498405062375e-09, 0.0, 0.0, 0.0, 1.3378169683164742e-09, 1.0368239600211382e-09, 2.7284841053187847e-11, 0.0, 1.1025758084315385e-09, 1.0109033610206097e-09, 2.7284841053187847e-11, 0.0, 1.1613998651682778e-09, 0.0, 0.0, 0.0, 5.806981562272995e-10, 0.0, 0.0, 0.0, 2.9034907811364974e-10, 5.313722795108333e-10, 1.3642420526593924e-11, 0.0, 3.9757352965352766e-10, 0.0, 0.0, 0.0, 1.9878854118360323e-10, 2.721662895055488e-10, 6.821210263296962e-12, 0.0, 2.2867396864967304e-10, 0.0, 0.0, 0.0, 1.1433698432483652e-10, 1.3932321962784044e-10, 3.410605131648481e-12, 0.0, 1.233466662142746e-10, 7.128164725145325e-11, 1.7053025658242404e-12, 0.0, 9.552891810926667e-11, 3.645084234449314e-11, 8.526512829121202e-13, 0.0, 1.031466467793507e-09, 5.184119800105691e-10, 1.3642420526593924e-11, 0.0, 7.619789244017738e-10, 2.6568613975541666e-10, 6.821210263296962e-12, 0.0, 6.508216188194638e-11, 1.8630430531629827e-11, 4.263256414560601e-13, 0.0, 5.071889574992383e-10, 0.0, 0.0, 0.0, 2.5359447874961916e-10, 1.360831447527744e-10, 3.410605131648481e-12, 0.0, 1.914362002253256e-10, 0.0, 0.0, 0.0, 9.57189882910825e-11, 6.966160981392022e-11, 1.7053025658242404e-12, 0.0, 8.095035752830881e-11, 3.5640823625726625e-11, 8.526512829121202e-13, 0.0, 5.7402971265219094e-11, 1.822542117224657e-11, 4.263256414560601e-13, 0.0, 4.1389114358025836e-11, 9.517719945506542e-12, 2.1316282072803006e-13, 0.0, 3.735856068942667e-11, 9.315215265814913e-12, 2.1316282072803006e-13, 0.0, 2.3103297053239658e-11, 0.0, 0.0, 0.0, 2.5217161692125956e-11, 4.856559598920285e-12, 1.0658141036401503e-13, 0.0, 1.1551648526619829e-11, 4.758859972753271e-12, 1.0658141036401503e-13, 0.0, 8.038014698286133e-12, 0.0, 0.0, 0.0, 1.4914292023604503e-11, 2.476241434123949e-12, 5.3290705182007514e-14, 0.0, 8.633094239485217e-12, 0.0, 0.0, 0.0, 4.316547119742609e-12, 1.2647660696529783e-12, 2.842170943040401e-14, 0.0, 4.019895527562767e-12, 2.4282797994601424e-12, 5.3290705182007514e-14, 0.0, 2.7569058147491887e-12, 6.465938895416912e-13, 1.4210854715202004e-14, 0.0, 1.687538997430238e-12, 0.0, 0.0, 0.0, 3.161915174132446e-12, 0.0, 0.0, 0.0, 8.419931418757187e-13, 0.0, 0.0, 0.0, 4.227729277772596e-13, 0.0, 0.0, 0.0, 2.0961010704922955e-13, 0.0, 0.0, 0.0, 1.580957587066223e-12, 0.0, 0.0, 0.0, 7.904787935331115e-13, 0.0, 0.0, 0.0, 1.0302869668521453e-13, 3.304023721284466e-13, 7.105427357601002e-15, 0.0, 2.0961010704922955e-13, 0.0, 0.0, 0.0, 3.943512183468556e-13, 0.0, 0.0, 0.0, 1.971756091734278e-13, 0.0, 0.0, 0.0, 9.947598300641403e-14, 0.0, 0.0, 0.0], [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26, 26, 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 32, 32, 32, 32, 33, 33, 33, 33, 34, 34, 34, 34, 35, 35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 38, 38, 38, 38, 39, 39, 39, 39, 40, 40, 40, 40, 41, 41, 41, 41, 42, 42, 42, 42, 43, 43, 43, 43, 44, 44, 44, 44, 45, 45, 45, 45, 46, 46, 46, 46, 47, 47, 47, 47, 48, 48, 48, 48, 49, 49, 49, 49, 50, 50, 50, 50, 51, 51, 51, 51, 52, 52, 52, 52, 53, 53, 53, 53, 54, 54, 54, 54, 55, 55, 55, 55, 56, 56, 56, 56, 57, 57, 57, 57, 58, 58, 58, 58, 59, 59, 59, 59, 60, 60, 60, 60, 61, 61, 61, 61, 62, 62, 62, 62, 63, 63, 63, 63, 64, 64, 64, 64, 65, 65, 65, 65, 66, 66, 66, 66, 67, 67, 67, 67, 68, 68, 68, 68, 69, 69, 69, 69, 70, 70, 70, 70, 71, 71, 71, 71, 72, 72, 72, 72, 73, 73, 73, 73, 74, 74, 74, 74, 75, 75, 75, 75, 76, 76, 76, 76, 77, 77, 77, 77, 78, 78, 78, 78, 79, 79, 79, 79, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82, 82, 82, 83, 83, 83, 83, 84, 84, 84, 84, 85, 85, 85, 85, 86, 86, 86, 86, 87, 87, 87, 87, 88, 88, 88, 88, 89, 89, 89, 89, 90, 90, 90, 90, 91, 91, 91, 91, 92, 92, 92, 92, 93, 93, 93, 93, 94, 94, 94, 94, 95, 95, 95, 95, 96, 96, 96, 96, 97, 97, 97, 97, 98, 98, 98, 98, 99, 99, 99, 99, 100, 100, 100, 100, 101, 101, 101, 101, 102, 102, 102, 102, 103, 103, 103, 103, 104, 104, 104, 104, 105, 105, 105, 105, 106, 106, 106, 106, 107, 107, 107, 107, 108, 108, 108, 108, 109, 109, 109, 109, 110, 110, 110, 110, 111, 111, 111, 111, 112, 112, 112, 112, 113, 113, 113, 113, 114, 114, 114, 114, 115, 115, 115, 115, 116, 116, 116, 116, 117, 117, 117, 117, 118, 118, 118, 118, 119, 119, 119, 119, 120, 120, 120, 120, 121, 121, 121, 121, 122, 122, 122, 122, 123, 123, 123, 123, 124, 124, 124, 124, 125, 125, 125, 125, 126, 126, 126, 126, 127, 127, 127, 127, 128, 128, 128, 128, 129, 129, 129, 129, 130, 130, 130, 130, 131, 131, 131, 131, 132, 132, 132, 132, 133, 133, 133, 133, 134, 134, 134, 134, 135, 135, 135, 135, 136, 136, 136, 136, 137, 137, 137, 137, 138, 138, 138, 138, 139, 139, 139, 139, 140, 140, 140, 140, 141, 141, 141, 141, 142, 142, 142, 142, 143, 143, 143, 143, 144, 144, 144, 144, 145, 145, 145, 145, 146, 146, 146, 146, 147, 147, 147, 147, 148, 148, 148, 148, 149, 149, 149, 149, 150, 150, 150, 150, 151, 151, 151, 151, 152, 152, 152, 152, 153, 153, 153, 153, 154, 154, 154, 154, 155, 155, 155, 155, 156, 156, 156, 156, 157, 157, 157, 157, 158, 158, 158, 158, 159, 159, 159, 159, 160, 160, 160, 160, 161, 161, 161, 161, 162, 162, 162, 162, 163, 163, 163, 163, 164, 164, 164, 164, 165, 165, 165, 165, 166, 166, 166, 166, 167, 167, 167, 167, 168, 168, 168, 168, 169, 169, 169, 169, 170, 170, 170, 170, 171, 171, 171, 171, 172, 172, 172, 172, 173, 173, 173, 173, 174, 174, 174, 174, 175, 175, 175, 175, 176, 176, 176, 176, 177, 177, 177, 177, 178, 178, 178, 178, 179, 179, 179, 179, 180, 180, 180, 180, 181, 181, 181, 181, 182, 182, 182, 182, 183, 183, 183, 183, 184, 184, 184, 184, 185, 185, 185, 185, 186, 186, 186, 186, 187, 187, 187, 187, 188, 188, 188, 188, 189, 189, 189, 189, 190, 190, 190, 190, 191, 191, 191, 191, 192, 192, 192, 192, 193, 193, 193, 193, 194, 194, 194, 194, 195, 195, 195, 195, 196, 196, 196, 196, 197, 197, 197, 197, 198, 198, 198, 198, 199, 199, 199, 199, 200, 200, 200, 200], [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26, 26, 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 32, 32, 32, 32, 33, 33, 33, 33, 34, 34, 34, 34, 35, 35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 38, 38, 38, 38, 39, 39, 39, 39, 40, 40, 40, 40, 41, 41, 41, 41, 42, 42, 42, 42, 43, 43, 43, 43, 44, 44, 44, 44, 45, 45, 45, 45, 46, 46, 46, 46, 47, 47, 47, 47, 48, 48, 48, 48, 49, 49, 49, 49, 50, 50, 50, 50, 51, 51, 51, 51, 52, 52, 52, 52, 53, 53, 53, 53, 54, 54, 54, 54, 55, 55, 55, 55, 56, 56, 56, 56, 57, 57, 57, 57, 58, 58, 58, 58, 59, 59, 59, 59, 60, 60, 60, 60, 61, 61, 61, 61, 62, 62, 62, 62, 63, 63, 63, 63, 64, 64, 64, 64, 65, 65, 65, 65, 66, 66, 66, 66, 67, 67, 67, 67, 68, 68, 68, 68, 69, 69, 69, 69, 70, 70, 70, 70, 71, 71, 71, 71, 72, 72, 72, 72, 73, 73, 73, 73, 74, 74, 74, 74, 75, 75, 75, 75, 76, 76, 76, 76, 77, 77, 77, 77, 78, 78, 78, 78, 79, 79, 79, 79, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82, 82, 82, 83, 83, 83, 83, 84, 84, 84, 84, 85, 85, 85, 85, 86, 86, 86, 86, 87, 87, 87, 87, 88, 88, 88, 88, 89, 89, 89, 89, 90, 90, 90, 90, 91, 91, 91, 91, 92, 92, 92, 92, 93, 93, 93, 93, 94, 94, 94, 94, 95, 95, 95, 95, 96, 96, 96, 96, 97, 97, 97, 97, 98, 98, 98, 98, 99, 99, 99, 99, 100, 100, 100, 100, 101, 101, 101, 101, 102, 102, 102, 102, 103, 103, 103, 103, 104, 104, 104, 104, 105, 105, 105, 105, 106, 106, 106, 106, 107, 107, 107, 107, 108, 108, 108, 108, 109, 109, 109, 109, 110, 110, 110, 110, 111, 111, 111, 111, 112, 112, 112, 112, 113, 113, 113, 113, 114, 114, 114, 114, 115, 115, 115, 115, 116, 116, 116, 116, 117, 117, 117, 117, 118, 118, 118, 118, 119, 119, 119, 119, 120, 120, 120, 120, 121, 121, 121, 121, 122, 122, 122, 122, 123, 123, 123, 123, 124, 124, 124, 124, 125, 125, 125, 125, 126, 126, 126, 126, 127, 127, 127, 127, 128, 128, 128, 128, 129, 129, 129, 129, 130, 130, 130, 130, 131, 131, 131, 131, 132, 132, 132, 132, 133, 133, 133, 133, 134, 134, 134, 134, 135, 135, 135, 135, 136, 136, 136, 136, 137, 137, 137, 137, 138, 138, 138, 138, 139, 139, 139, 139, 140, 140, 140, 140, 141, 141, 141, 141, 142, 142, 142, 142, 143, 143, 143, 143, 144, 144, 144, 144, 145, 145, 145, 145, 146, 146, 146, 146, 147, 147, 147, 147, 148, 148, 148, 148, 149, 149, 149, 149, 150, 150, 150, 150, 151, 151, 151, 151, 152, 152, 152, 152, 153, 153, 153, 153, 154, 154, 154, 154, 155, 155, 155, 155, 156, 156, 156, 156, 157, 157, 157, 157, 158, 158, 158, 158, 159, 159, 159, 159, 160, 160, 160, 160, 161, 161, 161, 161, 162, 162, 162, 162, 163, 163, 163, 163, 164, 164, 164, 164, 165, 165, 165, 165, 166, 166, 166, 166, 167, 167, 167, 167, 168, 168, 168, 168, 169, 169, 169, 169, 170, 170, 170, 170, 171, 171, 171, 171, 172, 172, 172, 172, 173, 173, 173, 173, 174, 174, 174, 174, 175, 175, 175, 175, 176, 176, 176, 176, 177, 177, 177, 177, 178, 178, 178, 178, 179, 179, 179, 179, 180, 180, 180, 180, 181, 181, 181, 181, 182, 182, 182, 182, 183, 183, 183, 183, 184, 184, 184, 184, 185, 185, 185, 185, 186, 186, 186, 186, 187, 187, 187, 187, 188, 188, 188, 188, 189, 189, 189, 189, 190, 190, 190, 190, 191, 191, 191, 191, 192, 192, 192, 192, 193, 193, 193, 193, 194, 194, 194, 194, 195, 195, 195, 195, 196, 196, 196, 196, 197, 197, 197, 197, 198, 198, 198, 198, 199, 199, 199, 199, 200, 200, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "print(rl_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Learning Phase\n",
    "\n",
    "v_state_base = rl_base[0]\n",
    "relearning_episodes = 200\n",
    "relearning_start_states = [2, 3]\n",
    "\n",
    "# Reward Revaluation\n",
    "v_state_reward = []\n",
    "for i in range(len(v_state_base)):\n",
    "    v_state_reward.append(v_state_base[i].copy())\n",
    "rewards_reward = [[0, 0], [0, 0], [0, 0], [45], [0], [30], [0], [0], [0], [0], [0], [0]]\n",
    "rl_reward = punc2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_reward, transitions_base, v_state_reward, state_list, action_list, RPE_list, epi_num_list)\n",
    "\n",
    "# Transition Revaluation\n",
    "v_state_transition = []\n",
    "for i in range(len(v_state_base)):\n",
    "    v_state_transition.append(v_state_base[i].copy())\n",
    "transitions_transition = [[2, 3], [5, 6], [4, 5], [7], [8], [9], [10], [11], [12], [], [], []]\n",
    "rl_transition = punc2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_base, transitions_transition, v_state_transition, state_list, action_list, RPE_list, epi_num_list)\n",
    "\n",
    "# Policy Revaluation\n",
    "v_state_policy = []\n",
    "for i in range(len(v_state_base)):\n",
    "    v_state_policy.append(v_state_base[i].copy())\n",
    "rewards_policy = [[0, 0], [0, 0], [0, 0], [45], [15], [30], [0], [0], [0], [0], [0], [0]]\n",
    "rl_policy = punc2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_policy, transitions_base, v_state_policy, state_list, action_list, RPE_list, epi_num_list)\n",
    "\n",
    "# Goal State Revaluation\n",
    "v_state_goal = []\n",
    "for i in range(len(v_state_base)):\n",
    "    v_state_goal.append(v_state_base[i].copy())\n",
    "rewards_goal = [[0, 0], [0, 0], [0, 0], [15], [0], [30], [30], [0], [0], [0], [0], [0]]\n",
    "rl_goal = punc2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_goal, transitions_base, v_state_goal, state_list, action_list, RPE_list, epi_num_list)\n",
    "\n",
    "# Control\n",
    "v_state_control = []\n",
    "for i in range(len(v_state_base)):\n",
    "    v_state_control.append(v_state_base[i].copy())\n",
    "rewards_control = [[0, 0], [0, 0], [0, 0], [15], [0], [30], [0], [0], [45], [0], [0], [0]]\n",
    "rl_control = punc2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_control, transitions_base, v_state_control, state_list, action_list, RPE_list, epi_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Values\n",
      "[13.537499999998747, 27.074999999999733]\n",
      "Reward Revaluation\n",
      "[13.537499999998747, 27.074999999999733]\n",
      "Transition Revaluation\n",
      "[13.537499999998747, 27.074999999999733]\n",
      "Policy Revaluation\n",
      "[13.537499999998747, 27.074999999999733]\n",
      "Goal State Revaluation\n",
      "[13.537499999998747, 27.074999999999733]\n",
      "Control\n",
      "[13.537499999998747, 27.074999999999733]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Values\")\n",
    "print(v_state_base[0])\n",
    "\n",
    "print(\"Reward Revaluation\")\n",
    "print(rl_reward[0][0])\n",
    "\n",
    "print(\"Transition Revaluation\")\n",
    "print(rl_transition[0][0])\n",
    "\n",
    "print(\"Policy Revaluation\")\n",
    "print(rl_policy[0][0])\n",
    "\n",
    "print(\"Goal State Revaluation\")\n",
    "print(rl_goal[0][0])\n",
    "\n",
    "print(\"Control\")\n",
    "print(rl_control[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulations with various parameters\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "\n",
    "seed_list = [22, 76, 50, 57, 30, 55, 33, 54,  0]\n",
    "index = 0\n",
    "\n",
    "for gamma in [0.95, 0.97, 0.99]:\n",
    "    for stay_prob in [0.50, 0.75, 0.90]:\n",
    "        \n",
    "        rd.seed(seed_list[index])\n",
    "        \n",
    "        # set constant variables\n",
    "        sim_num = 100\n",
    "        epi_num = 200\n",
    "        alpha = 0.50\n",
    "        state_n = 10\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        RPE_list = []\n",
    "        epi_num_list = []\n",
    "        \n",
    "        # simulation\n",
    "        rl = punc3(sim_num, epi_num, gamma, alpha, state_n, stay_prob, state_list,\n",
    "                  action_list, RPE_list, epi_num_list)\n",
    "        \n",
    "        # create dataframe and convert it to csv\n",
    "        result = pd.DataFrame({'Simulation': rl[5], 'Episode': rl[4], 'State': rl[1],\n",
    "                              'Action': rl[2], 'RPE': rl[3]})\n",
    "        result.to_csv('./Punctate/g{:.0f}_s{:.0f}_{:.0f}states.csv'.format(100*gamma, 100*stay_prob, state_n))\n",
    "        index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
