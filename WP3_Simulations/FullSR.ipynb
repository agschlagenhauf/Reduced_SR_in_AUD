{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a foler to store results\n",
    "import pathlib\n",
    "pathlib.Path('./Full_SR').mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_flatten(list):\n",
    "    return [item for row in list for item in row]\n",
    "\n",
    "def get_flattened_index(list, row, item):\n",
    "    index = 0\n",
    "    for i in range(row):\n",
    "        index += len(list[i])\n",
    "    index += item\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for one episode\n",
    "def fullSR1(gamma, alpha, explore_chance, end_states, start_states, rewards, transitions, num_pairs, init_sr, init_weight, state_list, action_list, RPE_list, weight_list): # weight_list=[[],[],[], ..., []]\n",
    "    weight = init_weight # starts with zeros\n",
    "    time_step = 1\n",
    "    feat = init_sr # feature vector; Successor Representation: starts with zeros\n",
    "    \n",
    "    # Initial state value (will also be zeros)\n",
    "    v_state = []\n",
    "    for k in range(num_pairs):\n",
    "        v_state.append(np.sum(weight*feat[k]))\n",
    "        \n",
    "    current_state = np.random.choice(start_states) - 1\n",
    "    timestep_list = []\n",
    "    not_end = True\n",
    "    end_states_adjusted = [i-1 for i in end_states]\n",
    "    \n",
    "    while not_end:\n",
    "        if current_state in end_states_adjusted:\n",
    "            not_end = False\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            # Determine the next state, either a random subsequent state or the highest-value subsequent state, depending on the exploration parameter\n",
    "            if np.random.uniform() < explore_chance:\n",
    "                next_move = np.random.randint(len(transitions[current_state]))\n",
    "            else:\n",
    "                next_move_index = get_flattened_index(transitions, current_state, 0)\n",
    "                next_values = v_state[next_move_index:(next_move_index+len(transitions[current_state]))]\n",
    "                next_move = np.argmax(next_values)\n",
    "\n",
    "            next_state = transitions[current_state][next_move] - 1\n",
    "\n",
    "            # Determine the best action to take from the NEXT state, used to calculate the one-hot vector for updating the successor matrix\n",
    "            next_move_index = get_flattened_index(transitions, next_state, 0)\n",
    "            next_values = v_state[next_move_index:(next_move_index+len(transitions[next_state]))]\n",
    "\n",
    "            # Occasionally assume a random next move as the immediate successor instead of the best next move.\n",
    "            # This way the successor matrix will reflect all possible successor states but have larger values for the highest-reward ones\n",
    "            # This is important for the policy reevaluation condition\n",
    "            best_next_move = np.argmax(next_values) + next_move_index\n",
    "            random_next_move = np.random.randint(len(transitions[next_state])) + next_move_index\n",
    "            if np.random.uniform() < explore_chance:\n",
    "                next_move_one_hot = random_next_move\n",
    "            else:\n",
    "                next_move_one_hot = best_next_move\n",
    "\n",
    "            # Get reward\n",
    "            reward = rewards[current_state][next_move]\n",
    "\n",
    "            weight_delta = reward - weight[get_flattened_index(rewards, current_state, next_move)]\n",
    "\n",
    "            weight[get_flattened_index(rewards, current_state, next_move)] += alpha * weight_delta\n",
    "\n",
    "            one_hot = np.zeros(num_pairs)\n",
    "\n",
    "            #one_hot[next_move_one_hot] = 1\n",
    "            one_hot[get_flattened_index(transitions, current_state, next_move)] = 1\n",
    "\n",
    "            # Needs to be based on current state & next move, not just current state\n",
    "            feat_delta = one_hot + gamma * feat[next_move_one_hot] - feat[get_flattened_index(transitions, current_state, next_move)]\n",
    "\n",
    "            feat[get_flattened_index(transitions, current_state, next_move)] += alpha * feat_delta\n",
    "            \n",
    "            '''\n",
    "            # calculate RPE and update weights and state values\n",
    "            if next_state in end_states: # reached the goal state\n",
    "                delta = reward + 0 - v_state[current_state][next_move]\n",
    "            else:\n",
    "                delta = reward + gamma*np.max(v_state[next_state]) - v_state[current_state][next_move]\n",
    "            \n",
    "            # update weights\n",
    "            weight += alpha * delta * feat[current_state]\n",
    "            '''\n",
    "            \n",
    "            # update state value\n",
    "            for k in range(num_pairs):\n",
    "                v_state[k] = np.sum(weight*feat[k])\n",
    "            \n",
    "            state_list.append(current_state + 1)\n",
    "            action_list.append(next_state + 1)\n",
    "            RPE_list.append(weight_delta)\n",
    "            timestep_list.append(time_step)\n",
    "            \n",
    "            for k in range(num_pairs):\n",
    "                weight_list[k].append(weight[k])\n",
    "            \n",
    "            # Move to the next state\n",
    "            current_state = next_state\n",
    "            \n",
    "            time_step += 1\n",
    "\n",
    "    return weight, feat, state_list, action_list, RPE_list, timestep_list, weight_list\n",
    "\n",
    "# function for multi episodes\n",
    "def fullSR2(epi_num, gamma, alpha, explore_chance, end_states, start_states, rewards, transitions, num_pairs, init_sr, init_weight, state_list, action_list, RPE_list, weight_list, epi_num_list):\n",
    "    epi_length = []\n",
    "    for k in range(epi_num):\n",
    "        c_weight, c_feat, c_state_list, c_action_list, c_RPE_list, timestep_list, c_weight_list = \\\n",
    "        fullSR1(gamma, alpha, explore_chance, end_states, start_states, rewards, transitions, num_pairs, init_sr, init_weight, state_list, action_list, RPE_list, weight_list)\n",
    "        \n",
    "        for j in range(len(timestep_list)):\n",
    "            epi_num_list.append(k+1)\n",
    "                \n",
    "        for j in range(len(timestep_list)):\n",
    "            epi_length.append(k+1)\n",
    "        \n",
    "        init_weight = c_weight\n",
    "        init_sr = c_feat\n",
    "        state_list = c_state_list\n",
    "        action_list = c_action_list\n",
    "        RPE_list = c_RPE_list\n",
    "        weight_list = c_weight_list\n",
    "        \n",
    "    return c_weight, c_feat, c_state_list, c_action_list, c_RPE_list, c_weight_list, epi_num_list, epi_length\n",
    "\n",
    "\n",
    "# function for multi simulations\n",
    "def fullSR3(sim_num, epi_num, gamma, alpha, end_states, start_states, rewards, transitions, num_pairs, state_list, action_list, RPE_list, weight_list, epi_num_list):\n",
    "    sim_num_list = []\n",
    "    \n",
    "    # SR\n",
    "    '''\n",
    "    init_sr = np.array([])\n",
    "    for j in range(state_n):\n",
    "        row = np.array([])\n",
    "    \n",
    "        z = np.zeros(j)\n",
    "        row = np.append(row, z)\n",
    "\n",
    "        for k in range(state_n - j):\n",
    "            row = np.append(row, gamma**(k))\n",
    "            \n",
    "        init_sr = np.append(init_sr, row)\n",
    "\n",
    "    init_sr = init_sr.reshape((state_n, state_n))\n",
    "    '''\n",
    "\n",
    "    init_sr = np.zeros((num_pairs, num_pairs))\n",
    "\n",
    "    '''\n",
    "    for i in range(num_pairs):\n",
    "        init_sr[i][i] = 1\n",
    "    '''\n",
    "    \n",
    "    # Simulation\n",
    "    for t in range(sim_num):\n",
    "\n",
    "        init_weight = np.zeros(num_pairs)\n",
    "\n",
    "        '''\n",
    "        init_weight = []\n",
    "        for i in range(len(rewards)):\n",
    "            for j in range(len(rewards[i])):\n",
    "                init_weight.append(0)\n",
    "        '''\n",
    "        \n",
    "        c_weight, c_feat, c_state_list, c_action_list, c_RPE_list, c_weight_list, c_epi_num_list, epi_length = \\\n",
    "        fullSR2(epi_num, gamma, alpha, explore_chance, end_states, start_states, rewards, transitions, num_pairs, init_sr, init_weight, state_list, action_list, RPE_list, weight_list, epi_num_list)\n",
    "        \n",
    "        for u in range(len(epi_length)):\n",
    "            sim_num_list.append(t+1)\n",
    "    \n",
    "        state_list = c_state_list\n",
    "        action_list = c_action_list\n",
    "        RPE_list = c_RPE_list\n",
    "        weight_list = c_weight_list\n",
    "        epi_num_list = c_epi_num_list\n",
    "    \n",
    "    return c_weight, c_feat, c_state_list, c_action_list, c_RPE_list, c_weight_list, c_epi_num_list, sim_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_num = 1\n",
    "epi_num = 200\n",
    "gamma = 0.97\n",
    "alpha = 0.50\n",
    "explore_chance = 0.5\n",
    "\n",
    "end_states_base = [10, 11, 12, 13]\n",
    "start_states_base = [1]\n",
    "rewards_base = [[0, 0], [0, 0], [0, 0], [15], [0], [30], [0], [0], [0], [0], [0], [0]]\n",
    "transitions_base = [[2, 3], [4, 5], [5, 6], [7], [8], [9], [10], [11], [12], [13], [13], [13]]\n",
    "num_pairs = len(list_flatten(rewards_base))\n",
    "\n",
    "init_sr_base = np.zeros((num_pairs, num_pairs))\n",
    "init_weight_base = np.zeros(num_pairs)\n",
    "\n",
    "state_list = []\n",
    "action_list = []\n",
    "RPE_list = []\n",
    "weight_list = [[] for k in range(num_pairs)]\n",
    "epi_num_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_base = fullSR2(epi_num, gamma, alpha, explore_chance, end_states_base, start_states_base, rewards_base, transitions_base, num_pairs, init_sr_base, init_weight_base, \\\n",
    "    state_list, action_list, RPE_list, weight_list, epi_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_states_policy_base = [10, 11, 12, 13]\n",
    "start_states_policy_base = [1]\n",
    "rewards_policy_base = [[0, 0], [0, 0], [0, 0], [0], [15], [30], [0], [0], [0], [0], [0], [0]]\n",
    "transitions_policy_base = [[2, 3], [4, 5], [5, 6], [7], [8], [9], [10], [11], [12], [13], [13], [13]]\n",
    "num_pairs = len(list_flatten(rewards_policy_base))\n",
    "\n",
    "init_sr_policy_base = np.zeros((num_pairs, num_pairs))\n",
    "init_weight_policy_base = np.zeros(num_pairs)\n",
    "\n",
    "state_list = []\n",
    "action_list = []\n",
    "RPE_list = []\n",
    "weight_list = [[] for k in range(num_pairs)]\n",
    "epi_num_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_policy_base = fullSR2(epi_num, gamma, alpha, explore_chance, end_states_policy_base, start_states_policy_base, rewards_policy_base, transitions_policy_base, num_pairs, \\\n",
    "    init_sr_policy_base, init_weight_policy_base, state_list, action_list, RPE_list, weight_list, epi_num_list)\n",
    "init_weight_policy_base = rl_policy_base[0]\n",
    "init_sr_policy_base = rl_policy_base[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.555 22.931]\n"
     ]
    }
   ],
   "source": [
    "v_state_base = np.zeros(num_pairs)\n",
    "for k in range(num_pairs):\n",
    "    v_state_base[k] = np.sum(rl_base[0]*rl_base[1][k])\n",
    "\n",
    "print(np.around(v_state_base[0:2], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Learning Phase\n",
    "init_weight_base = rl_base[0]\n",
    "init_sr_base = rl_base[1]\n",
    "relearning_episodes = 200\n",
    "relearning_start_states = [2, 3]\n",
    "\n",
    "# Reward Revaluation\n",
    "init_weight_reward = init_weight_base.copy()\n",
    "init_sr_reward = np.copy(init_sr_base)\n",
    "rewards_reward = [[0, 0], [0, 0], [0, 0], [45], [0], [30], [0], [0], [0], [0], [0], [0]]\n",
    "rl_reward = fullSR2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_reward, transitions_base, num_pairs, init_sr_reward, init_weight_reward, state_list, action_list, RPE_list, weight_list, epi_num_list)\n",
    "\n",
    "# Transition Revaluation\n",
    "init_weight_transition = init_weight_base.copy()\n",
    "init_sr_transition = np.copy(init_sr_base)\n",
    "transitions_transition = [[2, 3], [5, 6], [4, 5], [7], [8], [9], [10], [11], [12], [13], [13], [13]]\n",
    "rl_transition = fullSR2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_base, transitions_transition, num_pairs, init_sr_transition, init_weight_transition, state_list, action_list, RPE_list, weight_list, epi_num_list)\n",
    "\n",
    "# Policy Revaluation\n",
    "init_weight_policy = init_weight_policy_base.copy()\n",
    "init_sr_policy = np.copy(init_sr_policy_base)\n",
    "rewards_policy = [[0, 0], [0, 0], [0, 0], [45], [15], [30], [0], [0], [0], [0], [0], [0]]\n",
    "rl_policy = fullSR2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_policy, transitions_base, num_pairs, init_sr_policy, init_weight_policy, state_list, action_list, RPE_list, weight_list, epi_num_list)\n",
    "\n",
    "# Goal State Revaluation\n",
    "init_weight_goal = init_weight_base.copy()\n",
    "init_sr_goal = np.copy(init_sr_base)\n",
    "rewards_goal = [[0, 0], [0, 0], [0, 0], [15], [0], [30], [30], [0], [0], [0], [0], [0]]\n",
    "rl_goal = fullSR2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_goal, transitions_base, num_pairs, init_sr_goal, init_weight_goal, state_list, action_list, RPE_list, weight_list, epi_num_list)\n",
    "\n",
    "# Control\n",
    "init_weight_control = init_weight_base.copy()\n",
    "init_sr_control = np.copy(init_sr_base)\n",
    "rewards_control = [[0, 0], [0, 0], [0, 0], [15], [0], [30], [0], [0], [45], [0], [0], [0]]\n",
    "rl_control = fullSR2(relearning_episodes, gamma, alpha, explore_chance, end_states_base, relearning_start_states, rewards_control, transitions_base, num_pairs, init_sr_control, init_weight_control, state_list, action_list, RPE_list, weight_list, epi_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0. 15.  0. 30.  0.  0.  0.  0.  0.  0.]\n",
      "[[1.   0.   0.93 0.04 0.   0.   0.9  0.04 0.   0.88 0.04 0.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.   0.18 0.79 0.   0.18 0.76 0.   0.17 0.74 0.   0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.97 0.   0.   0.94 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.   0.   0.   0.   0.97 0.   0.   0.94 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.   0.   0.   0.97 0.   0.   0.94 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.   0.   0.   0.97 0.   0.   0.94 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   1.   0.   0.   0.97 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.97 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.97 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "weight_base = rl_base[0]\n",
    "feat_base = rl_base[1]\n",
    "\n",
    "print(weight_base)\n",
    "print(np.around(feat_base, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Values\n",
      "[13.555 22.931]\n",
      "Reward Revaluation\n",
      "[40.665 22.931]\n",
      "Transition Revaluation\n",
      "[13.555 22.931]\n",
      "Policy Revaluation\n",
      "[17.285 20.684]\n",
      "Goal State Revaluation\n",
      "[39.852 22.931]\n",
      "Control\n",
      "[13.555 56.296]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Values\")\n",
    "v_state_base = np.zeros(num_pairs)\n",
    "for k in range(num_pairs):\n",
    "    v_state_base[k] = np.sum(rl_base[0]*rl_base[1][k])\n",
    "print(np.around(v_state_base[0:2], 3))\n",
    "\n",
    "print(\"Reward Revaluation\")\n",
    "v_state_reward = np.zeros(num_pairs)\n",
    "for k in range(num_pairs):\n",
    "    v_state_reward[k] = np.sum(rl_reward[0]*rl_reward[1][k])\n",
    "print(np.around(v_state_reward[0:2], 3))\n",
    "\n",
    "print(\"Transition Revaluation\")\n",
    "v_state_transition = np.zeros(num_pairs)\n",
    "for k in range(num_pairs):\n",
    "    v_state_transition[k] = np.sum(rl_transition[0]*rl_transition[1][k])\n",
    "print(np.around(v_state_transition[0:2], 3))\n",
    "\n",
    "print(\"Policy Revaluation\")\n",
    "v_state_policy = np.zeros(num_pairs)\n",
    "for k in range(num_pairs):\n",
    "    v_state_policy[k] = np.sum(rl_policy[0]*rl_policy[1][k])\n",
    "print(np.around(v_state_policy[0:2], 3))\n",
    "\n",
    "print(\"Goal State Revaluation\")\n",
    "v_state_goal = np.zeros(num_pairs)\n",
    "for k in range(num_pairs):\n",
    "    v_state_goal[k] = np.sum(rl_goal[0]*rl_goal[1][k])\n",
    "print(np.around(v_state_goal[0:2], 3))\n",
    "\n",
    "print(\"Control\")\n",
    "v_state_control = np.zeros(num_pairs)\n",
    "for k in range(num_pairs):\n",
    "    v_state_control[k] = np.sum(rl_control[0]*rl_control[1][k])\n",
    "print(np.around(v_state_control[0:2], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m weight \u001b[39m=\u001b[39m rl[\u001b[39m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m feat \u001b[39m=\u001b[39m rl[\u001b[39m1\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(weight)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rl' is not defined"
     ]
    }
   ],
   "source": [
    "weight = rl[0]\n",
    "feat = rl[1]\n",
    "\n",
    "print(weight)\n",
    "print(np.around(rl[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.55 29.1  15.    0.    0.   30.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n"
     ]
    }
   ],
   "source": [
    "v_state = np.zeros(num_pairs)\n",
    "for k in range(num_pairs):\n",
    "    v_state[k] = np.sum(rl[0]*rl[1][k])\n",
    "\n",
    "print(np.around(v_state, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Simulations\n",
    "\n",
    "sim_num = 100\n",
    "epi_num = 200\n",
    "gamma = 0.97\n",
    "alpha = 0.50\n",
    "state_n = 10\n",
    "stay_prob = 0.75\n",
    "state_list = []\n",
    "action_list = []\n",
    "RPE_list = []\n",
    "weight_list = [[] for k in range(state_n)]\n",
    "init_weight = []\n",
    "epi_num_list = []\n",
    "\n",
    "rl = fullSR3(sim_num, epi_num, gamma, alpha, state_n, init_weight, stay_prob, \n",
    "             state_list, action_list, RPE_list, weight_list, epi_num_list)\n",
    "\n",
    "# Create dataframe\n",
    "import pandas as pd\n",
    "\n",
    "result = \\\n",
    "pd.DataFrame({'Simulation': rl[6], 'Episode': rl[5], 'State': rl[1], 'Action': rl[2], \n",
    "              'RPE': rl[3], 'W1': rl[4][0], 'W2': rl[4][1], 'W3': rl[4][2], 'W4': rl[4][3], \n",
    "              'W5': rl[4][4], 'W6': rl[4][5], 'W7': rl[4][6], 'W8': rl[4][7], 'W9': rl[4][8], \n",
    "              'W10': rl[4][9]})\n",
    "\n",
    "# Convert dataframe to csv\n",
    "result.to_csv('./Full_SR/{}sim_{}epi_g{:.0f}_s{:.0f}_{:.0f}states.csv'.format(sim_num, epi_num, 100*gamma, 100*stay_prob, state_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
