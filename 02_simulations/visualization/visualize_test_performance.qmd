---
title: "Visualize simulation results"
doi: "https://doi.org/10.17605/OSF.IO/9TUZE"
author: "Milena Musial"
date: 01/29/2025
format: html
editor: visual
toc: TRUE
code-fold: TRUE
number-sections: TRUE
embed-resources: true
execute: 
  warning: FALSE
editor_options: 
  chunk_output_type: console
---

# Preparation

```{r}
rm(list = ls(all = TRUE))

# Load packages
packages <- c("ggplot2", "dplyr", "tidyr", "kableExtra", "gmodels", "lme4", "sjPlot", "viridis", "forcats", "lmerTest", "car", "ggpubr", "ggpattern")
#install.packages(packages)
sapply(packages, require, character.only = TRUE)

# load data
data_path <- "~/Library/CloudStorage/OneDrive-Charité-UniversitätsmedizinBerlin/PhD/04_B01/WP3/Reduced_SR_in_AUD/simulations/results"
figure_path <- "/Users/milenamusial/Library/CloudStorage/OneDrive-Charité-UniversitätsmedizinBerlin/PhD/04_B01/WP3/Project_Docs/Figures"

```

# Plots based on simulations

## Define functions

### Filter based on values and last 5 learning trials

```{r}
# define function to get learning performance
filter_model_df_learning <- function(df) {

  # get simulation numbers that had less than 3 out of the last 5 learning trials correct
  last5_3incorrect <- df %>%
    dplyr::select(simulation_number, condition, phase, trial, state, action, reward) %>%
    filter(phase %in% c("learning")) %>%
    mutate(correct_path = case_when((state == 1 & action == 2 & lead(action) == 2) ~ 1,
                                     (state == 1 & action == 2 & lead(action) == 1) ~ 0,
                                     (state == 1 & action == 1) ~ 0)) %>%
    filter(state=="1") %>%
    group_by(simulation_number, condition) %>%
    slice_tail(n = 5) %>%
    summarise(sum_correct = sum(correct_path)) %>%
    filter(sum_correct < 3) %>%
    mutate(cond_sim = paste(simulation_number, "_", condition, sep=""))


  df %>%
    dplyr::select(simulation_number, condition, phase, trial, state, action, reward) %>%
    mutate(cond_sim = paste(simulation_number, "_", condition, sep="")) %>%
    filter(phase %in% c("learning_test")) %>%
    mutate(correct_path = case_when((state == 1 &
                                       action == 2 &
                                       lead(action) == 2 &
                                       !cond_sim %in% last5_3incorrect$cond_sim) ~ 1,
                                    (state == 1 &
                                       action == 2 &
                                       lead(action) == 2 &
                                       cond_sim %in% last5_3incorrect$cond_sim) ~ 0,
                                    (state == 1 &
                                        action == 2 &
                                        lead(action) == 1) ~ 0,
                                    (state == 1 &
                                        action == 1) ~ 0)) %>%
    filter(state=="1") %>%
    mutate(condition = case_when(
        condition == "reward" ~ "Reward revaluation",
        condition == "transition" ~ "Transition revaluation",
        condition == "goal" ~ "Goal-state revaluation",
        condition == "policy" ~ "Policy revaluation",
        condition == "control" ~ "Control")) %>%
    mutate(condition = fct_relevel(as.factor(condition),
                                   c("Reward revaluation",
                                     "Goal-state revaluation",
                                     "Transition revaluation",
                                     "Policy revaluation",
                                     "Control")))
}


# define function to get revaluation performance
filter_model_df_revaluation <- function(df) {

  last5_3incorrect <- df %>%
    dplyr::select(simulation_number, condition, phase, trial, state, action, reward) %>%
    filter(phase %in% c("learning")) %>%
    mutate(correct_path = case_when((state == 1 & action == 2 & lead(action) == 2) ~ 1,
                                     (state == 1 & action == 2 & lead(action) == 1) ~ 0,
                                     (state == 1 & action == 1) ~ 0)) %>%
    filter(state=="1") %>%
    group_by(simulation_number, condition) %>%
    slice_tail(n = 5) %>%
    summarise(sum_correct = sum(correct_path)) %>%
    filter(sum_correct < 3) %>%
    mutate(cond_sim = paste(simulation_number, "_", condition, sep=""))

  correct_learning_df <- df %>%
    dplyr::select(simulation_number, condition, phase, trial, state, action, reward) %>%
    mutate(cond_sim = paste(simulation_number, "_", condition, sep="")) %>%
    filter(phase %in% c("learning_test")) %>%
    mutate(correct_path = case_when((state == 1 &
                                       action == 2 &
                                       lead(action) == 2 &
                                       !cond_sim %in% last5_3incorrect$cond_sim) ~ 1,
                                    (state == 1 &
                                       action == 2 &
                                       lead(action) == 2 &
                                       cond_sim %in% last5_3incorrect$cond_sim) ~ 0,
                                    (state == 1 &
                                        action == 2 &
                                        lead(action) == 1) ~ 0,
                                    (state == 1 &
                                        action == 1) ~ 0)) %>%
    filter(state=="1" & correct_path == 1) %>%
    mutate(condition = case_when(
        condition == "reward" ~ "Reward revaluation",
        condition == "transition" ~ "Transition revaluation",
        condition == "goal" ~ "Goal-state revaluation",
        condition == "policy" ~ "Policy revaluation",
        condition == "control" ~ "Control")) %>%
    mutate(condition = fct_relevel(as.factor(condition),
                                   c("Reward revaluation",
                                     "Goal-state revaluation",
                                     "Transition revaluation",
                                     "Policy revaluation",
                                     "Control")))

  df %>%
    dplyr::select(simulation_number, condition, phase, trial, state, action, reward) %>%
    mutate(cond_sim = paste(simulation_number, "_", condition, sep="")) %>%
    filter(phase %in% c("relearning_test")) %>%
    filter(cond_sim %in% correct_learning_df$cond_sim) %>%
    mutate(correct_path = case_when((condition == "control" & state == 1 & action == 2 & lead(action) == 2) ~ 1,
                                     (condition == "control" & state == 1 & action == 2 & lead(action) == 1) ~ 0,
                                     (condition == "control" & state == 1 & action == 1) ~ 0,

                                     (condition != "control" & state == 1 & action == 1 & lead(action) == 1) ~ 1,
                                     (condition != "control" & state == 1 & action == 1 & lead(action) == 2) ~ 0,
                                     (condition != "control" & state == 1 & action == 2) ~ 0)) %>%
    filter(state=="1") %>%
    mutate(switch_path = case_when(condition == "control" & correct_path == 1 ~ 0,
                                   condition == "control" & correct_path == 0 ~ 1,
                                   condition != "control" & correct_path == 1 ~ 1,
                                   condition != "control" & correct_path == 0 ~ 0)) %>%
    mutate(condition = case_when(
        condition == "reward" ~ "Reward revaluation",
        condition == "transition" ~ "Transition revaluation",
        condition == "goal" ~ "Goal-state revaluation",
        condition == "policy" ~ "Policy revaluation",
        condition == "control" ~ "Control")) %>%
    mutate(condition = fct_relevel(as.factor(condition),
                                   c("Reward revaluation",
                                     "Goal-state revaluation",
                                     "Transition revaluation",
                                     "Policy revaluation",
                                     "Control")))
}
```

<!-- ### Filter based on values only -->

<!-- ```{r} -->

<!-- # define function to get learning performance -->

<!-- filter_model_df_learning <- function(df) { -->

<!--   df %>% -->

<!--     dplyr::select(simulation_number, condition, phase, trial, state, action, reward) %>% -->

<!--     filter(phase %in% c("learning_test")) %>% -->

<!--     mutate(correct_path = case_when((state == 1 & -->

<!--                                        action == 2 & -->

<!--                                        lead(action) == 2) ~ 1, -->

<!--                                     (state == 1 & -->

<!--                                         action == 2 & -->

<!--                                         lead(action) == 1) ~ 0, -->

<!--                                     (state == 1 & -->

<!--                                         action == 1) ~ 0)) %>% -->

<!--     filter(state=="1") %>% -->

<!--     mutate(condition = case_when( -->

<!--         condition == "reward" ~ "Reward revaluation", -->

<!--         condition == "transition" ~ "Transition revaluation", -->

<!--         condition == "goal" ~ "Goal-state revaluation", -->

<!--         condition == "policy" ~ "Policy revaluation", -->

<!--         condition == "control" ~ "Control")) %>% -->

<!--     mutate(condition = fct_relevel(as.factor(condition), -->

<!--                                    c("Reward revaluation", -->

<!--                                      "Goal-state revaluation", -->

<!--                                      "Transition revaluation", -->

<!--                                      "Policy revaluation", -->

<!--                                      "Control"))) -->

<!-- } -->

<!-- # define function to get revaluation performance -->

<!-- filter_model_df_revaluation <- function(df) { -->

<!--   correct_learning_df <- df %>% -->

<!--     dplyr::select(simulation_number, condition, phase, trial, state, action, reward) %>% -->

<!--     filter(phase %in% c("learning_test")) %>% -->

<!--     mutate(correct_path = case_when((state == 1 & -->

<!--                                        action == 2 & -->

<!--                                        lead(action) == 2) ~ 1, -->

<!--                                     (state == 1 & -->

<!--                                         action == 2 & -->

<!--                                         lead(action) == 1) ~ 0, -->

<!--                                     (state == 1 & -->

<!--                                         action == 1) ~ 0)) %>% -->

<!--     filter(state=="1" & correct_path == 1) %>% -->

<!--     mutate(condition = case_when( -->

<!--         condition == "reward" ~ "Reward revaluation", -->

<!--         condition == "transition" ~ "Transition revaluation", -->

<!--         condition == "goal" ~ "Goal-state revaluation", -->

<!--         condition == "policy" ~ "Policy revaluation", -->

<!--         condition == "control" ~ "Control")) %>% -->

<!--     mutate(condition = fct_relevel(as.factor(condition), -->

<!--                                    c("Reward revaluation", -->

<!--                                      "Goal-state revaluation", -->

<!--                                      "Transition revaluation", -->

<!--                                      "Policy revaluation", -->

<!--                                      "Control"))) %>% -->

<!--     mutate(cond_sim = paste(simulation_number, "_", condition, sep="")) -->

<!--   df %>% -->

<!--     dplyr::select(simulation_number, condition, phase, trial, state, action, reward) %>% -->

<!--     filter(phase %in% c("relearning_test")) %>% -->

<!--     mutate(correct_path = case_when((condition == "control" & state == 1 & action == 2 & lead(action) == 2) ~ 1, -->

<!--                                      (condition == "control" & state == 1 & action == 2 & lead(action) == 1) ~ 0, -->

<!--                                      (condition == "control" & state == 1 & action == 1) ~ 0, -->

<!--                                      (condition != "control" & state == 1 & action == 1 & lead(action) == 1) ~ 1, -->

<!--                                      (condition != "control" & state == 1 & action == 1 & lead(action) == 2) ~ 0, -->

<!--                                      (condition != "control" & state == 1 & action == 2) ~ 0)) %>% -->

<!--     filter(state=="1") %>% -->

<!--     mutate(switch_path = case_when(condition == "control" & correct_path == 1 ~ 0, -->

<!--                                    condition == "control" & correct_path == 0 ~ 1, -->

<!--                                    condition != "control" & correct_path == 1 ~ 1, -->

<!--                                    condition != "control" & correct_path == 0 ~ 0)) %>% -->

<!--     mutate(condition = case_when( -->

<!--         condition == "reward" ~ "Reward revaluation", -->

<!--         condition == "transition" ~ "Transition revaluation", -->

<!--         condition == "goal" ~ "Goal-state revaluation", -->

<!--         condition == "policy" ~ "Policy revaluation", -->

<!--         condition == "control" ~ "Control")) %>% -->

<!--     mutate(condition = fct_relevel(as.factor(condition), -->

<!--                                    c("Reward revaluation", -->

<!--                                      "Goal-state revaluation", -->

<!--                                      "Transition revaluation", -->

<!--                                      "Policy revaluation", -->

<!--                                      "Control"))) %>% -->

<!--     mutate(cond_sim = paste(simulation_number, "_", condition, sep="")) %>% -->

<!--     filter(cond_sim %in% correct_learning_df$cond_sim) -->

<!-- } -->

<!-- ``` -->

## Read and handle data

### alpha 05

```{r}
# beta=0.9, alpha=0.5, gamma=0.5
mb_b1_a05_g05 <- read.csv(file.path(data_path, "model_based_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv"))
mf_b1_a05_g05 <- read.csv(file.path(data_path, "model_free_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv"))
full_sr_b1_a05_g05 <- read.csv(file.path(data_path, "full_sr_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv"))
red_sr_b1_a05_g05 <- read.csv(file.path(data_path, "reduced_sr_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv"))
red_sr_2_b1_a05_g05 <- read.csv(file.path(data_path, "reduced_sr_2goalstates_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv"))
red_sr_4_b1_a05_g05 <- read.csv(file.path(data_path, "reduced_sr_4goalstates_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv"))
random_sr_update_b1_a05_g05 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeatMfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv")) 
random_sr_rupdate_b1_a05_g05 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv")) 
random_sr_noupdate_b1_a05_g05 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wnoupdate_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv")) 
random_red_sr_b1_a05_g05 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv")) 
random_red_sr_late_b1_a05_g05 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv")) 
random_red_sr_2_b1_a05_g05 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv")) 
random_red_sr_2_late_b1_a05_g05 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv")) 
random_red_sr_4_b1_a05_g05 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv")) 
random_red_sr_4_late_b1_a05_g05 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.5.csv")) 


# beta=0.9, alpha=0.5, gamma=0.7
mb_b1_a05_g07 <- read.csv(file.path(data_path, "model_based_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv"))
mf_b1_a05_g07 <- read.csv(file.path(data_path, "model_free_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv"))
full_sr_b1_a05_g07 <- read.csv(file.path(data_path, "full_sr_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv"))
red_sr_b1_a05_g07 <- read.csv(file.path(data_path, "reduced_sr_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv"))
red_sr_2_b1_a05_g07 <- read.csv(file.path(data_path, "reduced_sr_2goalstates_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv"))
red_sr_4_b1_a05_g07 <- read.csv(file.path(data_path, "reduced_sr_4goalstates_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv"))
random_sr_update_b1_a05_g07 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeatMfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv")) 
random_sr_rupdate_b1_a05_g07 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv")) 
random_sr_noupdate_b1_a05_g07 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wnoupdate_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv")) 
random_red_sr_b1_a05_g07 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv")) 
random_red_sr_late_b1_a05_g07 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv")) 
random_red_sr_2_b1_a05_g07 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv")) 
random_red_sr_2_late_b1_a05_g07 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv")) 
random_red_sr_4_b1_a05_g07 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv")) 
random_red_sr_4_late_b1_a05_g07 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.7.csv")) 

# beta=0.9, alpha=0.5, gamma=0.9
mb_b1_a05_g09 <- read.csv(file.path(data_path, "model_based_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv"))
mf_b1_a05_g09 <- read.csv(file.path(data_path, "model_free_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv"))
full_sr_b1_a05_g09 <- read.csv(file.path(data_path, "full_sr_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv"))
red_sr_b1_a05_g09 <- read.csv(file.path(data_path, "reduced_sr_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv"))
red_sr_2_b1_a05_g09 <- read.csv(file.path(data_path, "reduced_sr_2goalstates_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv"))
red_sr_4_b1_a05_g09 <- read.csv(file.path(data_path, "reduced_sr_4goalstates_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv"))
random_sr_update_b1_a05_g09 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeatMfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv")) 
random_sr_rupdate_b1_a05_g09 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv")) 
random_sr_noupdate_b1_a05_g09 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wnoupdate_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv"))
random_red_sr_b1_a05_g09 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv")) 
random_red_sr_late_b1_a05_g09 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv")) 
random_red_sr_2_b1_a05_g09 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv")) 
random_red_sr_2_late_b1_a05_g09 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv")) 
random_red_sr_4_b1_a05_g09 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv")) 
random_red_sr_4_late_b1_a05_g09 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.5_alpha_m0.5_beta1_gamma0.9.csv")) 

filtered_dfs_learning <- list(mf_b1_a05_g05, mb_b1_a05_g05, full_sr_b1_a05_g05, red_sr_b1_a05_g05, red_sr_2_b1_a05_g05, red_sr_4_b1_a05_g05, random_sr_update_b1_a05_g05, random_sr_rupdate_b1_a05_g05, random_sr_noupdate_b1_a05_g05, random_red_sr_b1_a05_g05, random_red_sr_late_b1_a05_g05, random_red_sr_2_b1_a05_g05, random_red_sr_2_late_b1_a05_g05, random_red_sr_4_b1_a05_g05, random_red_sr_4_late_b1_a05_g05, 
                              mf_b1_a05_g07, mb_b1_a05_g07, full_sr_b1_a05_g07, red_sr_b1_a05_g07, red_sr_2_b1_a05_g07, red_sr_4_b1_a05_g07, random_sr_update_b1_a05_g07, random_sr_rupdate_b1_a05_g07, random_sr_noupdate_b1_a05_g07, random_red_sr_b1_a05_g07, random_red_sr_late_b1_a05_g07, random_red_sr_2_b1_a05_g07, random_red_sr_2_late_b1_a05_g07, random_red_sr_4_b1_a05_g07, random_red_sr_4_late_b1_a05_g07, 
                              mf_b1_a05_g09, mb_b1_a05_g09, full_sr_b1_a05_g09, red_sr_b1_a05_g09, red_sr_2_b1_a05_g09, red_sr_4_b1_a05_g09, random_sr_update_b1_a05_g09, random_sr_rupdate_b1_a05_g09, random_sr_noupdate_b1_a05_g09, random_red_sr_b1_a05_g09, random_red_sr_late_b1_a05_g09, random_red_sr_2_b1_a05_g09, random_red_sr_2_late_b1_a05_g09, random_red_sr_4_b1_a05_g09, random_red_sr_4_late_b1_a05_g09) %>%
    lapply(filter_model_df_learning)

filtered_dfs_revaluation <- list(mf_b1_a05_g05, mb_b1_a05_g05, full_sr_b1_a05_g05, red_sr_b1_a05_g05, red_sr_2_b1_a05_g05, red_sr_4_b1_a05_g05, random_sr_update_b1_a05_g05, random_sr_rupdate_b1_a05_g05, random_sr_noupdate_b1_a05_g05, random_red_sr_b1_a05_g05, random_red_sr_late_b1_a05_g05, random_red_sr_2_b1_a05_g05, random_red_sr_2_late_b1_a05_g05, random_red_sr_4_b1_a05_g05, random_red_sr_4_late_b1_a05_g05, 
                              mf_b1_a05_g07, mb_b1_a05_g07, full_sr_b1_a05_g07, red_sr_b1_a05_g07, red_sr_2_b1_a05_g07, red_sr_4_b1_a05_g07, random_sr_update_b1_a05_g07, random_sr_rupdate_b1_a05_g07, random_sr_noupdate_b1_a05_g07, random_red_sr_b1_a05_g07, random_red_sr_late_b1_a05_g07, random_red_sr_2_b1_a05_g07, random_red_sr_2_late_b1_a05_g07, random_red_sr_4_b1_a05_g07, random_red_sr_4_late_b1_a05_g07, 
                              mf_b1_a05_g09, mb_b1_a05_g09, full_sr_b1_a05_g09, red_sr_b1_a05_g09, red_sr_2_b1_a05_g09, red_sr_4_b1_a05_g09, random_sr_update_b1_a05_g09, random_sr_rupdate_b1_a05_g09, random_sr_noupdate_b1_a05_g09, random_red_sr_b1_a05_g09, random_red_sr_late_b1_a05_g09, random_red_sr_2_b1_a05_g09, random_red_sr_2_late_b1_a05_g09, random_red_sr_4_b1_a05_g09, random_red_sr_4_late_b1_a05_g09) %>%
    lapply(filter_model_df_revaluation)

mf_b1_a05_g05_learning <- filtered_dfs_learning[[1]]
mb_b1_a05_g05_learning <- filtered_dfs_learning[[2]]
full_sr_b1_a05_g05_learning <- filtered_dfs_learning[[3]]
red_sr_b1_a05_g05_learning <- filtered_dfs_learning[[4]]
red_sr_2_b1_a05_g05_learning <- filtered_dfs_learning[[5]]
red_sr_4_b1_a05_g05_learning <- filtered_dfs_learning[[6]]
random_sr_update_b1_a05_g05_learning <- filtered_dfs_learning[[7]]
random_sr_rupdate_b1_a05_g05_learning <- filtered_dfs_learning[[8]]
random_sr_noupdate_b1_a05_g05_learning <- filtered_dfs_learning[[9]]
random_red_sr_b1_a05_g05_learning <- filtered_dfs_learning[[10]]
random_red_sr_late_b1_a05_g05_learning <- filtered_dfs_learning[[11]]
random_red_sr_2_b1_a05_g05_learning <- filtered_dfs_learning[[12]]
random_red_sr_2_late_b1_a05_g05_learning <- filtered_dfs_learning[[13]]
random_red_sr_4_b1_a05_g05_learning <- filtered_dfs_learning[[14]]
random_red_sr_4_late_b1_a05_g05_learning <- filtered_dfs_learning[[15]]

mf_b1_a05_g07_learning <- filtered_dfs_learning[[16]]
mb_b1_a05_g07_learning <- filtered_dfs_learning[[17]]
full_sr_b1_a05_g07_learning <- filtered_dfs_learning[[18]]
red_sr_b1_a05_g07_learning <- filtered_dfs_learning[[19]]
red_sr_2_b1_a05_g07_learning <- filtered_dfs_learning[[20]]
red_sr_4_b1_a05_g07_learning <- filtered_dfs_learning[[21]]
random_sr_update_b1_a05_g07_learning <- filtered_dfs_learning[[22]]
random_sr_rupdate_b1_a05_g07_learning <- filtered_dfs_learning[[23]]
random_sr_noupdate_b1_a05_g07_learning <- filtered_dfs_learning[[24]]
random_red_sr_b1_a05_g07_learning <- filtered_dfs_learning[[25]]
random_red_sr_late_b1_a05_g07_learning <- filtered_dfs_learning[[26]]
random_red_sr_2_b1_a05_g07_learning <- filtered_dfs_learning[[27]]
random_red_sr_2_late_b1_a05_g07_learning <- filtered_dfs_learning[[28]]
random_red_sr_4_b1_a05_g07_learning <- filtered_dfs_learning[[29]]
random_red_sr_4_late_b1_a05_g07_learning <- filtered_dfs_learning[[30]]

mf_b1_a05_g09_learning <- filtered_dfs_learning[[31]]
mb_b1_a05_g09_learning <- filtered_dfs_learning[[32]]
full_sr_b1_a05_g09_learning <- filtered_dfs_learning[[33]]
red_sr_b1_a05_g09_learning <- filtered_dfs_learning[[34]]
red_sr_2_b1_a05_g09_learning <- filtered_dfs_learning[[35]]
red_sr_4_b1_a05_g09_learning <- filtered_dfs_learning[[36]]
random_sr_update_b1_a05_g09_learning <- filtered_dfs_learning[[37]]
random_sr_rupdate_b1_a05_g09_learning <- filtered_dfs_learning[[38]]
random_sr_noupdate_b1_a05_g09_learning <- filtered_dfs_learning[[39]]
random_red_sr_b1_a05_g09_learning <- filtered_dfs_learning[[40]]
random_red_sr_late_b1_a05_g09_learning <- filtered_dfs_learning[[41]]
random_red_sr_2_b1_a05_g09_learning <- filtered_dfs_learning[[42]]
random_red_sr_2_late_b1_a05_g09_learning <- filtered_dfs_learning[[43]]
random_red_sr_4_b1_a05_g09_learning <- filtered_dfs_learning[[44]]
random_red_sr_4_late_b1_a05_g09_learning <- filtered_dfs_learning[[45]]

mf_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[1]]
mb_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[2]]
full_sr_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[3]]
red_sr_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[4]]
red_sr_2_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[5]]
red_sr_4_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[6]]
random_sr_update_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[7]]
random_sr_rupdate_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[8]]
random_sr_noupdate_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[9]]
random_red_sr_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[10]]
random_red_sr_late_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[11]]
random_red_sr_2_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[12]]
random_red_sr_2_late_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[13]]
random_red_sr_4_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[14]]
random_red_sr_4_late_b1_a05_g05_revaluation <- filtered_dfs_revaluation[[15]]

mf_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[16]]
mb_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[17]]
full_sr_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[18]]
red_sr_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[19]]
red_sr_2_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[20]]
red_sr_4_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[21]]
random_sr_update_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[22]]
random_sr_rupdate_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[23]]
random_sr_noupdate_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[24]]
random_red_sr_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[25]]
random_red_sr_late_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[26]]
random_red_sr_2_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[27]]
random_red_sr_2_late_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[28]]
random_red_sr_4_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[29]]
random_red_sr_4_late_b1_a05_g07_revaluation <- filtered_dfs_revaluation[[30]]

mf_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[31]]
mb_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[32]]
full_sr_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[33]]
red_sr_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[34]]
red_sr_2_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[35]]
red_sr_4_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[36]]
random_sr_update_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[37]]
random_sr_rupdate_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[38]]
random_sr_noupdate_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[39]]
random_red_sr_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[40]]
random_red_sr_late_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[41]]
random_red_sr_2_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[42]]
random_red_sr_2_late_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[43]]
random_red_sr_4_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[44]]
random_red_sr_4_late_b1_a05_g09_revaluation <- filtered_dfs_revaluation[[45]]

rm(mf_b1_a05_g05, mb_b1_a05_g05, full_sr_b1_a05_g05, red_sr_b1_a05_g05, red_sr_2_b1_a05_g05, red_sr_4_b1_a05_g05, random_sr_update_b1_a05_g05, random_sr_rupdate_b1_a05_g05, random_sr_noupdate_b1_a05_g05, random_red_sr_b1_a05_g05, random_red_sr_late_b1_a05_g05, random_red_sr_2_b1_a05_g05, random_red_sr_2_late_b1_a05_g05, random_red_sr_4_b1_a05_g05, random_red_sr_4_late_b1_a05_g05, 
                              mf_b1_a05_g07, mb_b1_a05_g07, full_sr_b1_a05_g07, red_sr_b1_a05_g07, red_sr_2_b1_a05_g07, red_sr_4_b1_a05_g07, random_sr_update_b1_a05_g07, random_sr_rupdate_b1_a05_g07, random_sr_noupdate_b1_a05_g07, random_red_sr_b1_a05_g07, random_red_sr_late_b1_a05_g07, random_red_sr_2_b1_a05_g07, random_red_sr_2_late_b1_a05_g07, random_red_sr_4_b1_a05_g07, random_red_sr_4_late_b1_a05_g07, 
                              mf_b1_a05_g09, mb_b1_a05_g09, full_sr_b1_a05_g09, red_sr_b1_a05_g09, red_sr_2_b1_a05_g09, red_sr_4_b1_a05_g09, random_sr_update_b1_a05_g09, random_sr_rupdate_b1_a05_g09, random_sr_noupdate_b1_a05_g09, random_red_sr_b1_a05_g09, random_red_sr_late_b1_a05_g09, random_red_sr_2_b1_a05_g09, random_red_sr_2_late_b1_a05_g09, random_red_sr_4_b1_a05_g09, random_red_sr_4_late_b1_a05_g09,
             filtered_dfs_learning, filtered_dfs_revaluation)

# create additional variables
mf_b1_a05_g05_learning <- mf_b1_a05_g05_learning %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_b1_a05_g05_learning <- mb_b1_a05_g05_learning %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

full_sr_b1_a05_g05_learning <- full_sr_b1_a05_g05_learning %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_b1_a05_g05_learning <- red_sr_b1_a05_g05_learning %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a05_g05_learning <- red_sr_2_b1_a05_g05_learning %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a05_g05_learning <- red_sr_4_b1_a05_g05_learning %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a05_g05_learning <- random_sr_update_b1_a05_g05_learning %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a05_g05_learning <- random_sr_rupdate_b1_a05_g05_learning %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a05_g05_learning <- random_sr_noupdate_b1_a05_g05_learning %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a05_g05_learning <- random_red_sr_b1_a05_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a05_g05_learning <- random_red_sr_late_b1_a05_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a05_g05_learning <- random_red_sr_2_b1_a05_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a05_g05_learning <- random_red_sr_2_late_b1_a05_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a05_g05_learning <- random_red_sr_4_b1_a05_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a05_g05_learning <- random_red_sr_4_late_b1_a05_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")



mf_b1_a05_g05_revaluation <- mf_b1_a05_g05_revaluation %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_b1_a05_g05_revaluation<- mb_b1_a05_g05_revaluation %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

full_sr_b1_a05_g05_revaluation <- full_sr_b1_a05_g05_revaluation %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_b1_a05_g05_revaluation <- red_sr_b1_a05_g05_revaluation %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a05_g05_revaluation <- red_sr_2_b1_a05_g05_revaluation %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a05_g05_revaluation <- red_sr_4_b1_a05_g05_revaluation %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a05_g05_revaluation <- random_sr_update_b1_a05_g05_revaluation %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a05_g05_revaluation <- random_sr_rupdate_b1_a05_g05_revaluation %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a05_g05_revaluation <- random_sr_noupdate_b1_a05_g05_revaluation %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a05_g05_revaluation <- random_red_sr_b1_a05_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a05_g05_revaluation <- random_red_sr_late_b1_a05_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a05_g05_revaluation <- random_red_sr_2_b1_a05_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a05_g05_revaluation <- random_red_sr_2_late_b1_a05_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a05_g05_revaluation <- random_red_sr_4_b1_a05_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a05_g05_revaluation <- random_red_sr_4_late_b1_a05_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition") 




mf_b1_a05_g07_learning <- mf_b1_a05_g07_learning %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_b1_a05_g07_learning <- mb_b1_a05_g07_learning %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

full_sr_b1_a05_g07_learning <- full_sr_b1_a05_g07_learning %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_b1_a05_g07_learning <- red_sr_b1_a05_g07_learning %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a05_g07_learning <- red_sr_2_b1_a05_g07_learning %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a05_g07_learning <- red_sr_4_b1_a05_g07_learning %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a05_g07_learning <- random_sr_update_b1_a05_g07_learning %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a05_g07_learning <- random_sr_rupdate_b1_a05_g07_learning %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a05_g07_learning <- random_sr_noupdate_b1_a05_g07_learning %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a05_g07_learning <- random_red_sr_b1_a05_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a05_g07_learning <- random_red_sr_late_b1_a05_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a05_g07_learning <- random_red_sr_2_b1_a05_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a05_g07_learning <- random_red_sr_2_late_b1_a05_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a05_g07_learning <- random_red_sr_4_b1_a05_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a05_g07_learning <- random_red_sr_4_late_b1_a05_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")




mf_b1_a05_g07_revaluation <- mf_b1_a05_g07_revaluation %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_b1_a05_g07_revaluation<- mb_b1_a05_g07_revaluation %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

full_sr_b1_a05_g07_revaluation <- full_sr_b1_a05_g07_revaluation %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_b1_a05_g07_revaluation <- red_sr_b1_a05_g07_revaluation %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a05_g07_revaluation <- red_sr_2_b1_a05_g07_revaluation %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a05_g07_revaluation <- red_sr_4_b1_a05_g07_revaluation %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a05_g07_revaluation <- random_sr_update_b1_a05_g07_revaluation %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a05_g07_revaluation <- random_sr_rupdate_b1_a05_g07_revaluation %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a05_g07_revaluation <- random_sr_noupdate_b1_a05_g07_revaluation%>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a05_g07_revaluation <- random_red_sr_b1_a05_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a05_g07_revaluation <- random_red_sr_late_b1_a05_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a05_g07_revaluation <- random_red_sr_2_b1_a05_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a05_g07_revaluation <- random_red_sr_2_late_b1_a05_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a05_g07_revaluation <- random_red_sr_4_b1_a05_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a05_g07_revaluation <- random_red_sr_4_late_b1_a05_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition") 




mf_b1_a05_g09_learning <- mf_b1_a05_g09_learning %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_b1_a05_g09_learning <- mb_b1_a05_g09_learning %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

full_sr_b1_a05_g09_learning <- full_sr_b1_a05_g09_learning %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_b1_a05_g09_learning <- red_sr_b1_a05_g09_learning %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a05_g09_learning <- red_sr_2_b1_a05_g09_learning %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a05_g09_learning <- red_sr_4_b1_a05_g09_learning %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a05_g09_learning <- random_sr_update_b1_a05_g09_learning %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a05_g09_learning <- random_sr_rupdate_b1_a05_g09_learning %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a05_g09_learning <- random_sr_noupdate_b1_a05_g09_learning %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a05_g09_learning <- random_red_sr_b1_a05_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a05_g09_learning <- random_red_sr_late_b1_a05_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a05_g09_learning <- random_red_sr_2_b1_a05_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a05_g09_learning <- random_red_sr_2_late_b1_a05_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a05_g09_learning <- random_red_sr_4_b1_a05_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a05_g09_learning <- random_red_sr_4_late_b1_a05_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")



mf_b1_a05_g09_revaluation <- mf_b1_a05_g09_revaluation %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_b1_a05_g09_revaluation<- mb_b1_a05_g09_revaluation %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

full_sr_b1_a05_g09_revaluation <- full_sr_b1_a05_g09_revaluation %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_b1_a05_g09_revaluation <- red_sr_b1_a05_g09_revaluation %>%
  mutate(Model = "RedSR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a05_g09_revaluation <- red_sr_2_b1_a05_g09_revaluation %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a05_g09_revaluation <- red_sr_4_b1_a05_g09_revaluation %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a05_g09_revaluation <- random_sr_update_b1_a05_g09_revaluation %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a05_g09_revaluation <- random_sr_rupdate_b1_a05_g09_revaluation %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a05_g09_revaluation <- random_sr_noupdate_b1_a05_g09_revaluation%>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a05_g09_revaluation <- random_red_sr_b1_a05_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a05_g09_revaluation <- random_red_sr_late_b1_a05_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a05_g09_revaluation <- random_red_sr_2_b1_a05_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a05_g09_revaluation <- random_red_sr_2_late_b1_a05_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a05_g09_revaluation <- random_red_sr_4_b1_a05_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a05_g09_revaluation <- random_red_sr_4_late_b1_a05_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition") 



# merge dfs
df_alpha05_learning <- rbind(mf_b1_a05_g05_learning,
                    mb_b1_a05_g05_learning,
                    full_sr_b1_a05_g05_learning,
                    red_sr_b1_a05_g05_learning,
                    red_sr_2_b1_a05_g05_learning,
                    red_sr_4_b1_a05_g05_learning,
                    random_sr_update_b1_a05_g05_learning,
                    random_sr_rupdate_b1_a05_g05_learning,
                    random_sr_noupdate_b1_a05_g05_learning,
                    random_red_sr_b1_a05_g05_learning,
                    random_red_sr_late_b1_a05_g05_learning,
                    random_red_sr_2_b1_a05_g05_learning,
                    random_red_sr_2_late_b1_a05_g05_learning,
                    random_red_sr_4_b1_a05_g05_learning,
                    random_red_sr_4_late_b1_a05_g05_learning,
                    
                    mf_b1_a05_g07_learning,
                    mb_b1_a05_g07_learning,
                    full_sr_b1_a05_g07_learning,
                    red_sr_b1_a05_g07_learning,
                    red_sr_2_b1_a05_g07_learning,
                    red_sr_4_b1_a05_g07_learning,
                    random_sr_update_b1_a05_g07_learning,
                    random_sr_rupdate_b1_a05_g07_learning,
                    random_sr_noupdate_b1_a05_g07_learning,
                    random_red_sr_b1_a05_g07_learning,
                    random_red_sr_late_b1_a05_g07_learning,
                    random_red_sr_2_b1_a05_g07_learning,
                    random_red_sr_2_late_b1_a05_g07_learning,
                    random_red_sr_4_b1_a05_g07_learning,
                    random_red_sr_4_late_b1_a05_g07_learning,
                    
                    mf_b1_a05_g09_learning,
                    mb_b1_a05_g09_learning,
                    full_sr_b1_a05_g09_learning,
                    red_sr_b1_a05_g09_learning,
                    red_sr_2_b1_a05_g09_learning,
                    red_sr_4_b1_a05_g09_learning,
                    random_sr_update_b1_a05_g09_learning,
                    random_sr_rupdate_b1_a05_g09_learning,
                    random_sr_noupdate_b1_a05_g09_learning,
                    random_red_sr_b1_a05_g09_learning,
                    random_red_sr_late_b1_a05_g09_learning,
                    random_red_sr_2_b1_a05_g09_learning,
                    random_red_sr_2_late_b1_a05_g09_learning,
                    random_red_sr_4_b1_a05_g09_learning,
                    random_red_sr_4_late_b1_a05_g09_learning
                    )

df_alpha05_revaluation <- rbind(mf_b1_a05_g05_revaluation,
                    mb_b1_a05_g05_revaluation,
                    full_sr_b1_a05_g05_revaluation,
                    red_sr_b1_a05_g05_revaluation,
                    red_sr_2_b1_a05_g05_revaluation,
                    red_sr_4_b1_a05_g05_revaluation,
                    random_sr_update_b1_a05_g05_revaluation,
                    random_sr_rupdate_b1_a05_g05_revaluation,
                    random_sr_noupdate_b1_a05_g05_revaluation,
                    random_red_sr_b1_a05_g05_revaluation,
                    random_red_sr_late_b1_a05_g05_revaluation,
                    random_red_sr_2_b1_a05_g05_revaluation,
                    random_red_sr_2_late_b1_a05_g05_revaluation,
                    random_red_sr_4_b1_a05_g05_revaluation,
                    random_red_sr_4_late_b1_a05_g05_revaluation,
                    
                    mf_b1_a05_g07_revaluation,
                    mb_b1_a05_g07_revaluation,
                    full_sr_b1_a05_g07_revaluation,
                    red_sr_b1_a05_g07_revaluation,
                    red_sr_2_b1_a05_g07_revaluation,
                    red_sr_4_b1_a05_g07_revaluation,
                    random_sr_update_b1_a05_g07_revaluation,
                    random_sr_rupdate_b1_a05_g07_revaluation,
                    random_sr_noupdate_b1_a05_g07_revaluation,
                    random_red_sr_b1_a05_g07_revaluation,
                    random_red_sr_late_b1_a05_g07_revaluation,
                    random_red_sr_2_b1_a05_g07_revaluation,
                    random_red_sr_2_late_b1_a05_g07_revaluation,
                    random_red_sr_4_b1_a05_g07_revaluation,
                    random_red_sr_4_late_b1_a05_g07_revaluation,

                    mf_b1_a05_g09_revaluation,
                    mb_b1_a05_g09_revaluation,
                    full_sr_b1_a05_g09_revaluation,
                    red_sr_b1_a05_g09_revaluation,
                    red_sr_2_b1_a05_g09_revaluation,
                    red_sr_4_b1_a05_g09_revaluation,
                    random_sr_update_b1_a05_g09_revaluation,
                    random_sr_rupdate_b1_a05_g09_revaluation,
                    random_sr_noupdate_b1_a05_g09_revaluation,
                    random_red_sr_b1_a05_g09_revaluation,
                    random_red_sr_late_b1_a05_g09_revaluation,
                    random_red_sr_2_b1_a05_g09_revaluation,
                    random_red_sr_2_late_b1_a05_g09_revaluation,
                    random_red_sr_4_b1_a05_g09_revaluation,
                    random_red_sr_4_late_b1_a05_g09_revaluation
                    )

rm(mf_b1_a05_g05_learning,
    mb_b1_a05_g05_learning,
    full_sr_b1_a05_g05_learning,
    red_sr_b1_a05_g05_learning,
    red_sr_2_b1_a05_g05_learning,
    red_sr_4_b1_a05_g05_learning,
    random_sr_update_b1_a05_g05_learning,
    random_sr_rupdate_b1_a05_g05_learning,
    random_sr_noupdate_b1_a05_g05_learning,
    random_red_sr_b1_a05_g05_learning,
    random_red_sr_late_b1_a05_g05_learning,
    random_red_sr_2_b1_a05_g05_learning,
    random_red_sr_2_late_b1_a05_g05_learning,
    random_red_sr_4_b1_a05_g05_learning,
    random_red_sr_4_late_b1_a05_g05_learning,
    
    mf_b1_a05_g07_learning,
    mb_b1_a05_g07_learning,
    full_sr_b1_a05_g07_learning,
    red_sr_b1_a05_g07_learning,
    red_sr_2_b1_a05_g07_learning,
    red_sr_4_b1_a05_g07_learning,
    random_sr_update_b1_a05_g07_learning,
    random_sr_rupdate_b1_a05_g07_learning,
    random_sr_noupdate_b1_a05_g07_learning,
    random_red_sr_b1_a05_g07_learning,
    random_red_sr_late_b1_a05_g07_learning,
    random_red_sr_2_b1_a05_g07_learning,
    random_red_sr_2_late_b1_a05_g07_learning,
    random_red_sr_4_b1_a05_g07_learning,
    random_red_sr_4_late_b1_a05_g07_learning,
    
    mf_b1_a05_g09_learning,
    mb_b1_a05_g09_learning,
    full_sr_b1_a05_g09_learning,
    red_sr_b1_a05_g09_learning,
    red_sr_2_b1_a05_g09_learning,
    red_sr_4_b1_a05_g09_learning,
    random_sr_update_b1_a05_g09_learning,
    random_sr_rupdate_b1_a05_g09_learning,
    random_sr_noupdate_b1_a05_g09_learning,
    random_red_sr_b1_a05_g09_learning,
    random_red_sr_late_b1_a05_g09_learning,
    random_red_sr_2_b1_a05_g09_learning,
    random_red_sr_2_late_b1_a05_g09_learning,
    random_red_sr_4_b1_a05_g09_learning,
    random_red_sr_4_late_b1_a05_g09_learning,
   
    mf_b1_a05_g05_revaluation,
    mb_b1_a05_g05_revaluation,
    full_sr_b1_a05_g05_revaluation,
    red_sr_b1_a05_g05_revaluation,
    red_sr_2_b1_a05_g05_revaluation,
    red_sr_4_b1_a05_g05_revaluation,
    random_sr_update_b1_a05_g05_revaluation,
    random_sr_rupdate_b1_a05_g05_revaluation,
    random_sr_noupdate_b1_a05_g05_revaluation,
    random_red_sr_b1_a05_g05_revaluation,
    random_red_sr_late_b1_a05_g05_revaluation,
    random_red_sr_2_b1_a05_g05_revaluation,
    random_red_sr_2_late_b1_a05_g05_revaluation,
    random_red_sr_4_b1_a05_g05_revaluation,
    random_red_sr_4_late_b1_a05_g05_revaluation,
    
    mf_b1_a05_g07_revaluation,
    mb_b1_a05_g07_revaluation,
    full_sr_b1_a05_g07_revaluation,
    red_sr_b1_a05_g07_revaluation,
    red_sr_2_b1_a05_g07_revaluation,
    red_sr_4_b1_a05_g07_revaluation,
    random_sr_update_b1_a05_g07_revaluation,
    random_sr_rupdate_b1_a05_g07_revaluation,
    random_sr_noupdate_b1_a05_g07_revaluation,
    random_red_sr_b1_a05_g07_revaluation,
    random_red_sr_late_b1_a05_g07_revaluation,
    random_red_sr_2_b1_a05_g07_revaluation,
    random_red_sr_2_late_b1_a05_g07_revaluation,
    random_red_sr_4_b1_a05_g07_revaluation,
    random_red_sr_4_late_b1_a05_g07_revaluation,

    mf_b1_a05_g09_revaluation,
    mb_b1_a05_g09_revaluation,
    full_sr_b1_a05_g09_revaluation,
    red_sr_b1_a05_g09_revaluation,
    red_sr_2_b1_a05_g09_revaluation,
    red_sr_4_b1_a05_g09_revaluation,
    random_sr_update_b1_a05_g09_revaluation,
    random_sr_rupdate_b1_a05_g09_revaluation,
    random_sr_noupdate_b1_a05_g09_revaluation,
    random_red_sr_b1_a05_g09_revaluation,
    random_red_sr_late_b1_a05_g09_revaluation,
    random_red_sr_2_b1_a05_g09_revaluation,
    random_red_sr_2_late_b1_a05_g09_revaluation,
    random_red_sr_4_b1_a05_g09_revaluation,
    random_red_sr_4_late_b1_a05_g09_revaluation
   
   )
```

### alpha 07

```{r}
# beta=0.9, alpha=0.7, gamma=0.5
mb_b1_a07_g05 <- read.csv(file.path(data_path, "model_based_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv"))
mf_b1_a07_g05 <- read.csv(file.path(data_path, "model_free_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv"))
full_sr_b1_a07_g05 <- read.csv(file.path(data_path, "full_sr_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv"))
red_sr_b1_a07_g05 <- read.csv(file.path(data_path, "reduced_sr_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv"))
red_sr_2_b1_a07_g05 <- read.csv(file.path(data_path, "reduced_sr_2goalstates_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv"))
red_sr_4_b1_a07_g05 <- read.csv(file.path(data_path, "reduced_sr_4goalstates_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv"))
random_sr_update_b1_a07_g05 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeatMfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv")) 
random_sr_rupdate_b1_a07_g05 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv")) 
random_sr_noupdate_b1_a07_g05 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wnoupdate_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv")) 
random_red_sr_b1_a07_g05 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv")) 
random_red_sr_late_b1_a07_g05 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv")) 
random_red_sr_2_b1_a07_g05 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv")) 
random_red_sr_2_late_b1_a07_g05 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv")) 
random_red_sr_4_b1_a07_g05 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv")) 
random_red_sr_4_late_b1_a07_g05 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.5.csv")) 


# beta=0.9, alpha=0.7, gamma=0.7
mb_b1_a07_g07 <- read.csv(file.path(data_path, "model_based_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv"))
mf_b1_a07_g07 <- read.csv(file.path(data_path, "model_free_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv"))
full_sr_b1_a07_g07 <- read.csv(file.path(data_path, "full_sr_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv"))
red_sr_b1_a07_g07 <- read.csv(file.path(data_path, "reduced_sr_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv"))
red_sr_2_b1_a07_g07 <- read.csv(file.path(data_path, "reduced_sr_2goalstates_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv"))
red_sr_4_b1_a07_g07 <- read.csv(file.path(data_path, "reduced_sr_4goalstates_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv"))
random_sr_update_b1_a07_g07 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeatMfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv")) 
random_sr_rupdate_b1_a07_g07 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv")) 
random_sr_noupdate_b1_a07_g07 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wnoupdate_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv")) 
random_red_sr_b1_a07_g07 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv")) 
random_red_sr_late_b1_a07_g07 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv")) 
random_red_sr_2_b1_a07_g07 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv")) 
random_red_sr_2_late_b1_a07_g07 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv")) 
random_red_sr_4_b1_a07_g07 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv")) 
random_red_sr_4_late_b1_a07_g07 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.7.csv")) 

# beta=0.9, alpha=0.7, gamma=0.9
mb_b1_a07_g09 <- read.csv(file.path(data_path, "model_based_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv"))
mf_b1_a07_g09 <- read.csv(file.path(data_path, "model_free_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv"))
full_sr_b1_a07_g09 <- read.csv(file.path(data_path, "full_sr_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv"))
red_sr_b1_a07_g09 <- read.csv(file.path(data_path, "reduced_sr_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv"))
red_sr_2_b1_a07_g09 <- read.csv(file.path(data_path, "reduced_sr_2goalstates_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv"))
red_sr_4_b1_a07_g09 <- read.csv(file.path(data_path, "reduced_sr_4goalstates_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv"))
random_sr_update_b1_a07_g09 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeatMfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv")) 
random_sr_rupdate_b1_a07_g09 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv")) 
random_sr_noupdate_b1_a07_g09 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wnoupdate_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv"))
random_red_sr_b1_a07_g09 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv")) 
random_red_sr_late_b1_a07_g09 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv")) 
random_red_sr_2_b1_a07_g09 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv")) 
random_red_sr_2_late_b1_a07_g09 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv")) 
random_red_sr_4_b1_a07_g09 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv")) 
random_red_sr_4_late_b1_a07_g09 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.7_alpha_m0.7_beta1_gamma0.9.csv")) 

filtered_dfs_learning <- list(mf_b1_a07_g05, mb_b1_a07_g05, full_sr_b1_a07_g05, red_sr_b1_a07_g05, red_sr_2_b1_a07_g05, red_sr_4_b1_a07_g05, random_sr_update_b1_a07_g05, random_sr_rupdate_b1_a07_g05, random_sr_noupdate_b1_a07_g05, random_red_sr_b1_a07_g05, random_red_sr_late_b1_a07_g05, random_red_sr_2_b1_a07_g05, random_red_sr_2_late_b1_a07_g05, random_red_sr_4_b1_a07_g05, random_red_sr_4_late_b1_a07_g05, 
                              mf_b1_a07_g07, mb_b1_a07_g07, full_sr_b1_a07_g07, red_sr_b1_a07_g07, red_sr_2_b1_a07_g07, red_sr_4_b1_a07_g07, random_sr_update_b1_a07_g07, random_sr_rupdate_b1_a07_g07, random_sr_noupdate_b1_a07_g07, random_red_sr_b1_a07_g07, random_red_sr_late_b1_a07_g07, random_red_sr_2_b1_a07_g07, random_red_sr_2_late_b1_a07_g07, random_red_sr_4_b1_a07_g07, random_red_sr_4_late_b1_a07_g07, 
                              mf_b1_a07_g09, mb_b1_a07_g09, full_sr_b1_a07_g09, red_sr_b1_a07_g09, red_sr_2_b1_a07_g09, red_sr_4_b1_a07_g09, random_sr_update_b1_a07_g09, random_sr_rupdate_b1_a07_g09, random_sr_noupdate_b1_a07_g09, random_red_sr_b1_a07_g09, random_red_sr_late_b1_a07_g09, random_red_sr_2_b1_a07_g09, random_red_sr_2_late_b1_a07_g09, random_red_sr_4_b1_a07_g09, random_red_sr_4_late_b1_a07_g09) %>%
    lapply(filter_model_df_learning)

filtered_dfs_revaluation <- list(mf_b1_a07_g05, mb_b1_a07_g05, full_sr_b1_a07_g05, red_sr_b1_a07_g05, red_sr_2_b1_a07_g05, red_sr_4_b1_a07_g05, random_sr_update_b1_a07_g05, random_sr_rupdate_b1_a07_g05, random_sr_noupdate_b1_a07_g05, random_red_sr_b1_a07_g05, random_red_sr_late_b1_a07_g05, random_red_sr_2_b1_a07_g05, random_red_sr_2_late_b1_a07_g05, random_red_sr_4_b1_a07_g05, random_red_sr_4_late_b1_a07_g05, 
                              mf_b1_a07_g07, mb_b1_a07_g07, full_sr_b1_a07_g07, red_sr_b1_a07_g07, red_sr_2_b1_a07_g07, red_sr_4_b1_a07_g07, random_sr_update_b1_a07_g07, random_sr_rupdate_b1_a07_g07, random_sr_noupdate_b1_a07_g07, random_red_sr_b1_a07_g07, random_red_sr_late_b1_a07_g07, random_red_sr_2_b1_a07_g07, random_red_sr_2_late_b1_a07_g07, random_red_sr_4_b1_a07_g07, random_red_sr_4_late_b1_a07_g07, 
                              mf_b1_a07_g09, mb_b1_a07_g09, full_sr_b1_a07_g09, red_sr_b1_a07_g09, red_sr_2_b1_a07_g09, red_sr_4_b1_a07_g09, random_sr_update_b1_a07_g09, random_sr_rupdate_b1_a07_g09, random_sr_noupdate_b1_a07_g09, random_red_sr_b1_a07_g09, random_red_sr_late_b1_a07_g09, random_red_sr_2_b1_a07_g09, random_red_sr_2_late_b1_a07_g09, random_red_sr_4_b1_a07_g09, random_red_sr_4_late_b1_a07_g09) %>%
    lapply(filter_model_df_revaluation)

mf_b1_a07_g05_learning <- filtered_dfs_learning[[1]]
mb_b1_a07_g05_learning <- filtered_dfs_learning[[2]]
full_sr_b1_a07_g05_learning <- filtered_dfs_learning[[3]]
red_sr_b1_a07_g05_learning <- filtered_dfs_learning[[4]]
red_sr_2_b1_a07_g05_learning <- filtered_dfs_learning[[5]]
red_sr_4_b1_a07_g05_learning <- filtered_dfs_learning[[6]]
random_sr_update_b1_a07_g05_learning <- filtered_dfs_learning[[7]]
random_sr_rupdate_b1_a07_g05_learning <- filtered_dfs_learning[[8]]
random_sr_noupdate_b1_a07_g05_learning <- filtered_dfs_learning[[9]]
random_red_sr_b1_a07_g05_learning <- filtered_dfs_learning[[10]]
random_red_sr_late_b1_a07_g05_learning <- filtered_dfs_learning[[11]]
random_red_sr_2_b1_a07_g05_learning <- filtered_dfs_learning[[12]]
random_red_sr_2_late_b1_a07_g05_learning <- filtered_dfs_learning[[13]]
random_red_sr_4_b1_a07_g05_learning <- filtered_dfs_learning[[14]]
random_red_sr_4_late_b1_a07_g05_learning <- filtered_dfs_learning[[15]]

mf_b1_a07_g07_learning <- filtered_dfs_learning[[16]]
mb_b1_a07_g07_learning <- filtered_dfs_learning[[17]]
full_sr_b1_a07_g07_learning <- filtered_dfs_learning[[18]]
red_sr_b1_a07_g07_learning <- filtered_dfs_learning[[19]]
red_sr_2_b1_a07_g07_learning <- filtered_dfs_learning[[20]]
red_sr_4_b1_a07_g07_learning <- filtered_dfs_learning[[21]]
random_sr_update_b1_a07_g07_learning <- filtered_dfs_learning[[22]]
random_sr_rupdate_b1_a07_g07_learning <- filtered_dfs_learning[[23]]
random_sr_noupdate_b1_a07_g07_learning <- filtered_dfs_learning[[24]]
random_red_sr_b1_a07_g07_learning <- filtered_dfs_learning[[25]]
random_red_sr_late_b1_a07_g07_learning <- filtered_dfs_learning[[26]]
random_red_sr_2_b1_a07_g07_learning <- filtered_dfs_learning[[27]]
random_red_sr_2_late_b1_a07_g07_learning <- filtered_dfs_learning[[28]]
random_red_sr_4_b1_a07_g07_learning <- filtered_dfs_learning[[29]]
random_red_sr_4_late_b1_a07_g07_learning <- filtered_dfs_learning[[30]]

mf_b1_a07_g09_learning <- filtered_dfs_learning[[31]]
mb_b1_a07_g09_learning <- filtered_dfs_learning[[32]]
full_sr_b1_a07_g09_learning <- filtered_dfs_learning[[33]]
red_sr_b1_a07_g09_learning <- filtered_dfs_learning[[34]]
red_sr_2_b1_a07_g09_learning <- filtered_dfs_learning[[35]]
red_sr_4_b1_a07_g09_learning <- filtered_dfs_learning[[36]]
random_sr_update_b1_a07_g09_learning <- filtered_dfs_learning[[37]]
random_sr_rupdate_b1_a07_g09_learning <- filtered_dfs_learning[[38]]
random_sr_noupdate_b1_a07_g09_learning <- filtered_dfs_learning[[39]]
random_red_sr_b1_a07_g09_learning <- filtered_dfs_learning[[40]]
random_red_sr_late_b1_a07_g09_learning <- filtered_dfs_learning[[41]]
random_red_sr_2_b1_a07_g09_learning <- filtered_dfs_learning[[42]]
random_red_sr_2_late_b1_a07_g09_learning <- filtered_dfs_learning[[43]]
random_red_sr_4_b1_a07_g09_learning <- filtered_dfs_learning[[44]]
random_red_sr_4_late_b1_a07_g09_learning <- filtered_dfs_learning[[45]]

mf_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[1]]
mb_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[2]]
full_sr_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[3]]
red_sr_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[4]]
red_sr_2_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[5]]
red_sr_4_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[6]]
random_sr_update_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[7]]
random_sr_rupdate_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[8]]
random_sr_noupdate_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[9]]
random_red_sr_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[10]]
random_red_sr_late_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[11]]
random_red_sr_2_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[12]]
random_red_sr_2_late_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[13]]
random_red_sr_4_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[14]]
random_red_sr_4_late_b1_a07_g05_revaluation <- filtered_dfs_revaluation[[15]]

mf_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[16]]
mb_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[17]]
full_sr_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[18]]
red_sr_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[19]]
red_sr_2_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[20]]
red_sr_4_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[21]]
random_sr_update_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[22]]
random_sr_rupdate_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[23]]
random_sr_noupdate_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[24]]
random_red_sr_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[25]]
random_red_sr_late_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[26]]
random_red_sr_2_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[27]]
random_red_sr_2_late_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[28]]
random_red_sr_4_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[29]]
random_red_sr_4_late_b1_a07_g07_revaluation <- filtered_dfs_revaluation[[30]]

mf_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[31]]
mb_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[32]]
full_sr_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[33]]
red_sr_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[34]]
red_sr_2_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[35]]
red_sr_4_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[36]]
random_sr_update_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[37]]
random_sr_rupdate_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[38]]
random_sr_noupdate_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[39]]
random_red_sr_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[40]]
random_red_sr_late_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[41]]
random_red_sr_2_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[42]]
random_red_sr_2_late_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[43]]
random_red_sr_4_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[44]]
random_red_sr_4_late_b1_a07_g09_revaluation <- filtered_dfs_revaluation[[45]]

rm(mf_b1_a07_g05, mb_b1_a07_g05, full_sr_b1_a07_g05, red_sr_b1_a07_g05, red_sr_2_b1_a07_g05, red_sr_4_b1_a07_g05, random_sr_update_b1_a07_g05, random_sr_rupdate_b1_a07_g05, random_sr_noupdate_b1_a07_g05, random_red_sr_b1_a07_g05, random_red_sr_late_b1_a07_g05, random_red_sr_2_b1_a07_g05, random_red_sr_2_late_b1_a07_g05, random_red_sr_4_b1_a07_g05, random_red_sr_4_late_b1_a07_g05, 
                              mf_b1_a07_g07, mb_b1_a07_g07, full_sr_b1_a07_g07, red_sr_b1_a07_g07, red_sr_2_b1_a07_g07, red_sr_4_b1_a07_g07, random_sr_update_b1_a07_g07, random_sr_rupdate_b1_a07_g07, random_sr_noupdate_b1_a07_g07, random_red_sr_b1_a07_g07, random_red_sr_late_b1_a07_g07, random_red_sr_2_b1_a07_g07, random_red_sr_2_late_b1_a07_g07, random_red_sr_4_b1_a07_g07, random_red_sr_4_late_b1_a07_g07, 
                              mf_b1_a07_g09, mb_b1_a07_g09, full_sr_b1_a07_g09, red_sr_b1_a07_g09, red_sr_2_b1_a07_g09, red_sr_4_b1_a07_g09, random_sr_update_b1_a07_g09, random_sr_rupdate_b1_a07_g09, random_sr_noupdate_b1_a07_g09, random_red_sr_b1_a07_g09, random_red_sr_late_b1_a07_g09, random_red_sr_2_b1_a07_g09, random_red_sr_2_late_b1_a07_g09, random_red_sr_4_b1_a07_g09, random_red_sr_4_late_b1_a07_g09,
             filtered_dfs_learning, filtered_dfs_revaluation)

# create additional variables
mf_b1_a07_g05_learning <- mf_b1_a07_g05_learning %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_b1_a07_g05_learning <- mb_b1_a07_g05_learning %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

full_sr_b1_a07_g05_learning <- full_sr_b1_a07_g05_learning %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_b1_a07_g05_learning <- red_sr_b1_a07_g05_learning %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a07_g05_learning <- red_sr_2_b1_a07_g05_learning %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a07_g05_learning <- red_sr_4_b1_a07_g05_learning %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a07_g05_learning <- random_sr_update_b1_a07_g05_learning %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a07_g05_learning <- random_sr_rupdate_b1_a07_g05_learning %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a07_g05_learning <- random_sr_noupdate_b1_a07_g05_learning %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a07_g05_learning <- random_red_sr_b1_a07_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a07_g05_learning <- random_red_sr_late_b1_a07_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a07_g05_learning <- random_red_sr_2_b1_a07_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a07_g05_learning <- random_red_sr_2_late_b1_a07_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a07_g05_learning <- random_red_sr_4_b1_a07_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a07_g05_learning <- random_red_sr_4_late_b1_a07_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")



mf_b1_a07_g05_revaluation <- mf_b1_a07_g05_revaluation %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_b1_a07_g05_revaluation<- mb_b1_a07_g05_revaluation %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

full_sr_b1_a07_g05_revaluation <- full_sr_b1_a07_g05_revaluation %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_b1_a07_g05_revaluation <- red_sr_b1_a07_g05_revaluation %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a07_g05_revaluation <- red_sr_2_b1_a07_g05_revaluation %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a07_g05_revaluation <- red_sr_4_b1_a07_g05_revaluation %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a07_g05_revaluation <- random_sr_update_b1_a07_g05_revaluation %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a07_g05_revaluation <- random_sr_rupdate_b1_a07_g05_revaluation %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a07_g05_revaluation <- random_sr_noupdate_b1_a07_g05_revaluation %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a07_g05_revaluation <- random_red_sr_b1_a07_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a07_g05_revaluation <- random_red_sr_late_b1_a07_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a07_g05_revaluation <- random_red_sr_2_b1_a07_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a07_g05_revaluation <- random_red_sr_2_late_b1_a07_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a07_g05_revaluation <- random_red_sr_4_b1_a07_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a07_g05_revaluation <- random_red_sr_4_late_b1_a07_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.5) %>%
  rename("Condition" = "condition") 




mf_b1_a07_g07_learning <- mf_b1_a07_g07_learning %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_b1_a07_g07_learning <- mb_b1_a07_g07_learning %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

full_sr_b1_a07_g07_learning <- full_sr_b1_a07_g07_learning %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_b1_a07_g07_learning <- red_sr_b1_a07_g07_learning %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a07_g07_learning <- red_sr_2_b1_a07_g07_learning %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a07_g07_learning <- red_sr_4_b1_a07_g07_learning %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a07_g07_learning <- random_sr_update_b1_a07_g07_learning %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a07_g07_learning <- random_sr_rupdate_b1_a07_g07_learning %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a07_g07_learning <- random_sr_noupdate_b1_a07_g07_learning %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a07_g07_learning <- random_red_sr_b1_a07_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a07_g07_learning <- random_red_sr_late_b1_a07_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a07_g07_learning <- random_red_sr_2_b1_a07_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a07_g07_learning <- random_red_sr_2_late_b1_a07_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a07_g07_learning <- random_red_sr_4_b1_a07_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a07_g07_learning <- random_red_sr_4_late_b1_a07_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")




mf_b1_a07_g07_revaluation <- mf_b1_a07_g07_revaluation %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_b1_a07_g07_revaluation<- mb_b1_a07_g07_revaluation %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

full_sr_b1_a07_g07_revaluation <- full_sr_b1_a07_g07_revaluation %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_b1_a07_g07_revaluation <- red_sr_b1_a07_g07_revaluation %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a07_g07_revaluation <- red_sr_2_b1_a07_g07_revaluation %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a07_g07_revaluation <- red_sr_4_b1_a07_g07_revaluation %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a07_g07_revaluation <- random_sr_update_b1_a07_g07_revaluation %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a07_g07_revaluation <- random_sr_rupdate_b1_a07_g07_revaluation %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a07_g07_revaluation <- random_sr_noupdate_b1_a07_g07_revaluation%>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a07_g07_revaluation <- random_red_sr_b1_a07_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a07_g07_revaluation <- random_red_sr_late_b1_a07_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a07_g07_revaluation <- random_red_sr_2_b1_a07_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a07_g07_revaluation <- random_red_sr_2_late_b1_a07_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a07_g07_revaluation <- random_red_sr_4_b1_a07_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a07_g07_revaluation <- random_red_sr_4_late_b1_a07_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.7) %>%
  rename("Condition" = "condition") 




mf_b1_a07_g09_learning <- mf_b1_a07_g09_learning %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_b1_a07_g09_learning <- mb_b1_a07_g09_learning %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

full_sr_b1_a07_g09_learning <- full_sr_b1_a07_g09_learning %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_b1_a07_g09_learning <- red_sr_b1_a07_g09_learning %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a07_g09_learning <- red_sr_2_b1_a07_g09_learning %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a07_g09_learning <- red_sr_4_b1_a07_g09_learning %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a07_g09_learning <- random_sr_update_b1_a07_g09_learning %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a07_g09_learning <- random_sr_rupdate_b1_a07_g09_learning %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a07_g09_learning <- random_sr_noupdate_b1_a07_g09_learning %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a07_g09_learning <- random_red_sr_b1_a07_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a07_g09_learning <- random_red_sr_late_b1_a07_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a07_g09_learning <- random_red_sr_2_b1_a07_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a07_g09_learning <- random_red_sr_2_late_b1_a07_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a07_g09_learning <- random_red_sr_4_b1_a07_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a07_g09_learning <- random_red_sr_4_late_b1_a07_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")



mf_b1_a07_g09_revaluation <- mf_b1_a07_g09_revaluation %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_b1_a07_g09_revaluation<- mb_b1_a07_g09_revaluation %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

full_sr_b1_a07_g09_revaluation <- full_sr_b1_a07_g09_revaluation %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_b1_a07_g09_revaluation <- red_sr_b1_a07_g09_revaluation %>%
  mutate(Model = "RedSR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a07_g09_revaluation <- red_sr_2_b1_a07_g09_revaluation %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a07_g09_revaluation <- red_sr_4_b1_a07_g09_revaluation %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a07_g09_revaluation <- random_sr_update_b1_a07_g09_revaluation %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a07_g09_revaluation <- random_sr_rupdate_b1_a07_g09_revaluation %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a07_g09_revaluation <- random_sr_noupdate_b1_a07_g09_revaluation%>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a07_g09_revaluation <- random_red_sr_b1_a07_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a07_g09_revaluation <- random_red_sr_late_b1_a07_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a07_g09_revaluation <- random_red_sr_2_b1_a07_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a07_g09_revaluation <- random_red_sr_2_late_b1_a07_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a07_g09_revaluation <- random_red_sr_4_b1_a07_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a07_g09_revaluation <- random_red_sr_4_late_b1_a07_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.7,
         gamma = 0.9) %>%
  rename("Condition" = "condition") 



# merge dfs
df_alpha07_learning <- rbind(mf_b1_a07_g05_learning,
                    mb_b1_a07_g05_learning,
                    full_sr_b1_a07_g05_learning,
                    red_sr_b1_a07_g05_learning,
                    red_sr_2_b1_a07_g05_learning,
                    red_sr_4_b1_a07_g05_learning,
                    random_sr_update_b1_a07_g05_learning,
                    random_sr_rupdate_b1_a07_g05_learning,
                    random_sr_noupdate_b1_a07_g05_learning,
                    random_red_sr_b1_a07_g05_learning,
                    random_red_sr_late_b1_a07_g05_learning,
                    random_red_sr_2_b1_a07_g05_learning,
                    random_red_sr_2_late_b1_a07_g05_learning,
                    random_red_sr_4_b1_a07_g05_learning,
                    random_red_sr_4_late_b1_a07_g05_learning,
                    
                    mf_b1_a07_g07_learning,
                    mb_b1_a07_g07_learning,
                    full_sr_b1_a07_g07_learning,
                    red_sr_b1_a07_g07_learning,
                    red_sr_2_b1_a07_g07_learning,
                    red_sr_4_b1_a07_g07_learning,
                    random_sr_update_b1_a07_g07_learning,
                    random_sr_rupdate_b1_a07_g07_learning,
                    random_sr_noupdate_b1_a07_g07_learning,
                    random_red_sr_b1_a07_g07_learning,
                    random_red_sr_late_b1_a07_g07_learning,
                    random_red_sr_2_b1_a07_g07_learning,
                    random_red_sr_2_late_b1_a07_g07_learning,
                    random_red_sr_4_b1_a07_g07_learning,
                    random_red_sr_4_late_b1_a07_g07_learning,
                    
                    mf_b1_a07_g09_learning,
                    mb_b1_a07_g09_learning,
                    full_sr_b1_a07_g09_learning,
                    red_sr_b1_a07_g09_learning,
                    red_sr_2_b1_a07_g09_learning,
                    red_sr_4_b1_a07_g09_learning,
                    random_sr_update_b1_a07_g09_learning,
                    random_sr_rupdate_b1_a07_g09_learning,
                    random_sr_noupdate_b1_a07_g09_learning,
                    random_red_sr_b1_a07_g09_learning,
                    random_red_sr_late_b1_a07_g09_learning,
                    random_red_sr_2_b1_a07_g09_learning,
                    random_red_sr_2_late_b1_a07_g09_learning,
                    random_red_sr_4_b1_a07_g09_learning,
                    random_red_sr_4_late_b1_a07_g09_learning
                    )

df_alpha07_revaluation <- rbind(mf_b1_a07_g05_revaluation,
                    mb_b1_a07_g05_revaluation,
                    full_sr_b1_a07_g05_revaluation,
                    red_sr_b1_a07_g05_revaluation,
                    red_sr_2_b1_a07_g05_revaluation,
                    red_sr_4_b1_a07_g05_revaluation,
                    random_sr_update_b1_a07_g05_revaluation,
                    random_sr_rupdate_b1_a07_g05_revaluation,
                    random_sr_noupdate_b1_a07_g05_revaluation,
                    random_red_sr_b1_a07_g05_revaluation,
                    random_red_sr_late_b1_a07_g05_revaluation,
                    random_red_sr_2_b1_a07_g05_revaluation,
                    random_red_sr_2_late_b1_a07_g05_revaluation,
                    random_red_sr_4_b1_a07_g05_revaluation,
                    random_red_sr_4_late_b1_a07_g05_revaluation,
                    
                    mf_b1_a07_g07_revaluation,
                    mb_b1_a07_g07_revaluation,
                    full_sr_b1_a07_g07_revaluation,
                    red_sr_b1_a07_g07_revaluation,
                    red_sr_2_b1_a07_g07_revaluation,
                    red_sr_4_b1_a07_g07_revaluation,
                    random_sr_update_b1_a07_g07_revaluation,
                    random_sr_rupdate_b1_a07_g07_revaluation,
                    random_sr_noupdate_b1_a07_g07_revaluation,
                    random_red_sr_b1_a07_g07_revaluation,
                    random_red_sr_late_b1_a07_g07_revaluation,
                    random_red_sr_2_b1_a07_g07_revaluation,
                    random_red_sr_2_late_b1_a07_g07_revaluation,
                    random_red_sr_4_b1_a07_g07_revaluation,
                    random_red_sr_4_late_b1_a07_g07_revaluation,

                    mf_b1_a07_g09_revaluation,
                    mb_b1_a07_g09_revaluation,
                    full_sr_b1_a07_g09_revaluation,
                    red_sr_b1_a07_g09_revaluation,
                    red_sr_2_b1_a07_g09_revaluation,
                    red_sr_4_b1_a07_g09_revaluation,
                    random_sr_update_b1_a07_g09_revaluation,
                    random_sr_rupdate_b1_a07_g09_revaluation,
                    random_sr_noupdate_b1_a07_g09_revaluation,
                    random_red_sr_b1_a07_g09_revaluation,
                    random_red_sr_late_b1_a07_g09_revaluation,
                    random_red_sr_2_b1_a07_g09_revaluation,
                    random_red_sr_2_late_b1_a07_g09_revaluation,
                    random_red_sr_4_b1_a07_g09_revaluation,
                    random_red_sr_4_late_b1_a07_g09_revaluation
                    )

rm(mf_b1_a07_g05_learning,
    mb_b1_a07_g05_learning,
    full_sr_b1_a07_g05_learning,
    red_sr_b1_a07_g05_learning,
    red_sr_2_b1_a07_g05_learning,
    red_sr_4_b1_a07_g05_learning,
    random_sr_update_b1_a07_g05_learning,
    random_sr_rupdate_b1_a07_g05_learning,
    random_sr_noupdate_b1_a07_g05_learning,
    random_red_sr_b1_a07_g05_learning,
    random_red_sr_late_b1_a07_g05_learning,
    random_red_sr_2_b1_a07_g05_learning,
    random_red_sr_2_late_b1_a07_g05_learning,
    random_red_sr_4_b1_a07_g05_learning,
    random_red_sr_4_late_b1_a07_g05_learning,
    
    mf_b1_a07_g07_learning,
    mb_b1_a07_g07_learning,
    full_sr_b1_a07_g07_learning,
    red_sr_b1_a07_g07_learning,
    red_sr_2_b1_a07_g07_learning,
    red_sr_4_b1_a07_g07_learning,
    random_sr_update_b1_a07_g07_learning,
    random_sr_rupdate_b1_a07_g07_learning,
    random_sr_noupdate_b1_a07_g07_learning,
    random_red_sr_b1_a07_g07_learning,
    random_red_sr_late_b1_a07_g07_learning,
    random_red_sr_2_b1_a07_g07_learning,
    random_red_sr_2_late_b1_a07_g07_learning,
    random_red_sr_4_b1_a07_g07_learning,
    random_red_sr_4_late_b1_a07_g07_learning,
    
    mf_b1_a07_g09_learning,
    mb_b1_a07_g09_learning,
    full_sr_b1_a07_g09_learning,
    red_sr_b1_a07_g09_learning,
    red_sr_2_b1_a07_g09_learning,
    red_sr_4_b1_a07_g09_learning,
    random_sr_update_b1_a07_g09_learning,
    random_sr_rupdate_b1_a07_g09_learning,
    random_sr_noupdate_b1_a07_g09_learning,
    random_red_sr_b1_a07_g09_learning,
    random_red_sr_late_b1_a07_g09_learning,
    random_red_sr_2_b1_a07_g09_learning,
    random_red_sr_2_late_b1_a07_g09_learning,
    random_red_sr_4_b1_a07_g09_learning,
    random_red_sr_4_late_b1_a07_g09_learning,
   
    mf_b1_a07_g05_revaluation,
    mb_b1_a07_g05_revaluation,
    full_sr_b1_a07_g05_revaluation,
    red_sr_b1_a07_g05_revaluation,
    red_sr_2_b1_a07_g05_revaluation,
    red_sr_4_b1_a07_g05_revaluation,
    random_sr_update_b1_a07_g05_revaluation,
    random_sr_rupdate_b1_a07_g05_revaluation,
    random_sr_noupdate_b1_a07_g05_revaluation,
    random_red_sr_b1_a07_g05_revaluation,
    random_red_sr_late_b1_a07_g05_revaluation,
    random_red_sr_2_b1_a07_g05_revaluation,
    random_red_sr_2_late_b1_a07_g05_revaluation,
    random_red_sr_4_b1_a07_g05_revaluation,
    random_red_sr_4_late_b1_a07_g05_revaluation,
    
    mf_b1_a07_g07_revaluation,
    mb_b1_a07_g07_revaluation,
    full_sr_b1_a07_g07_revaluation,
    red_sr_b1_a07_g07_revaluation,
    red_sr_2_b1_a07_g07_revaluation,
    red_sr_4_b1_a07_g07_revaluation,
    random_sr_update_b1_a07_g07_revaluation,
    random_sr_rupdate_b1_a07_g07_revaluation,
    random_sr_noupdate_b1_a07_g07_revaluation,
    random_red_sr_b1_a07_g07_revaluation,
    random_red_sr_late_b1_a07_g07_revaluation,
    random_red_sr_2_b1_a07_g07_revaluation,
    random_red_sr_2_late_b1_a07_g07_revaluation,
    random_red_sr_4_b1_a07_g07_revaluation,
    random_red_sr_4_late_b1_a07_g07_revaluation,

    mf_b1_a07_g09_revaluation,
    mb_b1_a07_g09_revaluation,
    full_sr_b1_a07_g09_revaluation,
    red_sr_b1_a07_g09_revaluation,
    red_sr_2_b1_a07_g09_revaluation,
    red_sr_4_b1_a07_g09_revaluation,
    random_sr_update_b1_a07_g09_revaluation,
    random_sr_rupdate_b1_a07_g09_revaluation,
    random_sr_noupdate_b1_a07_g09_revaluation,
    random_red_sr_b1_a07_g09_revaluation,
    random_red_sr_late_b1_a07_g09_revaluation,
    random_red_sr_2_b1_a07_g09_revaluation,
    random_red_sr_2_late_b1_a07_g09_revaluation,
    random_red_sr_4_b1_a07_g09_revaluation,
    random_red_sr_4_late_b1_a07_g09_revaluation
   
   )
```

### alpha 09

```{r}
# beta=0.9, alpha=0.9, gamma=0.5
mb_b1_a09_g05 <- read.csv(file.path(data_path, "model_based_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv"))
mf_b1_a09_g05 <- read.csv(file.path(data_path, "model_free_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv"))
full_sr_b1_a09_g05 <- read.csv(file.path(data_path, "full_sr_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv"))
red_sr_b1_a09_g05 <- read.csv(file.path(data_path, "reduced_sr_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv"))
red_sr_2_b1_a09_g05 <- read.csv(file.path(data_path, "reduced_sr_2goalstates_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv"))
red_sr_4_b1_a09_g05 <- read.csv(file.path(data_path, "reduced_sr_4goalstates_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv"))
random_sr_update_b1_a09_g05 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeatMfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv")) 
random_sr_rupdate_b1_a09_g05 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv")) 
random_sr_noupdate_b1_a09_g05 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wnoupdate_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv")) 
random_red_sr_b1_a09_g05 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv")) 
random_red_sr_late_b1_a09_g05 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv")) 
random_red_sr_2_b1_a09_g05 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv")) 
random_red_sr_2_late_b1_a09_g05 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv")) 
random_red_sr_4_b1_a09_g05 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv")) 
random_red_sr_4_late_b1_a09_g05 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.5.csv")) 


# beta=0.9, alpha=0.9, gamma=0.7
mb_b1_a09_g07 <- read.csv(file.path(data_path, "model_based_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv"))
mf_b1_a09_g07 <- read.csv(file.path(data_path, "model_free_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv"))
full_sr_b1_a09_g07 <- read.csv(file.path(data_path, "full_sr_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv"))
red_sr_b1_a09_g07 <- read.csv(file.path(data_path, "reduced_sr_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv"))
red_sr_2_b1_a09_g07 <- read.csv(file.path(data_path, "reduced_sr_2goalstates_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv"))
red_sr_4_b1_a09_g07 <- read.csv(file.path(data_path, "reduced_sr_4goalstates_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv"))
random_sr_update_b1_a09_g07 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeatMfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv")) 
random_sr_rupdate_b1_a09_g07 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv")) 
random_sr_noupdate_b1_a09_g07 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wnoupdate_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv")) 
random_red_sr_b1_a09_g07 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv")) 
random_red_sr_late_b1_a09_g07 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv")) 
random_red_sr_2_b1_a09_g07 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv")) 
random_red_sr_2_late_b1_a09_g07 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv")) 
random_red_sr_4_b1_a09_g07 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv")) 
random_red_sr_4_late_b1_a09_g07 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.7.csv")) 

# beta=0.9, alpha=0.9, gamma=0.9
mb_b1_a09_g09 <- read.csv(file.path(data_path, "model_based_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv"))
mf_b1_a09_g09 <- read.csv(file.path(data_path, "model_free_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv"))
full_sr_b1_a09_g09 <- read.csv(file.path(data_path, "full_sr_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv"))
red_sr_b1_a09_g09 <- read.csv(file.path(data_path, "reduced_sr_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv"))
red_sr_2_b1_a09_g09 <- read.csv(file.path(data_path, "reduced_sr_2goalstates_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv"))
red_sr_4_b1_a09_g09 <- read.csv(file.path(data_path, "reduced_sr_4goalstates_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv"))
random_sr_update_b1_a09_g09 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeatMfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv")) 
random_sr_rupdate_b1_a09_g09 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv")) 
random_sr_noupdate_b1_a09_g09 <- read.csv(file.path(data_path, "random_sr_from_mb_wTD_wnoupdate_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv"))
random_red_sr_b1_a09_g09 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv")) 
random_red_sr_late_b1_a09_g09 <- read.csv(file.path(data_path, "random_reduced_sr_1goalstate_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv")) 
random_red_sr_2_b1_a09_g09 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv")) 
random_red_sr_2_late_b1_a09_g09 <- read.csv(file.path(data_path, "random_reduced_sr_2goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv")) 
random_red_sr_4_b1_a09_g09 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv")) 
random_red_sr_4_late_b1_a09_g09 <- read.csv(file.path(data_path, "random_reduced_sr_4goalstates_from_mb_wTD_wfeat_late_nsimulations1000_alpha_td0.9_alpha_m0.9_beta1_gamma0.9.csv")) 

filtered_dfs_learning <- list(mf_b1_a09_g05, mb_b1_a09_g05, full_sr_b1_a09_g05, red_sr_b1_a09_g05, red_sr_2_b1_a09_g05, red_sr_4_b1_a09_g05, random_sr_update_b1_a09_g05, random_sr_rupdate_b1_a09_g05, random_sr_noupdate_b1_a09_g05, random_red_sr_b1_a09_g05, random_red_sr_late_b1_a09_g05, random_red_sr_2_b1_a09_g05, random_red_sr_2_late_b1_a09_g05, random_red_sr_4_b1_a09_g05, random_red_sr_4_late_b1_a09_g05, 
                              mf_b1_a09_g07, mb_b1_a09_g07, full_sr_b1_a09_g07, red_sr_b1_a09_g07, red_sr_2_b1_a09_g07, red_sr_4_b1_a09_g07, random_sr_update_b1_a09_g07, random_sr_rupdate_b1_a09_g07, random_sr_noupdate_b1_a09_g07, random_red_sr_b1_a09_g07, random_red_sr_late_b1_a09_g07, random_red_sr_2_b1_a09_g07, random_red_sr_2_late_b1_a09_g07, random_red_sr_4_b1_a09_g07, random_red_sr_4_late_b1_a09_g07, 
                              mf_b1_a09_g09, mb_b1_a09_g09, full_sr_b1_a09_g09, red_sr_b1_a09_g09, red_sr_2_b1_a09_g09, red_sr_4_b1_a09_g09, random_sr_update_b1_a09_g09, random_sr_rupdate_b1_a09_g09, random_sr_noupdate_b1_a09_g09, random_red_sr_b1_a09_g09, random_red_sr_late_b1_a09_g09, random_red_sr_2_b1_a09_g09, random_red_sr_2_late_b1_a09_g09, random_red_sr_4_b1_a09_g09, random_red_sr_4_late_b1_a09_g09) %>%
    lapply(filter_model_df_learning)

filtered_dfs_revaluation <- list(mf_b1_a09_g05, mb_b1_a09_g05, full_sr_b1_a09_g05, red_sr_b1_a09_g05, red_sr_2_b1_a09_g05, red_sr_4_b1_a09_g05, random_sr_update_b1_a09_g05, random_sr_rupdate_b1_a09_g05, random_sr_noupdate_b1_a09_g05, random_red_sr_b1_a09_g05, random_red_sr_late_b1_a09_g05, random_red_sr_2_b1_a09_g05, random_red_sr_2_late_b1_a09_g05, random_red_sr_4_b1_a09_g05, random_red_sr_4_late_b1_a09_g05, 
                              mf_b1_a09_g07, mb_b1_a09_g07, full_sr_b1_a09_g07, red_sr_b1_a09_g07, red_sr_2_b1_a09_g07, red_sr_4_b1_a09_g07, random_sr_update_b1_a09_g07, random_sr_rupdate_b1_a09_g07, random_sr_noupdate_b1_a09_g07, random_red_sr_b1_a09_g07, random_red_sr_late_b1_a09_g07, random_red_sr_2_b1_a09_g07, random_red_sr_2_late_b1_a09_g07, random_red_sr_4_b1_a09_g07, random_red_sr_4_late_b1_a09_g07, 
                              mf_b1_a09_g09, mb_b1_a09_g09, full_sr_b1_a09_g09, red_sr_b1_a09_g09, red_sr_2_b1_a09_g09, red_sr_4_b1_a09_g09, random_sr_update_b1_a09_g09, random_sr_rupdate_b1_a09_g09, random_sr_noupdate_b1_a09_g09, random_red_sr_b1_a09_g09, random_red_sr_late_b1_a09_g09, random_red_sr_2_b1_a09_g09, random_red_sr_2_late_b1_a09_g09, random_red_sr_4_b1_a09_g09, random_red_sr_4_late_b1_a09_g09) %>%
    lapply(filter_model_df_revaluation)

mf_b1_a09_g05_learning <- filtered_dfs_learning[[1]]
mb_b1_a09_g05_learning <- filtered_dfs_learning[[2]]
full_sr_b1_a09_g05_learning <- filtered_dfs_learning[[3]]
red_sr_b1_a09_g05_learning <- filtered_dfs_learning[[4]]
red_sr_2_b1_a09_g05_learning <- filtered_dfs_learning[[5]]
red_sr_4_b1_a09_g05_learning <- filtered_dfs_learning[[6]]
random_sr_update_b1_a09_g05_learning <- filtered_dfs_learning[[7]]
random_sr_rupdate_b1_a09_g05_learning <- filtered_dfs_learning[[8]]
random_sr_noupdate_b1_a09_g05_learning <- filtered_dfs_learning[[9]]
random_red_sr_b1_a09_g05_learning <- filtered_dfs_learning[[10]]
random_red_sr_late_b1_a09_g05_learning <- filtered_dfs_learning[[11]]
random_red_sr_2_b1_a09_g05_learning <- filtered_dfs_learning[[12]]
random_red_sr_2_late_b1_a09_g05_learning <- filtered_dfs_learning[[13]]
random_red_sr_4_b1_a09_g05_learning <- filtered_dfs_learning[[14]]
random_red_sr_4_late_b1_a09_g05_learning <- filtered_dfs_learning[[15]]

mf_b1_a09_g07_learning <- filtered_dfs_learning[[16]]
mb_b1_a09_g07_learning <- filtered_dfs_learning[[17]]
full_sr_b1_a09_g07_learning <- filtered_dfs_learning[[18]]
red_sr_b1_a09_g07_learning <- filtered_dfs_learning[[19]]
red_sr_2_b1_a09_g07_learning <- filtered_dfs_learning[[20]]
red_sr_4_b1_a09_g07_learning <- filtered_dfs_learning[[21]]
random_sr_update_b1_a09_g07_learning <- filtered_dfs_learning[[22]]
random_sr_rupdate_b1_a09_g07_learning <- filtered_dfs_learning[[23]]
random_sr_noupdate_b1_a09_g07_learning <- filtered_dfs_learning[[24]]
random_red_sr_b1_a09_g07_learning <- filtered_dfs_learning[[25]]
random_red_sr_late_b1_a09_g07_learning <- filtered_dfs_learning[[26]]
random_red_sr_2_b1_a09_g07_learning <- filtered_dfs_learning[[27]]
random_red_sr_2_late_b1_a09_g07_learning <- filtered_dfs_learning[[28]]
random_red_sr_4_b1_a09_g07_learning <- filtered_dfs_learning[[29]]
random_red_sr_4_late_b1_a09_g07_learning <- filtered_dfs_learning[[30]]

mf_b1_a09_g09_learning <- filtered_dfs_learning[[31]]
mb_b1_a09_g09_learning <- filtered_dfs_learning[[32]]
full_sr_b1_a09_g09_learning <- filtered_dfs_learning[[33]]
red_sr_b1_a09_g09_learning <- filtered_dfs_learning[[34]]
red_sr_2_b1_a09_g09_learning <- filtered_dfs_learning[[35]]
red_sr_4_b1_a09_g09_learning <- filtered_dfs_learning[[36]]
random_sr_update_b1_a09_g09_learning <- filtered_dfs_learning[[37]]
random_sr_rupdate_b1_a09_g09_learning <- filtered_dfs_learning[[38]]
random_sr_noupdate_b1_a09_g09_learning <- filtered_dfs_learning[[39]]
random_red_sr_b1_a09_g09_learning <- filtered_dfs_learning[[40]]
random_red_sr_late_b1_a09_g09_learning <- filtered_dfs_learning[[41]]
random_red_sr_2_b1_a09_g09_learning <- filtered_dfs_learning[[42]]
random_red_sr_2_late_b1_a09_g09_learning <- filtered_dfs_learning[[43]]
random_red_sr_4_b1_a09_g09_learning <- filtered_dfs_learning[[44]]
random_red_sr_4_late_b1_a09_g09_learning <- filtered_dfs_learning[[45]]

mf_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[1]]
mb_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[2]]
full_sr_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[3]]
red_sr_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[4]]
red_sr_2_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[5]]
red_sr_4_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[6]]
random_sr_update_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[7]]
random_sr_rupdate_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[8]]
random_sr_noupdate_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[9]]
random_red_sr_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[10]]
random_red_sr_late_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[11]]
random_red_sr_2_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[12]]
random_red_sr_2_late_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[13]]
random_red_sr_4_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[14]]
random_red_sr_4_late_b1_a09_g05_revaluation <- filtered_dfs_revaluation[[15]]

mf_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[16]]
mb_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[17]]
full_sr_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[18]]
red_sr_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[19]]
red_sr_2_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[20]]
red_sr_4_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[21]]
random_sr_update_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[22]]
random_sr_rupdate_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[23]]
random_sr_noupdate_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[24]]
random_red_sr_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[25]]
random_red_sr_late_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[26]]
random_red_sr_2_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[27]]
random_red_sr_2_late_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[28]]
random_red_sr_4_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[29]]
random_red_sr_4_late_b1_a09_g07_revaluation <- filtered_dfs_revaluation[[30]]

mf_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[31]]
mb_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[32]]
full_sr_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[33]]
red_sr_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[34]]
red_sr_2_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[35]]
red_sr_4_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[36]]
random_sr_update_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[37]]
random_sr_rupdate_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[38]]
random_sr_noupdate_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[39]]
random_red_sr_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[40]]
random_red_sr_late_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[41]]
random_red_sr_2_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[42]]
random_red_sr_2_late_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[43]]
random_red_sr_4_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[44]]
random_red_sr_4_late_b1_a09_g09_revaluation <- filtered_dfs_revaluation[[45]]

rm(mf_b1_a09_g05, mb_b1_a09_g05, full_sr_b1_a09_g05, red_sr_b1_a09_g05, red_sr_2_b1_a09_g05, red_sr_4_b1_a09_g05, random_sr_update_b1_a09_g05, random_sr_rupdate_b1_a09_g05, random_sr_noupdate_b1_a09_g05, random_red_sr_b1_a09_g05, random_red_sr_late_b1_a09_g05, random_red_sr_2_b1_a09_g05, random_red_sr_2_late_b1_a09_g05, random_red_sr_4_b1_a09_g05, random_red_sr_4_late_b1_a09_g05, 
                              mf_b1_a09_g07, mb_b1_a09_g07, full_sr_b1_a09_g07, red_sr_b1_a09_g07, red_sr_2_b1_a09_g07, red_sr_4_b1_a09_g07, random_sr_update_b1_a09_g07, random_sr_rupdate_b1_a09_g07, random_sr_noupdate_b1_a09_g07, random_red_sr_b1_a09_g07, random_red_sr_late_b1_a09_g07, random_red_sr_2_b1_a09_g07, random_red_sr_2_late_b1_a09_g07, random_red_sr_4_b1_a09_g07, random_red_sr_4_late_b1_a09_g07, 
                              mf_b1_a09_g09, mb_b1_a09_g09, full_sr_b1_a09_g09, red_sr_b1_a09_g09, red_sr_2_b1_a09_g09, red_sr_4_b1_a09_g09, random_sr_update_b1_a09_g09, random_sr_rupdate_b1_a09_g09, random_sr_noupdate_b1_a09_g09, random_red_sr_b1_a09_g09, random_red_sr_late_b1_a09_g09, random_red_sr_2_b1_a09_g09, random_red_sr_2_late_b1_a09_g09, random_red_sr_4_b1_a09_g09, random_red_sr_4_late_b1_a09_g09,
             filtered_dfs_learning, filtered_dfs_revaluation)

# create additional variables
mf_b1_a09_g05_learning <- mf_b1_a09_g05_learning %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_b1_a09_g05_learning <- mb_b1_a09_g05_learning %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

full_sr_b1_a09_g05_learning <- full_sr_b1_a09_g05_learning %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_b1_a09_g05_learning <- red_sr_b1_a09_g05_learning %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a09_g05_learning <- red_sr_2_b1_a09_g05_learning %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a09_g05_learning <- red_sr_4_b1_a09_g05_learning %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a09_g05_learning <- random_sr_update_b1_a09_g05_learning %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a09_g05_learning <- random_sr_rupdate_b1_a09_g05_learning %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a09_g05_learning <- random_sr_noupdate_b1_a09_g05_learning %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a09_g05_learning <- random_red_sr_b1_a09_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a09_g05_learning <- random_red_sr_late_b1_a09_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a09_g05_learning <- random_red_sr_2_b1_a09_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a09_g05_learning <- random_red_sr_2_late_b1_a09_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a09_g05_learning <- random_red_sr_4_b1_a09_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a09_g05_learning <- random_red_sr_4_late_b1_a09_g05_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")



mf_b1_a09_g05_revaluation <- mf_b1_a09_g05_revaluation %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_b1_a09_g05_revaluation<- mb_b1_a09_g05_revaluation %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

full_sr_b1_a09_g05_revaluation <- full_sr_b1_a09_g05_revaluation %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_b1_a09_g05_revaluation <- red_sr_b1_a09_g05_revaluation %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a09_g05_revaluation <- red_sr_2_b1_a09_g05_revaluation %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a09_g05_revaluation <- red_sr_4_b1_a09_g05_revaluation %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a09_g05_revaluation <- random_sr_update_b1_a09_g05_revaluation %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a09_g05_revaluation <- random_sr_rupdate_b1_a09_g05_revaluation %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a09_g05_revaluation <- random_sr_noupdate_b1_a09_g05_revaluation %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a09_g05_revaluation <- random_red_sr_b1_a09_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a09_g05_revaluation <- random_red_sr_late_b1_a09_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a09_g05_revaluation <- random_red_sr_2_b1_a09_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a09_g05_revaluation <- random_red_sr_2_late_b1_a09_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a09_g05_revaluation <- random_red_sr_4_b1_a09_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a09_g05_revaluation <- random_red_sr_4_late_b1_a09_g05_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.5) %>%
  rename("Condition" = "condition") 




mf_b1_a09_g07_learning <- mf_b1_a09_g07_learning %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_b1_a09_g07_learning <- mb_b1_a09_g07_learning %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

full_sr_b1_a09_g07_learning <- full_sr_b1_a09_g07_learning %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_b1_a09_g07_learning <- red_sr_b1_a09_g07_learning %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a09_g07_learning <- red_sr_2_b1_a09_g07_learning %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a09_g07_learning <- red_sr_4_b1_a09_g07_learning %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a09_g07_learning <- random_sr_update_b1_a09_g07_learning %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a09_g07_learning <- random_sr_rupdate_b1_a09_g07_learning %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a09_g07_learning <- random_sr_noupdate_b1_a09_g07_learning %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a09_g07_learning <- random_red_sr_b1_a09_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a09_g07_learning <- random_red_sr_late_b1_a09_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a09_g07_learning <- random_red_sr_2_b1_a09_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a09_g07_learning <- random_red_sr_2_late_b1_a09_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a09_g07_learning <- random_red_sr_4_b1_a09_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a09_g07_learning <- random_red_sr_4_late_b1_a09_g07_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")




mf_b1_a09_g07_revaluation <- mf_b1_a09_g07_revaluation %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_b1_a09_g07_revaluation<- mb_b1_a09_g07_revaluation %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

full_sr_b1_a09_g07_revaluation <- full_sr_b1_a09_g07_revaluation %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_b1_a09_g07_revaluation <- red_sr_b1_a09_g07_revaluation %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a09_g07_revaluation <- red_sr_2_b1_a09_g07_revaluation %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a09_g07_revaluation <- red_sr_4_b1_a09_g07_revaluation %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a09_g07_revaluation <- random_sr_update_b1_a09_g07_revaluation %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a09_g07_revaluation <- random_sr_rupdate_b1_a09_g07_revaluation %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a09_g07_revaluation <- random_sr_noupdate_b1_a09_g07_revaluation%>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a09_g07_revaluation <- random_red_sr_b1_a09_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a09_g07_revaluation <- random_red_sr_late_b1_a09_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a09_g07_revaluation <- random_red_sr_2_b1_a09_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a09_g07_revaluation <- random_red_sr_2_late_b1_a09_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a09_g07_revaluation <- random_red_sr_4_b1_a09_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a09_g07_revaluation <- random_red_sr_4_late_b1_a09_g07_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.7) %>%
  rename("Condition" = "condition") 




mf_b1_a09_g09_learning <- mf_b1_a09_g09_learning %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_b1_a09_g09_learning <- mb_b1_a09_g09_learning %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

full_sr_b1_a09_g09_learning <- full_sr_b1_a09_g09_learning %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_b1_a09_g09_learning <- red_sr_b1_a09_g09_learning %>%
  mutate(Model = "RedSR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a09_g09_learning <- red_sr_2_b1_a09_g09_learning %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a09_g09_learning <- red_sr_4_b1_a09_g09_learning %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a09_g09_learning <- random_sr_update_b1_a09_g09_learning %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a09_g09_learning <- random_sr_rupdate_b1_a09_g09_learning %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a09_g09_learning <- random_sr_noupdate_b1_a09_g09_learning %>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a09_g09_learning <- random_red_sr_b1_a09_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a09_g09_learning <- random_red_sr_late_b1_a09_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a09_g09_learning <- random_red_sr_2_b1_a09_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a09_g09_learning <- random_red_sr_2_late_b1_a09_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a09_g09_learning <- random_red_sr_4_b1_a09_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a09_g09_learning <- random_red_sr_4_late_b1_a09_g09_learning %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")



mf_b1_a09_g09_revaluation <- mf_b1_a09_g09_revaluation %>%
  mutate(Model = "MF",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_b1_a09_g09_revaluation<- mb_b1_a09_g09_revaluation %>%
  mutate(Model = "MB",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

full_sr_b1_a09_g09_revaluation <- full_sr_b1_a09_g09_revaluation %>%
  mutate(Model = "SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_b1_a09_g09_revaluation <- red_sr_b1_a09_g09_revaluation %>%
  mutate(Model = "RedSR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_2_b1_a09_g09_revaluation <- red_sr_2_b1_a09_g09_revaluation %>%
  mutate(Model = "RedSR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

red_sr_4_b1_a09_g09_revaluation <- red_sr_4_b1_a09_g09_revaluation %>%
  mutate(Model = "RedSR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_update_b1_a09_g09_revaluation <- random_sr_update_b1_a09_g09_revaluation %>%
  mutate(Model = "Random-policy SR",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_rupdate_b1_a09_g09_revaluation <- random_sr_rupdate_b1_a09_g09_revaluation %>%
  mutate(Model = "Random-policy SR (rigid M)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_sr_noupdate_b1_a09_g09_revaluation <- random_sr_noupdate_b1_a09_g09_revaluation%>%
  mutate(Model = "Random-policy SR (fully rigid)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_b1_a09_g09_revaluation <- random_red_sr_b1_a09_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_late_b1_a09_g09_revaluation <- random_red_sr_late_b1_a09_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (1 goal, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_b1_a09_g09_revaluation <- random_red_sr_2_b1_a09_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_2_late_b1_a09_g09_revaluation <- random_red_sr_2_late_b1_a09_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (2 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_b1_a09_g09_revaluation <- random_red_sr_4_b1_a09_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

random_red_sr_4_late_b1_a09_g09_revaluation <- random_red_sr_4_late_b1_a09_g09_revaluation %>%
  mutate(Model = "Reduced random-policy SR (4 goals, late)",
         beta = 1.0,
         alpha = 0.9,
         gamma = 0.9) %>%
  rename("Condition" = "condition") 



# merge dfs
df_alpha09_learning <- rbind(mf_b1_a09_g05_learning,
                    mb_b1_a09_g05_learning,
                    full_sr_b1_a09_g05_learning,
                    red_sr_b1_a09_g05_learning,
                    red_sr_2_b1_a09_g05_learning,
                    red_sr_4_b1_a09_g05_learning,
                    random_sr_update_b1_a09_g05_learning,
                    random_sr_rupdate_b1_a09_g05_learning,
                    random_sr_noupdate_b1_a09_g05_learning,
                    random_red_sr_b1_a09_g05_learning,
                    random_red_sr_late_b1_a09_g05_learning,
                    random_red_sr_2_b1_a09_g05_learning,
                    random_red_sr_2_late_b1_a09_g05_learning,
                    random_red_sr_4_b1_a09_g05_learning,
                    random_red_sr_4_late_b1_a09_g05_learning,
                    
                    mf_b1_a09_g07_learning,
                    mb_b1_a09_g07_learning,
                    full_sr_b1_a09_g07_learning,
                    red_sr_b1_a09_g07_learning,
                    red_sr_2_b1_a09_g07_learning,
                    red_sr_4_b1_a09_g07_learning,
                    random_sr_update_b1_a09_g07_learning,
                    random_sr_rupdate_b1_a09_g07_learning,
                    random_sr_noupdate_b1_a09_g07_learning,
                    random_red_sr_b1_a09_g07_learning,
                    random_red_sr_late_b1_a09_g07_learning,
                    random_red_sr_2_b1_a09_g07_learning,
                    random_red_sr_2_late_b1_a09_g07_learning,
                    random_red_sr_4_b1_a09_g07_learning,
                    random_red_sr_4_late_b1_a09_g07_learning,
                    
                    mf_b1_a09_g09_learning,
                    mb_b1_a09_g09_learning,
                    full_sr_b1_a09_g09_learning,
                    red_sr_b1_a09_g09_learning,
                    red_sr_2_b1_a09_g09_learning,
                    red_sr_4_b1_a09_g09_learning,
                    random_sr_update_b1_a09_g09_learning,
                    random_sr_rupdate_b1_a09_g09_learning,
                    random_sr_noupdate_b1_a09_g09_learning,
                    random_red_sr_b1_a09_g09_learning,
                    random_red_sr_late_b1_a09_g09_learning,
                    random_red_sr_2_b1_a09_g09_learning,
                    random_red_sr_2_late_b1_a09_g09_learning,
                    random_red_sr_4_b1_a09_g09_learning,
                    random_red_sr_4_late_b1_a09_g09_learning
                    )

df_alpha09_revaluation <- rbind(mf_b1_a09_g05_revaluation,
                    mb_b1_a09_g05_revaluation,
                    full_sr_b1_a09_g05_revaluation,
                    red_sr_b1_a09_g05_revaluation,
                    red_sr_2_b1_a09_g05_revaluation,
                    red_sr_4_b1_a09_g05_revaluation,
                    random_sr_update_b1_a09_g05_revaluation,
                    random_sr_rupdate_b1_a09_g05_revaluation,
                    random_sr_noupdate_b1_a09_g05_revaluation,
                    random_red_sr_b1_a09_g05_revaluation,
                    random_red_sr_late_b1_a09_g05_revaluation,
                    random_red_sr_2_b1_a09_g05_revaluation,
                    random_red_sr_2_late_b1_a09_g05_revaluation,
                    random_red_sr_4_b1_a09_g05_revaluation,
                    random_red_sr_4_late_b1_a09_g05_revaluation,
                    
                    mf_b1_a09_g07_revaluation,
                    mb_b1_a09_g07_revaluation,
                    full_sr_b1_a09_g07_revaluation,
                    red_sr_b1_a09_g07_revaluation,
                    red_sr_2_b1_a09_g07_revaluation,
                    red_sr_4_b1_a09_g07_revaluation,
                    random_sr_update_b1_a09_g07_revaluation,
                    random_sr_rupdate_b1_a09_g07_revaluation,
                    random_sr_noupdate_b1_a09_g07_revaluation,
                    random_red_sr_b1_a09_g07_revaluation,
                    random_red_sr_late_b1_a09_g07_revaluation,
                    random_red_sr_2_b1_a09_g07_revaluation,
                    random_red_sr_2_late_b1_a09_g07_revaluation,
                    random_red_sr_4_b1_a09_g07_revaluation,
                    random_red_sr_4_late_b1_a09_g07_revaluation,

                    mf_b1_a09_g09_revaluation,
                    mb_b1_a09_g09_revaluation,
                    full_sr_b1_a09_g09_revaluation,
                    red_sr_b1_a09_g09_revaluation,
                    red_sr_2_b1_a09_g09_revaluation,
                    red_sr_4_b1_a09_g09_revaluation,
                    random_sr_update_b1_a09_g09_revaluation,
                    random_sr_rupdate_b1_a09_g09_revaluation,
                    random_sr_noupdate_b1_a09_g09_revaluation,
                    random_red_sr_b1_a09_g09_revaluation,
                    random_red_sr_late_b1_a09_g09_revaluation,
                    random_red_sr_2_b1_a09_g09_revaluation,
                    random_red_sr_2_late_b1_a09_g09_revaluation,
                    random_red_sr_4_b1_a09_g09_revaluation,
                    random_red_sr_4_late_b1_a09_g09_revaluation
                    )

rm(mf_b1_a09_g05_learning,
    mb_b1_a09_g05_learning,
    full_sr_b1_a09_g05_learning,
    red_sr_b1_a09_g05_learning,
    red_sr_2_b1_a09_g05_learning,
    red_sr_4_b1_a09_g05_learning,
    random_sr_update_b1_a09_g05_learning,
    random_sr_rupdate_b1_a09_g05_learning,
    random_sr_noupdate_b1_a09_g05_learning,
    random_red_sr_b1_a09_g05_learning,
    random_red_sr_late_b1_a09_g05_learning,
    random_red_sr_2_b1_a09_g05_learning,
    random_red_sr_2_late_b1_a09_g05_learning,
    random_red_sr_4_b1_a09_g05_learning,
    random_red_sr_4_late_b1_a09_g05_learning,
    
    mf_b1_a09_g07_learning,
    mb_b1_a09_g07_learning,
    full_sr_b1_a09_g07_learning,
    red_sr_b1_a09_g07_learning,
    red_sr_2_b1_a09_g07_learning,
    red_sr_4_b1_a09_g07_learning,
    random_sr_update_b1_a09_g07_learning,
    random_sr_rupdate_b1_a09_g07_learning,
    random_sr_noupdate_b1_a09_g07_learning,
    random_red_sr_b1_a09_g07_learning,
    random_red_sr_late_b1_a09_g07_learning,
    random_red_sr_2_b1_a09_g07_learning,
    random_red_sr_2_late_b1_a09_g07_learning,
    random_red_sr_4_b1_a09_g07_learning,
    random_red_sr_4_late_b1_a09_g07_learning,
    
    mf_b1_a09_g09_learning,
    mb_b1_a09_g09_learning,
    full_sr_b1_a09_g09_learning,
    red_sr_b1_a09_g09_learning,
    red_sr_2_b1_a09_g09_learning,
    red_sr_4_b1_a09_g09_learning,
    random_sr_update_b1_a09_g09_learning,
    random_sr_rupdate_b1_a09_g09_learning,
    random_sr_noupdate_b1_a09_g09_learning,
    random_red_sr_b1_a09_g09_learning,
    random_red_sr_late_b1_a09_g09_learning,
    random_red_sr_2_b1_a09_g09_learning,
    random_red_sr_2_late_b1_a09_g09_learning,
    random_red_sr_4_b1_a09_g09_learning,
    random_red_sr_4_late_b1_a09_g09_learning,
   
    mf_b1_a09_g05_revaluation,
    mb_b1_a09_g05_revaluation,
    full_sr_b1_a09_g05_revaluation,
    red_sr_b1_a09_g05_revaluation,
    red_sr_2_b1_a09_g05_revaluation,
    red_sr_4_b1_a09_g05_revaluation,
    random_sr_update_b1_a09_g05_revaluation,
    random_sr_rupdate_b1_a09_g05_revaluation,
    random_sr_noupdate_b1_a09_g05_revaluation,
    random_red_sr_b1_a09_g05_revaluation,
    random_red_sr_late_b1_a09_g05_revaluation,
    random_red_sr_2_b1_a09_g05_revaluation,
    random_red_sr_2_late_b1_a09_g05_revaluation,
    random_red_sr_4_b1_a09_g05_revaluation,
    random_red_sr_4_late_b1_a09_g05_revaluation,
    
    mf_b1_a09_g07_revaluation,
    mb_b1_a09_g07_revaluation,
    full_sr_b1_a09_g07_revaluation,
    red_sr_b1_a09_g07_revaluation,
    red_sr_2_b1_a09_g07_revaluation,
    red_sr_4_b1_a09_g07_revaluation,
    random_sr_update_b1_a09_g07_revaluation,
    random_sr_rupdate_b1_a09_g07_revaluation,
    random_sr_noupdate_b1_a09_g07_revaluation,
    random_red_sr_b1_a09_g07_revaluation,
    random_red_sr_late_b1_a09_g07_revaluation,
    random_red_sr_2_b1_a09_g07_revaluation,
    random_red_sr_2_late_b1_a09_g07_revaluation,
    random_red_sr_4_b1_a09_g07_revaluation,
    random_red_sr_4_late_b1_a09_g07_revaluation,

    mf_b1_a09_g09_revaluation,
    mb_b1_a09_g09_revaluation,
    full_sr_b1_a09_g09_revaluation,
    red_sr_b1_a09_g09_revaluation,
    red_sr_2_b1_a09_g09_revaluation,
    red_sr_4_b1_a09_g09_revaluation,
    random_sr_update_b1_a09_g09_revaluation,
    random_sr_rupdate_b1_a09_g09_revaluation,
    random_sr_noupdate_b1_a09_g09_revaluation,
    random_red_sr_b1_a09_g09_revaluation,
    random_red_sr_late_b1_a09_g09_revaluation,
    random_red_sr_2_b1_a09_g09_revaluation,
    random_red_sr_2_late_b1_a09_g09_revaluation,
    random_red_sr_4_b1_a09_g09_revaluation,
    random_red_sr_4_late_b1_a09_g09_revaluation
   
   )
```

### Merge dfs

```{r}
# merge dfs
df_learning <- rbind(df_alpha05_learning, df_alpha07_learning, df_alpha09_learning)
df_revaluation <- rbind(df_alpha05_revaluation, df_alpha07_revaluation, df_alpha09_revaluation)
```

### alpha_td 09, alpha_m 05

```{r}
mb_learnt_b1_atd09_am_01_g05 <- read.csv(file.path(data_path, "model_based_learnt_nsimulations1000_alpha_td0.9_alpha_m0.1_beta1_gamma0.5.csv"))
mb_learnt_b1_atd09_am_03_g05 <- read.csv(file.path(data_path, "model_based_learnt_nsimulations1000_alpha_td0.9_alpha_m0.3_beta1_gamma0.5.csv"))
mb_learnt_b1_atd09_am_05_g05 <- read.csv(file.path(data_path, "model_based_learnt_nsimulations1000_alpha_td0.9_alpha_m0.5_beta1_gamma0.5.csv"))
mb_learnt_b1_atd09_am_01_g07 <- read.csv(file.path(data_path, "model_based_learnt_nsimulations1000_alpha_td0.9_alpha_m0.1_beta1_gamma0.7.csv"))
mb_learnt_b1_atd09_am_03_g07 <- read.csv(file.path(data_path, "model_based_learnt_nsimulations1000_alpha_td0.9_alpha_m0.3_beta1_gamma0.7.csv"))
mb_learnt_b1_atd09_am_05_g07 <- read.csv(file.path(data_path, "model_based_learnt_nsimulations1000_alpha_td0.9_alpha_m0.5_beta1_gamma0.7.csv"))
mb_learnt_b1_atd09_am_01_g09 <- read.csv(file.path(data_path, "model_based_learnt_nsimulations1000_alpha_td0.9_alpha_m0.1_beta1_gamma0.9.csv"))
mb_learnt_b1_atd09_am_03_g09 <- read.csv(file.path(data_path, "model_based_learnt_nsimulations1000_alpha_td0.9_alpha_m0.3_beta1_gamma0.9.csv"))
mb_learnt_b1_atd09_am_05_g09<- read.csv(file.path(data_path, "model_based_learnt_nsimulations1000_alpha_td0.9_alpha_m0.5_beta1_gamma0.9.csv"))


filtered_dfs_learning <- list(mb_learnt_b1_atd09_am_01_g05,
                              mb_learnt_b1_atd09_am_03_g05,
                              mb_learnt_b1_atd09_am_05_g05,
                              mb_learnt_b1_atd09_am_01_g07,
                              mb_learnt_b1_atd09_am_03_g07,
                              mb_learnt_b1_atd09_am_05_g07,
                              mb_learnt_b1_atd09_am_01_g09,
                              mb_learnt_b1_atd09_am_03_g09,
                              mb_learnt_b1_atd09_am_05_g09) %>%
    lapply(filter_model_df_learning)

filtered_dfs_revaluation <- list(mb_learnt_b1_atd09_am_01_g05,
                              mb_learnt_b1_atd09_am_03_g05,
                              mb_learnt_b1_atd09_am_05_g05,
                              mb_learnt_b1_atd09_am_01_g07,
                              mb_learnt_b1_atd09_am_03_g07,
                              mb_learnt_b1_atd09_am_05_g07,
                              mb_learnt_b1_atd09_am_01_g09,
                              mb_learnt_b1_atd09_am_03_g09,
                              mb_learnt_b1_atd09_am_05_g09) %>%
    lapply(filter_model_df_revaluation)

mb_learnt_b1_atd09_am01_g05_learning <- filtered_dfs_learning[[1]]
mb_learnt_b1_atd09_am03_g05_learning <- filtered_dfs_learning[[2]]
mb_learnt_b1_atd09_am05_g05_learning <- filtered_dfs_learning[[3]]
mb_learnt_b1_atd09_am01_g07_learning <- filtered_dfs_learning[[4]]
mb_learnt_b1_atd09_am03_g07_learning <- filtered_dfs_learning[[5]]
mb_learnt_b1_atd09_am05_g07_learning <- filtered_dfs_learning[[6]]
mb_learnt_b1_atd09_am01_g09_learning <- filtered_dfs_learning[[7]]
mb_learnt_b1_atd09_am03_g09_learning <- filtered_dfs_learning[[8]]
mb_learnt_b1_atd09_am05_g09_learning <- filtered_dfs_learning[[9]]

mb_learnt_b1_atd09_am01_g05_revaluation <- filtered_dfs_revaluation[[1]]
mb_learnt_b1_atd09_am03_g05_revaluation <- filtered_dfs_revaluation[[2]]
mb_learnt_b1_atd09_am05_g05_revaluation <- filtered_dfs_revaluation[[3]]
mb_learnt_b1_atd09_am01_g07_revaluation <- filtered_dfs_revaluation[[4]]
mb_learnt_b1_atd09_am03_g07_revaluation <- filtered_dfs_revaluation[[5]]
mb_learnt_b1_atd09_am05_g07_revaluation <- filtered_dfs_revaluation[[6]]
mb_learnt_b1_atd09_am01_g09_revaluation <- filtered_dfs_revaluation[[7]]
mb_learnt_b1_atd09_am03_g09_revaluation <- filtered_dfs_revaluation[[8]]
mb_learnt_b1_atd09_am05_g09_revaluation <- filtered_dfs_revaluation[[9]]

rm(mb_learnt_b1_atd09_am_01_g05,
    mb_learnt_b1_atd09_am_03_g05,
    mb_learnt_b1_atd09_am_05_g05,
    mb_learnt_b1_atd09_am_01_g07,
    mb_learnt_b1_atd09_am_03_g07,
    mb_learnt_b1_atd09_am_05_g07,
    mb_learnt_b1_atd09_am_01_g09,
    mb_learnt_b1_atd09_am_03_g09,
    mb_learnt_b1_atd09_am_05_g09,
    filtered_dfs_learning, filtered_dfs_revaluation)

# create additional variables
mb_learnt_b1_atd09_am01_g05_learning <- mb_learnt_b1_atd09_am01_g05_learning %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.1,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am03_g05_learning <- mb_learnt_b1_atd09_am03_g05_learning %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.3,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am05_g05_learning <- mb_learnt_b1_atd09_am05_g05_learning %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am01_g05_revaluation <- mb_learnt_b1_atd09_am01_g05_revaluation %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.1,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am03_g05_revaluation <- mb_learnt_b1_atd09_am03_g05_revaluation %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.3,
         gamma = 0.5) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am05_g05_revaluation <- mb_learnt_b1_atd09_am05_g05_revaluation %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.5,
         gamma = 0.5) %>%
  rename("Condition" = "condition")



mb_learnt_b1_atd09_am01_g07_learning <- mb_learnt_b1_atd09_am01_g07_learning %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.1,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am03_g07_learning <- mb_learnt_b1_atd09_am03_g07_learning %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.3,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am05_g07_learning <- mb_learnt_b1_atd09_am05_g07_learning %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am01_g07_revaluation <- mb_learnt_b1_atd09_am01_g07_revaluation %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.1,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am03_g07_revaluation <- mb_learnt_b1_atd09_am03_g07_revaluation %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.3,
         gamma = 0.7) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am05_g07_revaluation <- mb_learnt_b1_atd09_am05_g07_revaluation %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.5,
         gamma = 0.7) %>%
  rename("Condition" = "condition")




mb_learnt_b1_atd09_am01_g09_learning <- mb_learnt_b1_atd09_am01_g09_learning %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.1,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am03_g09_learning <- mb_learnt_b1_atd09_am03_g09_learning %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.3,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am05_g09_learning <- mb_learnt_b1_atd09_am05_g09_learning %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am01_g09_revaluation <- mb_learnt_b1_atd09_am01_g09_revaluation %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.1,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am03_g09_revaluation <- mb_learnt_b1_atd09_am03_g09_revaluation %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.3,
         gamma = 0.9) %>%
  rename("Condition" = "condition")

mb_learnt_b1_atd09_am05_g09_revaluation <- mb_learnt_b1_atd09_am05_g09_revaluation %>%
  mutate(Model = "MB α(R) > α(T)",
         beta = 1.0,
         alpha_R = 0.9,
         alpha_T = 0.5,
         gamma = 0.9) %>%
  rename("Condition" = "condition")


# merge dfs
df_mblearnt_learning <- rbind(mb_learnt_b1_atd09_am01_g05_learning,
                              mb_learnt_b1_atd09_am01_g07_learning,
                              mb_learnt_b1_atd09_am01_g09_learning,
                              mb_learnt_b1_atd09_am03_g05_learning,
                              mb_learnt_b1_atd09_am03_g07_learning,
                              mb_learnt_b1_atd09_am03_g09_learning,
                              mb_learnt_b1_atd09_am05_g05_learning,
                              mb_learnt_b1_atd09_am05_g07_learning,
                              mb_learnt_b1_atd09_am05_g09_learning)

df_mblearnt_revaluation <- rbind(mb_learnt_b1_atd09_am01_g05_revaluation,
                              mb_learnt_b1_atd09_am01_g07_revaluation,
                              mb_learnt_b1_atd09_am01_g09_revaluation,
                              mb_learnt_b1_atd09_am03_g05_revaluation,
                              mb_learnt_b1_atd09_am03_g07_revaluation,
                              mb_learnt_b1_atd09_am03_g09_revaluation,
                              mb_learnt_b1_atd09_am05_g05_revaluation,
                              mb_learnt_b1_atd09_am05_g07_revaluation,
                              mb_learnt_b1_atd09_am05_g09_revaluation)

rm(mb_learnt_b1_atd09_am01_g05_learning,
                              mb_learnt_b1_atd09_am01_g07_learning,
                              mb_learnt_b1_atd09_am01_g09_learning,
                              mb_learnt_b1_atd09_am03_g05_learning,
                              mb_learnt_b1_atd09_am03_g07_learning,
                              mb_learnt_b1_atd09_am03_g09_learning,
                              mb_learnt_b1_atd09_am05_g05_learning,
                              mb_learnt_b1_atd09_am05_g07_learning,
                              mb_learnt_b1_atd09_am05_g09_learning,
   
                              mb_learnt_b1_atd09_am01_g05_revaluation,
                              mb_learnt_b1_atd09_am01_g07_revaluation,
                              mb_learnt_b1_atd09_am01_g09_revaluation,
                              mb_learnt_b1_atd09_am03_g05_revaluation,
                              mb_learnt_b1_atd09_am03_g07_revaluation,
                              mb_learnt_b1_atd09_am03_g09_revaluation,
                              mb_learnt_b1_atd09_am05_g05_revaluation,
                              mb_learnt_b1_atd09_am05_g07_revaluation,
                              mb_learnt_b1_atd09_am05_g09_revaluation)
                    
```

## Create plotting dfs

```{r}
learning_plot_alpha05_df <- df_learning[df_learning$alpha==0.5,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(correct_path, na.rm = TRUE),
            se_correct = sd(correct_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(correct_path, na.rm=TRUE)[2],
            ci_u = ci(correct_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma),
         Model = factor(Model, levels= c("MB", "SR", "RedSR (1 goal)", "MF", 
                                         "RedSR (2 goals)", "RedSR (4 goals)",
                                         "Random-policy SR", "Random-policy SR (rigid M)", "Random-policy SR (fully rigid)", 
                                         "Reduced random-policy SR (1 goal)", "Reduced random-policy SR (1 goal, late)",
                                         "Reduced random-policy SR (2 goals)", "Reduced random-policy SR (2 goals, late)",
                                         "Reduced random-policy SR (4 goals)", "Reduced random-policy SR (4 goals, late)"
                                         )))

learning_plot_alpha07_df <- df_learning[df_learning$alpha==0.7,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(correct_path, na.rm = TRUE),
            se_correct = sd(correct_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(correct_path, na.rm=TRUE)[2],
            ci_u = ci(correct_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma),
         Model = factor(Model, levels= c("MB", "SR", "RedSR (1 goal)", "MF", 
                                         "RedSR (2 goals)", "RedSR (4 goals)",
                                         "Random-policy SR", "Random-policy SR (rigid M)", "Random-policy SR (fully rigid)", 
                                         "Reduced random-policy SR (1 goal)", "Reduced random-policy SR (1 goal, late)",
                                         "Reduced random-policy SR (2 goals)", "Reduced random-policy SR (2 goals, late)",
                                         "Reduced random-policy SR (4 goals)", "Reduced random-policy SR (4 goals, late)")))

learning_plot_alpha09_df <- df_learning[df_learning$alpha==0.9,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(correct_path, na.rm = TRUE),
            se_correct = sd(correct_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(correct_path, na.rm=TRUE)[2],
            ci_u = ci(correct_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma),
         Model = factor(Model, levels= c("MB", "SR", "RedSR (1 goal)", "MF", 
                                         "RedSR (2 goals)", "RedSR (4 goals)",
                                         "Random-policy SR", "Random-policy SR (rigid M)", "Random-policy SR (fully rigid)", 
                                         "Reduced random-policy SR (1 goal)", "Reduced random-policy SR (1 goal, late)",
                                         "Reduced random-policy SR (2 goals)", "Reduced random-policy SR (2 goals, late)",
                                         "Reduced random-policy SR (4 goals)", "Reduced random-policy SR (4 goals, late)")))

revaluation_plot_alpha05_df <- df_revaluation[df_revaluation$alpha==0.5,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(switch_path, na.rm = TRUE),
            se_correct = sd(switch_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(switch_path, na.rm=TRUE)[2],
            ci_u = ci(switch_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma),
         Model = factor(Model, levels= c("MB", "SR", "RedSR (1 goal)", "MF", 
                                         "RedSR (2 goals)", "RedSR (4 goals)",
                                         "Random-policy SR", "Random-policy SR (rigid M)", "Random-policy SR (fully rigid)", 
                                         "Reduced random-policy SR (1 goal)", "Reduced random-policy SR (1 goal, late)",
                                         "Reduced random-policy SR (2 goals)", "Reduced random-policy SR (2 goals, late)",
                                         "Reduced random-policy SR (4 goals)", "Reduced random-policy SR (4 goals, late)")))

revaluation_plot_alpha07_df <- df_revaluation[df_revaluation$alpha==0.7,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(switch_path, na.rm = TRUE),
            se_correct = sd(switch_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(switch_path, na.rm=TRUE)[2],
            ci_u = ci(switch_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma),
         Model = factor(Model, levels= c("MB", "SR", "RedSR (1 goal)", "MF", 
                                         "RedSR (2 goals)", "RedSR (4 goals)",
                                         "Random-policy SR", "Random-policy SR (rigid M)", "Random-policy SR (fully rigid)", 
                                         "Reduced random-policy SR (1 goal)", "Reduced random-policy SR (1 goal, late)",
                                         "Reduced random-policy SR (2 goals)", "Reduced random-policy SR (2 goals, late)",
                                         "Reduced random-policy SR (4 goals)", "Reduced random-policy SR (4 goals, late)")))

revaluation_plot_alpha09_df <- df_revaluation[df_revaluation$alpha==0.9,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(switch_path, na.rm = TRUE),
            se_correct = sd(switch_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(switch_path, na.rm=TRUE)[2],
            ci_u = ci(switch_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma),
         Model = factor(Model, levels= c("MB", "SR", "RedSR (1 goal)", "MF", 
                                         "RedSR (2 goals)", "RedSR (4 goals)",
                                         "Random-policy SR", "Random-policy SR (rigid M)", "Random-policy SR (fully rigid)", 
                                         "Reduced random-policy SR (1 goal)", "Reduced random-policy SR (1 goal, late)",
                                         "Reduced random-policy SR (2 goals)", "Reduced random-policy SR (2 goals, late)",
                                         "Reduced random-policy SR (4 goals)", "Reduced random-policy SR (4 goals, late)")))


# MB learnt
learning_MBlearnt_plot_aR09_aT01_df <- df_mblearnt_learning[df_mblearnt_learning$alpha_R==0.9 & df_mblearnt_learning$alpha_T==0.1,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(correct_path, na.rm = TRUE),
            se_correct = sd(correct_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(correct_path, na.rm=TRUE)[2],
            ci_u = ci(correct_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma))

learning_MBlearnt_plot_aR09_aT03_df <- df_mblearnt_learning[df_mblearnt_learning$alpha_R==0.9 & df_mblearnt_learning$alpha_T==0.3,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(correct_path, na.rm = TRUE),
            se_correct = sd(correct_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(correct_path, na.rm=TRUE)[2],
            ci_u = ci(correct_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma))

learning_MBlearnt_plot_aR09_aT05_df <- df_mblearnt_learning[df_mblearnt_learning$alpha_R==0.9 & df_mblearnt_learning$alpha_T==0.5,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(correct_path, na.rm = TRUE),
            se_correct = sd(correct_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(correct_path, na.rm=TRUE)[2],
            ci_u = ci(correct_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma))

revaluation_MBlearnt_plot_aR09_aT01_df <- df_mblearnt_revaluation[df_mblearnt_revaluation$alpha_R==0.9 & df_mblearnt_revaluation$alpha_T==0.1,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(switch_path, na.rm = TRUE),
            se_correct = sd(switch_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(switch_path, na.rm=TRUE)[2],
            ci_u = ci(switch_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma))

revaluation_MBlearnt_plot_aR09_aT03_df <- df_mblearnt_revaluation[df_mblearnt_revaluation$alpha_R==0.9 & df_mblearnt_revaluation$alpha_T==0.3,] %>%
  group_by(Model,Condition, gamma) %>%
  summarise(mean_correct = mean(switch_path, na.rm = TRUE),
            se_correct = sd(switch_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(switch_path, na.rm=TRUE)[2],
            ci_u = ci(switch_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma))

revaluation_MBlearnt_plot_aR09_aT05_df <- df_mblearnt_revaluation[df_mblearnt_revaluation$alpha_R==0.9 & df_mblearnt_revaluation$alpha_T==0.5,] %>%
  group_by(Model, Condition, gamma) %>%
  summarise(mean_correct = mean(switch_path, na.rm = TRUE),
            se_correct = sd(switch_path, na.rm = TRUE)/sqrt(n()),
            ci_l = ci(switch_path, na.rm=TRUE)[2],
            ci_u = ci(switch_path, na.rm=TRUE)[3],
            n = n()
            ) %>%
  mutate(gamma = as.factor(gamma))

revaluation_all_plot_alpha09_aT01 <- rbind(revaluation_plot_alpha09_df, revaluation_MBlearnt_plot_aR09_aT01_df) %>%
  mutate(Model = factor(Model, levels= c("MB α(R) > α(T)", "MB", "SR", "RedSR (1 goal)", "MF", 
                                         "RedSR (2 goals)", "RedSR (4 goals)",
                                         "Random-policy SR", "Random-policy SR (rigid M)", "Random-policy SR (fully rigid)", 
                                         "Reduced random-policy SR (1 goal)", "Reduced random-policy SR (1 goal, late)",
                                         "Reduced random-policy SR (2 goals)", "Reduced random-policy SR (2 goals, late)",
                                         "Reduced random-policy SR (4 goals)", "Reduced random-policy SR (4 goals, late)")))
```

## Supplementary plot: A priori models across range of parameters

```{r}
supp.labs <- c("MF", "MB", "SR", "RedSR")
names(supp.labs) <- c("MF", "MB", "SR", "RedSR (1 goal)")

learning_plot_alpha05 <- ggplot(learning_plot_alpha05_df[learning_plot_alpha05_df$Model %in% c("MF", "MB", "SR", "RedSR (1 goal)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 2) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.5") +
    xlab("Time discount factor (γ)")
learning_plot_alpha05

revaluation_plot_alpha05 <- ggplot(revaluation_plot_alpha05_df[revaluation_plot_alpha05_df$Model %in% c("MF", "MB", "SR", "RedSR (1 goal)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 2) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.5") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha05

learning_plot_alpha07 <- ggplot(learning_plot_alpha07_df[learning_plot_alpha07_df$Model %in% c("MF", "MB", "SR", "RedSR (1 goal)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 2) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.7") +
    xlab("Time discount factor (γ)")
learning_plot_alpha07

revaluation_plot_alpha07 <- ggplot(revaluation_plot_alpha07_df[revaluation_plot_alpha07_df$Model %in% c("MF", "MB", "SR", "RedSR (1 goal)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 2) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.7") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha07

learning_plot_alpha09 <- ggplot(learning_plot_alpha09_df[learning_plot_alpha09_df$Model %in% c("MF", "MB", "SR", "RedSR (1 goal)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 2) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.9") +
    xlab("Time discount factor (γ)")
learning_plot_alpha09

revaluation_plot_alpha09 <- ggplot(revaluation_plot_alpha09_df[revaluation_plot_alpha09_df$Model %in% c("MF", "MB", "SR", "RedSR (1 goal)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 2) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.9") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha09

ggarrange(learning_plot_alpha05, revaluation_plot_alpha05,
          learning_plot_alpha07, revaluation_plot_alpha07,
          learning_plot_alpha09, revaluation_plot_alpha09,
          ncol=2, nrow=3,
          common.legend = T, legend="bottom")

ggsave(filename = "suppl_simulation_apriorimodels.png", path = figure_path, width = 14, height = 14, device='png', dpi=600, bg="white")

```

## Supplementary plot: Reduced SR with different n of goal-states across range of parameters

```{r}

supp.labs <- c("RedSR (2 goal)", "RedSR (3 goals)", "RedSR (4 goals)")
names(supp.labs) <- c("RedSR (1 goal)", "RedSR (2 goals)", "RedSR (4 goals)")

learning_plot_alpha05 <- ggplot(learning_plot_alpha05_df[learning_plot_alpha05_df$Model %in% c("RedSR (1 goal)", "RedSR (2 goals)", "RedSR (4 goals)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.5") +
    xlab("Time discount factor (γ)")
learning_plot_alpha05

revaluation_plot_alpha05 <- ggplot(revaluation_plot_alpha05_df[revaluation_plot_alpha05_df$Model %in% c("RedSR (1 goal)", "RedSR (2 goals)", "RedSR (4 goals)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.5") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha05

learning_plot_alpha07 <- ggplot(learning_plot_alpha07_df[learning_plot_alpha07_df$Model %in% c("RedSR (1 goal)", "RedSR (2 goals)", "RedSR (4 goals)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.7") +
    xlab("Time discount factor (γ)")
learning_plot_alpha07

revaluation_plot_alpha07 <- ggplot(revaluation_plot_alpha07_df[revaluation_plot_alpha07_df$Model %in% c("RedSR (1 goal)", "RedSR (2 goals)", "RedSR (4 goals)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.7") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha07

learning_plot_alpha09 <- ggplot(learning_plot_alpha09_df[learning_plot_alpha09_df$Model %in% c("RedSR (1 goal)", "RedSR (2 goals)", "RedSR (4 goals)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.9") +
    xlab("Time discount factor (γ)")
learning_plot_alpha09

revaluation_plot_alpha09 <- ggplot(revaluation_plot_alpha09_df[revaluation_plot_alpha09_df$Model %in% c("RedSR (1 goal)", "RedSR (2 goals)", "RedSR (4 goals)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.9") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha09

ggarrange(learning_plot_alpha05, revaluation_plot_alpha05,
          learning_plot_alpha07, revaluation_plot_alpha07,
          learning_plot_alpha09, revaluation_plot_alpha09,
          ncol=2, nrow=3,
          common.legend = T, legend="bottom")

ggsave(filename = "suppl_simulation_redSR.png", path = figure_path, width = 14, height = 12, device='png', dpi=600, bg="white")

```

## Main plot: A priori models at selected parameters

```{r}
supp.labs <- c("MF", "MB", "SR", "RedSR")
names(supp.labs) <- c("MF", "MB", "SR", "RedSR (1 goal)")

apriori_plot <- ggplot(
  revaluation_plot_alpha09_df[revaluation_plot_alpha09_df$Model %in% c("MB", "SR", "RedSR (1 goal)", "MF") & revaluation_plot_alpha09_df$gamma == 0.5,], 
  aes(x=Condition, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "none",
          axis.title.x=element_blank(),
          axis.text.x = element_text(angle = 50, vjust = 1, hjust=1, size=16),
          strip.text.x = element_text(size = 16),
          axis.text.y = element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE)
apriori_plot

ggsave(filename = "main_simulation_apriorimodels.png", path = figure_path, width = 13, height = 5.5, device='png', dpi=600, bg="white")

```

## Supplementary plot: MB with different learning rates

```{r}
learning_plot_MBlearnt_aT01 <- ggplot(learning_MBlearnt_plot_aR09_aT01_df, aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    #facet_wrap(vars(Model), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α(T)=0.1") +
    xlab("Time discount factor (γ)")
learning_plot_MBlearnt_aT01

revaluation_plot_MBlearnt_aT01 <- ggplot(revaluation_MBlearnt_plot_aR09_aT01_df, aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    #facet_wrap(vars(Model), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α(T)=0.1") +
    xlab("Time discount factor (γ)")
revaluation_plot_MBlearnt_aT01

learning_plot_MBlearnt_aT03 <- ggplot(learning_MBlearnt_plot_aR09_aT03_df, aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    #facet_wrap(vars(Model), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α(T)=0.3") +
    xlab("Time discount factor (γ)")
learning_plot_MBlearnt_aT03

revaluation_plot_MBlearnt_aT03 <- ggplot(revaluation_MBlearnt_plot_aR09_aT03_df, aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    #facet_wrap(vars(Model), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α(T)=0.3") +
    xlab("Time discount factor (γ)")
revaluation_plot_MBlearnt_aT03

learning_plot_MBlearnt_aT05 <- ggplot(learning_MBlearnt_plot_aR09_aT05_df, aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    #facet_wrap(vars(Model), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α(T)=0.5") +
    xlab("Time discount factor (γ)")
learning_plot_MBlearnt_aT05

revaluation_plot_MBlearnt_aT05 <- ggplot(revaluation_MBlearnt_plot_aR09_aT05_df, aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    #facet_wrap(vars(Model), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α(T)=0.5") +
    xlab("Time discount factor (γ)")
revaluation_plot_MBlearnt_aT05

ggarrange(learning_plot_MBlearnt_aT01, revaluation_plot_MBlearnt_aT01,
          learning_plot_MBlearnt_aT03, revaluation_plot_MBlearnt_aT03,
          learning_plot_MBlearnt_aT05, revaluation_plot_MBlearnt_aT05,
          nrow=3, ncol=2,
          common.legend = T, legend= "bottom")

ggsave(filename = "suppl_simulation_MBmodels.png", path = figure_path, width = 14, height = 12, device='png', dpi=600, bg="white")

```

## Supplementary plot: Random-policy SR

```{r}
supp.labs <- c("Random-policy SR \n (rigid M)", "Random-policy SR \n (rigid M & w)")
names(supp.labs) <- c("Random-policy SR (rigid M)", "Random-policy SR (fully rigid)")

learning_plot_alpha05 <- ggplot(learning_plot_alpha05_df[learning_plot_alpha05_df$Model %in% c("Random-policy SR (rigid M)", "Random-policy SR (fully rigid)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.5") +
    xlab("Time discount factor (γ)")
learning_plot_alpha05

revaluation_plot_alpha05 <- ggplot(revaluation_plot_alpha05_df[revaluation_plot_alpha05_df$Model %in% c("Random-policy SR (rigid M)", "Random-policy SR (fully rigid)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.5") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha05

learning_plot_alpha07 <- ggplot(learning_plot_alpha07_df[learning_plot_alpha07_df$Model %in% c("Random-policy SR (rigid M)", "Random-policy SR (fully rigid)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.7") +
    xlab("Time discount factor (γ)")
learning_plot_alpha07

revaluation_plot_alpha07 <- ggplot(revaluation_plot_alpha07_df[revaluation_plot_alpha07_df$Model %in% c("Random-policy SR (rigid M)", "Random-policy SR (fully rigid)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.7") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha07

learning_plot_alpha09 <- ggplot(learning_plot_alpha09_df[learning_plot_alpha09_df$Model %in% c("Random-policy SR (rigid M)", "Random-policy SR (fully rigid)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.9") +
    xlab("Time discount factor (γ)")
learning_plot_alpha09

revaluation_plot_alpha09 <- ggplot(revaluation_plot_alpha09_df[revaluation_plot_alpha09_df$Model %in% c("Random-policy SR (rigid M)", "Random-policy SR (fully rigid)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.9") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha09

ggarrange(learning_plot_alpha05, revaluation_plot_alpha05,
          learning_plot_alpha07, revaluation_plot_alpha07,
          learning_plot_alpha09, revaluation_plot_alpha09,
          ncol=2, nrow=3,
          common.legend = T, legend="bottom")

ggsave(filename = "suppl_simulation_randSRmodels.png", path = figure_path, width = 14, height = 12, device='png', dpi=600, bg="white")

```

## Supplementary plot: Reduced random-policy SR

```{r}
supp.labs <- c("Reduced \n random-policy SR \n (2 goals)", "Reduced \n random-policy SR \n (3 goals)", "Reduced \n random-policy SR \n (4 goals)")
names(supp.labs) <- c("Reduced random-policy SR (1 goal, late)", "Reduced random-policy SR (2 goals, late)", "Reduced random-policy SR (4 goals, late)")

learning_plot_alpha05 <- ggplot(learning_plot_alpha05_df[learning_plot_alpha05_df$Model %in% c("Reduced random-policy SR (1 goal, late)", "Reduced random-policy SR (2 goals, late)", "Reduced random-policy SR (4 goals, late)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1, ncol = 3) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.5") +
    xlab("Time discount factor (γ)")
learning_plot_alpha05

revaluation_plot_alpha05 <- ggplot(revaluation_plot_alpha05_df[revaluation_plot_alpha05_df$Model %in% c("Reduced random-policy SR (1 goal, late)", "Reduced random-policy SR (2 goals, late)", "Reduced random-policy SR (4 goals, late)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1, ncol = 3) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.5") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha05

learning_plot_alpha07 <- ggplot(learning_plot_alpha07_df[learning_plot_alpha07_df$Model %in% c("Reduced random-policy SR (1 goal, late)", "Reduced random-policy SR (2 goals, late)", "Reduced random-policy SR (4 goals, late)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1, ncol = 3) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.7") +
    xlab("Time discount factor (γ)")
learning_plot_alpha07

revaluation_plot_alpha07 <- ggplot(revaluation_plot_alpha07_df[revaluation_plot_alpha07_df$Model %in% c("Reduced random-policy SR (1 goal, late)", "Reduced random-policy SR (2 goals, late)", "Reduced random-policy SR (4 goals, late)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1, ncol = 3) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.7") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha07

learning_plot_alpha09 <- ggplot(learning_plot_alpha09_df[learning_plot_alpha09_df$Model %in% c("Reduced random-policy SR (1 goal, late)", "Reduced random-policy SR (2 goals, late)", "Reduced random-policy SR (4 goals, late)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    #geom_line(aes(color=Condition), size=1, alpha=0.5) +
    #geom_ribbon(aes(ymin = ci_l * 100, ymax = ci_u * 100, fill = Condition), alpha = 0.2) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(0.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1, ncol = 3) +
    scale_y_continuous("Proportion of simulation runs with \n optimal path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_color_viridis(option = "viridis", discrete = TRUE) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Learning performance at α=0.9") +
    xlab("Time discount factor (γ)")
learning_plot_alpha09

revaluation_plot_alpha09 <- ggplot(revaluation_plot_alpha09_df[revaluation_plot_alpha09_df$Model %in% c("Reduced random-policy SR (1 goal, late)", "Reduced random-policy SR (2 goals, late)", "Reduced random-policy SR (4 goals, late)"),], aes(x=gamma, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs), nrow = 1, ncol = 3) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "right",
          #axis.title.x=element_blank(),
          axis.text.x = element_text(size=16),
          axis.text.y = element_text(size=16),
          strip.text.x = element_text(size = 16),
          legend.text=element_text(size=16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE) +
    ggtitle("Revaluation performance at α=0.9") +
    xlab("Time discount factor (γ)")
revaluation_plot_alpha09

ggarrange(learning_plot_alpha05, revaluation_plot_alpha05,
          learning_plot_alpha07, revaluation_plot_alpha07,
          learning_plot_alpha09, revaluation_plot_alpha09, 
          ncol=2, nrow=3,
          common.legend = T, legend="bottom")

ggsave(filename = "suppl_simulation_redrandSRmodels.png", path = figure_path, width = 14, height = 12, device='png', dpi=600, bg="white")

```

## Main plot: Post hoc models

```{r}
supp.labs <- c("MB α(R) > α(T)", "Random-policy SR", "Reduced random-policy SR")
names(supp.labs) <- c("MB α(R) > α(T)", "Random-policy SR (fully rigid)", "Reduced random-policy SR (4 goals)")



posthoc_plot <- ggplot(revaluation_all_plot_alpha09_aT01[revaluation_all_plot_alpha09_aT01$Model %in% c("MB α(R) > α(T)", "Random-policy SR (fully rigid)", "Reduced random-policy SR (4 goals)") & revaluation_all_plot_alpha09_aT01$gamma == 0.5,], 
                       aes(x=Condition, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.8) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs),
               nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "none",
          axis.title.x=element_blank(),
          axis.text.x = element_text(angle = 50, vjust = 1, hjust=1, size=16),
          strip.text.x = element_text(size = 16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE)
posthoc_plot

ggsave(filename = "main_simulation_paths_posthocmodels.png", path = figure_path, width = 11, height = 5.5, device='png', dpi=600)

```

## Poster plot: A priori & post hoc models

```{r}
supp.labs <- c("SR", "RedSR", "Random-policy SR", "Reduced random-policy SR")
names(supp.labs) <- c("SR", "RedSR (1 goal)", "Random-policy SR (fully rigid)", "Reduced random-policy SR (4 goals)")

poster_plot <- ggplot(revaluation_plot_alpha09_df[revaluation_plot_alpha09_df$Model %in% c("SR", "RedSR (1 goal)", "Random-policy SR (fully rigid)", "Reduced random-policy SR (4 goals)") & revaluation_plot_alpha09_df$gamma == 0.5,], 
                       aes(x=Condition, y=mean_correct, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=0.6) +
    geom_errorbar(aes(ymin=mean_correct-se_correct, ymax=mean_correct+se_correct),
                  width=.2,
                  position=position_dodge(.9)) +
    facet_wrap(vars(Model), labeller = as_labeller(supp.labs),
               nrow = 1) +
    scale_y_continuous("Proportion of simulation runs with \n changed path preference", limits = c(-0.05, 1.05)) +
    theme_light(base_size = 16, base_family = "Aptos") +
    theme(legend.position = "none",
          axis.title.x=element_blank(),
          axis.text.x = element_text(angle = 50, vjust = 1, hjust=1, size=16),
          strip.text.x = element_text(size = 16)) +
    scale_fill_viridis(option = "viridis", discrete = TRUE)
poster_plot

ggsave(filename = "poster_simulations.png", path = figure_path, width = 13, height = 5, device='png', dpi=600, bg="white")
```

# Plot based on theoretical considerations

## Create data

```{r}
Model <- factor(c(rep("MF", 5), rep("MB", 5), rep("SR", 5), rep("RedSR", 5)), 
                levels = c("MF", "MB", "SR", "RedSR"))

Condition <- factor(rep(c("Reward revaluation", "Goal-state revaluation", "Transition revaluation", "Policy revaluation", "Control"), 4), 
                    levels = c("Reward revaluation", "Goal-state revaluation", "Transition revaluation", "Policy revaluation", "Control"))


mean_switch_mf <- c(0.01,0.01,0.01,0.01,0.01)
mean_switch_sr <- c(1,1,0.01,0.01,0.01)
mean_switch_redsr <- c(1,0.01,0.01,0.01,0.01)
mean_switch_mb <- c(1,1,1,1,0.01)
mean_switch <- c(mean_switch_mf, mean_switch_mb, mean_switch_sr, mean_switch_redsr)

theoretical_plot_df <- data.frame(Model, Condition, mean_switch)

```

## Plot

```{r}
theoretical_plot <- ggplot(theoretical_plot_df, aes(x=Condition, y=mean_switch, fill=Condition)) +
    geom_bar(stat="identity", aes(fill = Condition), position=position_dodge(), alpha=1) +
    facet_wrap(vars(Model), nrow = 1) +
    scale_y_continuous("% Trials with changed preference \n regarding optimal path after re-learning", limits = c(-0.05, 1.05)) +
    theme_bw(base_size = 16) +
    theme(legend.position = "bottom",
          axis.title.x=element_blank(),
          axis.text.x = element_blank()) +
    scale_fill_viridis(option = "viridis", discrete = TRUE)
theoretical_plot
```
