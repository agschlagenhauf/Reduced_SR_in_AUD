
# Simulating agents' behavior in our sequential sedicion-making task environment
This code runs simulations of the AUD decision-making task on various RL models.



## How to Use

simulate.py's simulate function can be called to run a specified number of simulations for a given model and condition.

test_script.py is a premade script that runs simulate for every model and condition, and additionally calculates the overall success rates of each, which are written to a text file. If you want this summary for just a specific model or condition you can change the content of the "models" and "conditions" list near the top of the script.



## Logs

There are currently three types of logs generated by test_script.py, all of which are stored in the Results folder:

[model]_[condition].csv - updates every time step, includes the model and condition, simulation, phase and episode numbers, current state, action chosen, reward prediction error, and the current Q-values. Files get very large with a large number of trials, so generating this file can be turned off by setting the full_logging variable of the Simulate function to False.

[model]_[condition]_Results.csv - updates at the end of each learning and relearning phase, includes the simulation number and current (just finished) phase, the currently-preferred action from state 1, and the relevant learned values for the given model. Q-values for Punctate, weights (rewards) and transition matrix for Model-Based, and weights and successor matrix for Successor Representation. The transition and successor matrices are flattened.

success_rates.txt - compilation of the success rates of each model and condition tested in test_script.py. For each combination, includes the percentage of all trials with a successful test after the learning phase and the percentage of ONLY initially successful trials that also had a successful test after the relearning phase.



## How to change task structure

Hyperparameters (alpha, gamma, explore_chance) can be directly changed in simulate.py

To change the number of episodes allowed for learning or relearning, you can change the start_states variable in the pretraining and retraining functions of each model's file, each value corresponds to one episode starting at the given state.

Changing the task structure can be done by changing the transitions and rewards lists, which has to be done both in simulate.py and in each model's update_parameters function. Each row in the top-level list corresponds to a state of the task, and each item in that row correponds to a state it can transition into, or the reward given for that transition, respectively. You'll also need to change the variables end_states, num_pairs (the total number of actions) and num_states, in simulate.py
